# Conv3D - 3D convolution pipeline for the binding project

## General info
This is a usual PyTorch pipeline. The data is generated by another 
directory of this repo (binding-project). It is a list of 3D tensors
of features named following the PDBname_ligandID_ID.pdb convention.
The tensors are aligned to their eigen axis and some data analysis is conducted
in binding-project. 
There is also a dictionnary mapping the ligand ids to their embeddings
obtained in the cddd subproject.

The software requirements and version are stated in requirements.txt

##Setup
Mostly data to retrieve. One should run 
 ```bash
 python src/utils.py -s
  ```
  for basic file setup.
 It should be available for download or contact the authors.
The data must be saved in pockets/unique_pockets (resp unique_pockets_hard) and the
ligand file should be saved in ligands/whole_dict_embed_128.p
Then the environment should be created either as a conda environment or running the setup
script. Then the main.py can be run with a few options 'man main.py' for help.

## Data
Mostly the dataset loader. The data is produced as u8 to get the most 
compressed representation, so it needs to be cast to floats.
We use the data alignment to eigen axis in 'unique_pockets' . Overall, this aligned 
representation enables us to reduce the possible poses of the ligand 
on the grid, which should improve learning as well as reduce the size
of the model. The grid size is chosen to keep almost all of the data
in a (40,30,30) grid. To get all rotation augmentation, and get data loading, 
we offer 3 approaches. We build the explicit enumeration of all 2**3 poses * n 
data points and then the loader can 
- construct the tensor on the fly or load the whole data on the RAM (argument : ram)
- Do some data augmentation or just load a pre-processed dataset (argument : augment_flip)

We also can use a 'siamese version' of the data Loader to have each batch computing an 
average over the 8 possible poses. This approach is an elegant way of bypassing the 
weight tying procedure but reduces the effective batch size quite a lot.
One should also pay attention to the testing phase.

## Src
The training utilities. The logging uses tensorboard and the rest is pure pytorch.
Some helper functions can be found in utils.py (logging, model saving...), while the
machine learning pipeline is defined in learning.py

## models
The python definition of the modules we use

## logs
The dir to which logs are saved

## saved models
The serialised neural nets
