{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logs cleaner\n",
    "If some log files were created with different metrics (for instance if we used a wrong name or a different normalisation constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import Tensorboard\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "renaming = {\n",
    "    \"Train loss during training\": \"Training epoch loss\",\n",
    "    \"Training loss\": \"Training batch loss\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss 0 0.3135956823825836\n",
      "Training batch loss 20 0.20322471857070923\n",
      "Training batch loss 40 0.21193651854991913\n",
      "Training batch loss 60 0.20471489429473877\n",
      "Training batch loss 80 0.2039337009191513\n",
      "Training batch loss 100 0.20527958869934082\n",
      "Training batch loss 120 0.20113185048103333\n",
      "Training batch loss 140 0.19975367188453674\n",
      "Training batch loss 160 0.19449299573898315\n",
      "Training batch loss 180 0.20171234011650085\n",
      "Training batch loss 200 0.1982298493385315\n",
      "Training batch loss 220 0.20060361921787262\n",
      "Training batch loss 240 0.20291303098201752\n",
      "Training batch loss 260 0.19478058815002441\n",
      "Training batch loss 280 0.2046169638633728\n",
      "Training batch loss 300 0.18883202970027924\n",
      "Training batch loss 320 0.1986381560564041\n",
      "Training batch loss 340 0.2043256014585495\n",
      "Training batch loss 360 0.19653505086898804\n",
      "Training batch loss 380 0.20710082352161407\n",
      "Training batch loss 400 0.19963066279888153\n",
      "Training batch loss 420 0.1923399120569229\n",
      "Training batch loss 440 0.20434895157814026\n",
      "Training batch loss 460 0.1982041299343109\n",
      "Training batch loss 480 0.1964259147644043\n",
      "Training batch loss 500 0.20276162028312683\n",
      "Training batch loss 520 0.19995266199111938\n",
      "Training batch loss 540 0.20053374767303467\n",
      "Training batch loss 560 0.20419462025165558\n",
      "Training batch loss 580 0.19355568289756775\n",
      "Training batch loss 600 0.2048252820968628\n",
      "Training batch loss 620 0.19314610958099365\n",
      "Training batch loss 640 0.19375945627689362\n",
      "Training batch loss 660 0.1998109519481659\n",
      "Training batch loss 680 0.19788317382335663\n",
      "Training batch loss 700 0.20714040100574493\n",
      "Training batch loss 720 0.19833719730377197\n",
      "Training batch loss 740 0.20672592520713806\n",
      "Training batch loss 760 0.20096419751644135\n",
      "Training batch loss 780 0.20220455527305603\n",
      "Training batch loss 800 0.19973373413085938\n",
      "Training batch loss 820 0.19837722182273865\n",
      "Training batch loss 840 0.2077568769454956\n",
      "Training batch loss 860 0.20800678431987762\n",
      "Training batch loss 880 0.19870218634605408\n",
      "Training batch loss 900 0.2025786191225052\n",
      "Training batch loss 920 0.21476739645004272\n",
      "Training batch loss 940 0.198239266872406\n",
      "Training batch loss 960 0.2068939059972763\n",
      "Training batch loss 980 0.20437930524349213\n",
      "Training batch loss 1000 0.19694547355175018\n",
      "Training batch loss 1020 0.19438502192497253\n",
      "Training batch loss 1040 0.20146793127059937\n",
      "Training batch loss 1060 0.2046075463294983\n",
      "Training batch loss 1080 0.19441208243370056\n",
      "Training batch loss 1100 0.20671413838863373\n",
      "Training batch loss 1120 0.19916987419128418\n",
      "Training batch loss 1140 0.20158490538597107\n",
      "Training batch loss 1160 0.18888022005558014\n",
      "Training batch loss 1180 0.19998373091220856\n",
      "Training batch loss 1200 0.20493584871292114\n",
      "Training batch loss 1220 0.20006613433361053\n",
      "Training batch loss 1240 0.20484164357185364\n",
      "Training batch loss 1260 0.19267809391021729\n",
      "Training batch loss 1280 0.19703245162963867\n",
      "Training batch loss 1300 0.19661742448806763\n",
      "Training batch loss 1320 0.1951989084482193\n",
      "Training batch loss 1340 0.20196616649627686\n",
      "Training epoch loss 0 0.1997641623020172\n",
      "Test loss during training 0 0.19803737103939056\n",
      "Test accuracy during training 0 0.0\n",
      "Training batch loss 1355 0.19206300377845764\n",
      "Training batch loss 1375 0.19585442543029785\n",
      "Training batch loss 1395 0.20323409140110016\n",
      "Training batch loss 1415 0.1991167962551117\n",
      "Training batch loss 1435 0.19913436472415924\n",
      "Training batch loss 1455 0.20128130912780762\n",
      "Training batch loss 1475 0.19860893487930298\n",
      "Training batch loss 1495 0.1973905861377716\n",
      "Training batch loss 1515 0.19309601187705994\n",
      "Training batch loss 1535 0.19936364889144897\n",
      "Training batch loss 1555 0.19685204327106476\n",
      "Training batch loss 1575 0.19909918308258057\n",
      "Training batch loss 1595 0.20131562650203705\n",
      "Training batch loss 1615 0.19383665919303894\n",
      "Training batch loss 1635 0.20223544538021088\n",
      "Training batch loss 1655 0.18748793005943298\n",
      "Training batch loss 1675 0.19774551689624786\n",
      "Training batch loss 1695 0.20328116416931152\n",
      "Training batch loss 1715 0.1959361732006073\n",
      "Training batch loss 1735 0.20628002285957336\n",
      "Training batch loss 1755 0.19839394092559814\n",
      "Training batch loss 1775 0.191242516040802\n",
      "Training batch loss 1795 0.2040492296218872\n",
      "Training batch loss 1815 0.19746673107147217\n",
      "Training batch loss 1835 0.19586406648159027\n",
      "Training batch loss 1855 0.20188362896442413\n",
      "Training batch loss 1875 0.19930803775787354\n",
      "Training batch loss 1895 0.19898372888565063\n",
      "Training batch loss 1915 0.20378707349300385\n",
      "Training batch loss 1935 0.19292235374450684\n",
      "Training batch loss 1955 0.20408916473388672\n",
      "Training batch loss 1975 0.1932428479194641\n",
      "Training batch loss 1995 0.19375082850456238\n",
      "Training batch loss 2015 0.19977760314941406\n",
      "Training batch loss 2035 0.1970514953136444\n",
      "Training batch loss 2055 0.20662304759025574\n",
      "Training batch loss 2075 0.1974162757396698\n",
      "Training batch loss 2095 0.20620107650756836\n",
      "Training batch loss 2115 0.20036233961582184\n",
      "Training batch loss 2135 0.20180484652519226\n",
      "Training batch loss 2155 0.19950011372566223\n",
      "Training batch loss 2175 0.19827276468276978\n",
      "Training batch loss 2195 0.2064993977546692\n",
      "Training batch loss 2215 0.2077428102493286\n",
      "Training batch loss 2235 0.19850042462348938\n",
      "Training batch loss 2255 0.20175200700759888\n",
      "Training batch loss 2275 0.21360838413238525\n",
      "Training batch loss 2295 0.19792699813842773\n",
      "Training batch loss 2315 0.20610427856445312\n",
      "Training batch loss 2335 0.2041403353214264\n",
      "Training batch loss 2355 0.19669084250926971\n",
      "Training batch loss 2375 0.19395779073238373\n",
      "Training batch loss 2395 0.20087887346744537\n",
      "Training batch loss 2415 0.20434148609638214\n",
      "Training batch loss 2435 0.1941080093383789\n",
      "Training batch loss 2455 0.20673999190330505\n",
      "Training batch loss 2475 0.19871242344379425\n",
      "Training batch loss 2495 0.20160754024982452\n",
      "Training batch loss 2515 0.18824298679828644\n",
      "Training batch loss 2535 0.20003819465637207\n",
      "Training batch loss 2555 0.20517978072166443\n",
      "Training batch loss 2575 0.19991528987884521\n",
      "Training batch loss 2595 0.20474791526794434\n",
      "Training batch loss 2615 0.19171607494354248\n",
      "Training batch loss 2635 0.19670148193836212\n",
      "Training batch loss 2655 0.19608430564403534\n",
      "Training batch loss 2675 0.19531583786010742\n",
      "Training batch loss 2695 0.20151512324810028\n",
      "Training epoch loss 1 0.19867829978466034\n",
      "Test loss during training 1 0.19784316420555115\n",
      "Test accuracy during training 1 0.0\n",
      "Training batch loss 2710 0.1920328289270401\n",
      "Training batch loss 2730 0.19584199786186218\n",
      "Training batch loss 2750 0.20218704640865326\n",
      "Training batch loss 2770 0.19891932606697083\n",
      "Training batch loss 2790 0.19927996397018433\n",
      "Training batch loss 2810 0.20120367407798767\n",
      "Training batch loss 2830 0.19813908636569977\n",
      "Training batch loss 2850 0.1969117671251297\n",
      "Training batch loss 2870 0.1928461194038391\n",
      "Training batch loss 2890 0.19920635223388672\n",
      "Training batch loss 2910 0.19668817520141602\n",
      "Training batch loss 2930 0.19890613853931427\n",
      "Training batch loss 2950 0.20131826400756836\n",
      "Training batch loss 2970 0.1936383992433548\n",
      "Training batch loss 2990 0.2021791934967041\n",
      "Training batch loss 3010 0.18774297833442688\n",
      "Training batch loss 3030 0.19729214906692505\n",
      "Training batch loss 3050 0.20267660915851593\n",
      "Training batch loss 3070 0.19584888219833374\n",
      "Training batch loss 3090 0.20622223615646362\n",
      "Training batch loss 3110 0.1985616683959961\n",
      "Training batch loss 3130 0.19091644883155823\n",
      "Training batch loss 3150 0.20309552550315857\n",
      "Training batch loss 3170 0.197442889213562\n",
      "Training batch loss 3190 0.19553323090076447\n",
      "Training batch loss 3210 0.20160329341888428\n",
      "Training batch loss 3230 0.19892708957195282\n",
      "Training batch loss 3250 0.1987810730934143\n",
      "Training batch loss 3270 0.20411339402198792\n",
      "Training batch loss 3290 0.19242426753044128\n",
      "Training batch loss 3310 0.20345208048820496\n",
      "Training batch loss 3330 0.1926741898059845\n",
      "Training batch loss 3350 0.19331321120262146\n",
      "Training batch loss 3370 0.19955049455165863\n",
      "Training batch loss 3390 0.1964280903339386\n",
      "Training batch loss 3410 0.206244558095932\n",
      "Training batch loss 3430 0.19679972529411316\n",
      "Training batch loss 3450 0.20583096146583557\n",
      "Training batch loss 3470 0.19993841648101807\n",
      "Training batch loss 3490 0.20171907544136047\n",
      "Training batch loss 3510 0.19966340065002441\n",
      "Training batch loss 3530 0.1978684961795807\n",
      "Training batch loss 3550 0.20679393410682678\n",
      "Training batch loss 3570 0.20751738548278809\n",
      "Training batch loss 3590 0.19817893207073212\n",
      "Training batch loss 3610 0.20119927823543549\n",
      "Training batch loss 3630 0.21283112466335297\n",
      "Training batch loss 3650 0.19755908846855164\n",
      "Training batch loss 3670 0.20566916465759277\n",
      "Training batch loss 3690 0.20390716195106506\n",
      "Training batch loss 3710 0.1961970031261444\n",
      "Training batch loss 3730 0.19401663541793823\n",
      "Training batch loss 3750 0.20066380500793457\n",
      "Training batch loss 3770 0.2040942907333374\n",
      "Training batch loss 3790 0.19381779432296753\n",
      "Training batch loss 3810 0.2063664197921753\n",
      "Training batch loss 3830 0.1985676884651184\n",
      "Training batch loss 3850 0.2012571096420288\n",
      "Training batch loss 3870 0.18843939900398254\n",
      "Training batch loss 3890 0.19874510169029236\n",
      "Training batch loss 3910 0.20463700592517853\n",
      "Training batch loss 3930 0.19962036609649658\n",
      "Training batch loss 3950 0.2045051008462906\n",
      "Training batch loss 3970 0.19140900671482086\n",
      "Training batch loss 3990 0.19655264914035797\n",
      "Training batch loss 4010 0.1956838220357895\n",
      "Training batch loss 4030 0.19484034180641174\n",
      "Training batch loss 4050 0.20140710473060608\n",
      "Training epoch loss 2 0.1986009180545807\n",
      "Test loss during training 2 0.19764496386051178\n",
      "Test accuracy during training 2 0.0\n",
      "Training batch loss 4065 0.19178488850593567\n",
      "Training batch loss 4085 0.19583788514137268\n",
      "Training batch loss 4105 0.20133374631404877\n",
      "Training batch loss 4125 0.1983582079410553\n",
      "Training batch loss 4145 0.19918131828308105\n",
      "Training batch loss 4165 0.20095285773277283\n",
      "Training batch loss 4185 0.1980438232421875\n",
      "Training batch loss 4205 0.19715481996536255\n",
      "Training batch loss 4225 0.19260834157466888\n",
      "Training batch loss 4245 0.19880059361457825\n",
      "Training batch loss 4265 0.19632908701896667\n",
      "Training batch loss 4285 0.19872142374515533\n",
      "Training batch loss 4305 0.20110972225666046\n",
      "Training batch loss 4325 0.19334591925144196\n",
      "Training batch loss 4345 0.20116105675697327\n",
      "Training batch loss 4365 0.18688997626304626\n",
      "Training batch loss 4385 0.19738757610321045\n",
      "Training batch loss 4405 0.20211458206176758\n",
      "Training batch loss 4425 0.19543084502220154\n",
      "Training batch loss 4445 0.20585858821868896\n",
      "Training batch loss 4465 0.19723540544509888\n",
      "Training batch loss 4485 0.19043806195259094\n",
      "Training batch loss 4505 0.2028396874666214\n",
      "Training batch loss 4525 0.19655665755271912\n",
      "Training batch loss 4545 0.19556552171707153\n",
      "Training batch loss 4565 0.20146635174751282\n",
      "Training batch loss 4585 0.19889888167381287\n",
      "Training batch loss 4605 0.1984199732542038\n",
      "Training batch loss 4625 0.20389030873775482\n",
      "Training batch loss 4645 0.19226676225662231\n",
      "Training batch loss 4665 0.2035360336303711\n",
      "Training batch loss 4685 0.19258025288581848\n",
      "Training batch loss 4705 0.19312888383865356\n",
      "Training batch loss 4725 0.19947078824043274\n",
      "Training batch loss 4745 0.19628731906414032\n",
      "Training batch loss 4765 0.20586426556110382\n",
      "Training batch loss 4785 0.19682946801185608\n",
      "Training batch loss 4805 0.20596849918365479\n",
      "Training batch loss 4825 0.19977633655071259\n",
      "Training batch loss 4845 0.20147523283958435\n",
      "Training batch loss 4865 0.19950416684150696\n",
      "Training batch loss 4885 0.19796088337898254\n",
      "Training batch loss 4905 0.2061750292778015\n",
      "Training batch loss 4925 0.20745621621608734\n",
      "Training batch loss 4945 0.19798731803894043\n",
      "Training batch loss 4965 0.2013174146413803\n",
      "Training batch loss 4985 0.21274913847446442\n",
      "Training batch loss 5005 0.19735056161880493\n",
      "Training batch loss 5025 0.2057877480983734\n",
      "Training batch loss 5045 0.203861802816391\n",
      "Training batch loss 5065 0.19599980115890503\n",
      "Training batch loss 5085 0.1940610408782959\n",
      "Training batch loss 5105 0.20012862980365753\n",
      "Training batch loss 5125 0.20398634672164917\n",
      "Training batch loss 5145 0.1936894953250885\n",
      "Training batch loss 5165 0.20635241270065308\n",
      "Training batch loss 5185 0.19847460091114044\n",
      "Training batch loss 5205 0.20124362409114838\n",
      "Training batch loss 5225 0.18851247429847717\n",
      "Training batch loss 5245 0.19849786162376404\n",
      "Training batch loss 5265 0.20473235845565796\n",
      "Training batch loss 5285 0.19965004920959473\n",
      "Training batch loss 5305 0.20437780022621155\n",
      "Training batch loss 5325 0.1912764310836792\n",
      "Training batch loss 5345 0.196512371301651\n",
      "Training batch loss 5365 0.19570037722587585\n",
      "Training batch loss 5385 0.1947898119688034\n",
      "Training batch loss 5405 0.20144134759902954\n",
      "Training epoch loss 3 0.19857074320316315\n",
      "Test loss during training 3 0.1976664513349533\n",
      "Test accuracy during training 3 0.0\n",
      "Training batch loss 5420 0.1917003095149994\n",
      "Training batch loss 5440 0.19590085744857788\n",
      "Training batch loss 5460 0.20130908489227295\n",
      "Training batch loss 5480 0.1983065903186798\n",
      "Training batch loss 5500 0.19917170703411102\n",
      "Training batch loss 5520 0.20097887516021729\n",
      "Training batch loss 5540 0.19805817306041718\n",
      "Training batch loss 5560 0.19715023040771484\n",
      "Training batch loss 5580 0.19260317087173462\n",
      "Training batch loss 5600 0.19880220293998718\n",
      "Training batch loss 5620 0.19632604718208313\n",
      "Training batch loss 5640 0.19872203469276428\n",
      "Training batch loss 5660 0.20111091434955597\n",
      "Training batch loss 5680 0.19334378838539124\n",
      "Training batch loss 5700 0.20116139948368073\n",
      "Training batch loss 5720 0.18688790500164032\n",
      "Training batch loss 5740 0.1973869353532791\n",
      "Training batch loss 5760 0.20211748778820038\n",
      "Training batch loss 5780 0.19543084502220154\n",
      "Training batch loss 5800 0.2058580368757248\n",
      "Training batch loss 5820 0.19723232090473175\n",
      "Training batch loss 5840 0.19043956696987152\n",
      "Training batch loss 5860 0.20283858478069305\n",
      "Training batch loss 5880 0.19655540585517883\n",
      "Training batch loss 5900 0.1955660879611969\n",
      "Training batch loss 5920 0.20146524906158447\n",
      "Training batch loss 5940 0.19889965653419495\n",
      "Training batch loss 5960 0.19841960072517395\n",
      "Training batch loss 5980 0.20389118790626526\n",
      "Training batch loss 6000 0.19226807355880737\n",
      "Training batch loss 6020 0.20353808999061584\n",
      "Training batch loss 6040 0.1925780475139618\n",
      "Training batch loss 6060 0.19312989711761475\n",
      "Training batch loss 6080 0.19947361946105957\n",
      "Training batch loss 6100 0.19628657400608063\n",
      "Training batch loss 6120 0.2058638036251068\n",
      "Training batch loss 6140 0.19683045148849487\n",
      "Training batch loss 6160 0.20596954226493835\n",
      "Training batch loss 6180 0.19977594912052155\n",
      "Training batch loss 6200 0.20147430896759033\n",
      "Training batch loss 6220 0.1995045244693756\n",
      "Training batch loss 6240 0.1979614794254303\n",
      "Training batch loss 6260 0.20617514848709106\n",
      "Training batch loss 6280 0.20745590329170227\n",
      "Training batch loss 6300 0.1979876309633255\n",
      "Training batch loss 6320 0.2013189196586609\n",
      "Training batch loss 6340 0.2127508819103241\n",
      "Training batch loss 6360 0.19735071063041687\n",
      "Training batch loss 6380 0.20578761398792267\n",
      "Training batch loss 6400 0.20386266708374023\n",
      "Training batch loss 6420 0.19600000977516174\n",
      "Training batch loss 6440 0.19406010210514069\n",
      "Training batch loss 6460 0.2001291662454605\n",
      "Training batch loss 6480 0.2039862424135208\n",
      "Training batch loss 6500 0.1936904788017273\n",
      "Training batch loss 6520 0.20635277032852173\n",
      "Training batch loss 6540 0.19847360253334045\n",
      "Training batch loss 6560 0.20124441385269165\n",
      "Training batch loss 6580 0.18851244449615479\n",
      "Training batch loss 6600 0.19849783182144165\n",
      "Training batch loss 6620 0.2047313153743744\n",
      "Training batch loss 6640 0.19965024292469025\n",
      "Training batch loss 6660 0.20437724888324738\n",
      "Training batch loss 6680 0.19127681851387024\n",
      "Training batch loss 6700 0.19651293754577637\n",
      "Training batch loss 6720 0.19570118188858032\n",
      "Training batch loss 6740 0.1947896033525467\n",
      "Training batch loss 6760 0.20144084095954895\n",
      "Training epoch loss 4 0.19857187569141388\n",
      "Test loss during training 4 0.19766594469547272\n",
      "Test accuracy during training 4 0.0\n",
      "Training batch loss 6775 0.19170016050338745\n",
      "Training batch loss 6795 0.19590002298355103\n",
      "Training batch loss 6815 0.20131170749664307\n",
      "Training batch loss 6835 0.19830606877803802\n",
      "Training batch loss 6855 0.19917085766792297\n",
      "Training batch loss 6875 0.20097920298576355\n",
      "Training batch loss 6895 0.19805900752544403\n",
      "Training batch loss 6915 0.19715021550655365\n",
      "Training batch loss 6935 0.19260263442993164\n",
      "Training batch loss 6955 0.1988031268119812\n",
      "Training batch loss 6975 0.19632524251937866\n",
      "Training batch loss 6995 0.19872207939624786\n",
      "Training batch loss 7015 0.20111089944839478\n",
      "Training batch loss 7035 0.19334372878074646\n",
      "Training batch loss 7055 0.2011612504720688\n",
      "Training batch loss 7075 0.18688780069351196\n",
      "Training batch loss 7095 0.19738692045211792\n",
      "Training batch loss 7115 0.20211830735206604\n",
      "Training batch loss 7135 0.19543081521987915\n",
      "Training batch loss 7155 0.20585793256759644\n",
      "Training batch loss 7175 0.19723162055015564\n",
      "Training batch loss 7195 0.19044017791748047\n",
      "Training batch loss 7215 0.20283830165863037\n",
      "Training batch loss 7235 0.19655504822731018\n",
      "Training batch loss 7255 0.19556626677513123\n",
      "Training batch loss 7275 0.2014649659395218\n",
      "Training batch loss 7295 0.19889989495277405\n",
      "Training batch loss 7315 0.1984194815158844\n",
      "Training batch loss 7335 0.20389142632484436\n",
      "Training batch loss 7355 0.1922684907913208\n",
      "Training batch loss 7375 0.203538715839386\n",
      "Training batch loss 7395 0.19257742166519165\n",
      "Training batch loss 7415 0.19313015043735504\n",
      "Training batch loss 7435 0.19947442412376404\n",
      "Training batch loss 7455 0.19628635048866272\n",
      "Training batch loss 7475 0.20586372911930084\n",
      "Training batch loss 7495 0.19683074951171875\n",
      "Training batch loss 7515 0.20596981048583984\n",
      "Training batch loss 7535 0.1997758448123932\n",
      "Training batch loss 7555 0.20147410035133362\n",
      "Training batch loss 7575 0.19950462877750397\n",
      "Training batch loss 7595 0.19796165823936462\n",
      "Training batch loss 7615 0.20617517828941345\n",
      "Training batch loss 7635 0.2074558138847351\n",
      "Training batch loss 7655 0.19798770546913147\n",
      "Training batch loss 7675 0.2013193517923355\n",
      "Training batch loss 7695 0.2127513438463211\n",
      "Training batch loss 7715 0.19735080003738403\n",
      "Training batch loss 7735 0.2057875394821167\n",
      "Training batch loss 7755 0.20386290550231934\n",
      "Training batch loss 7775 0.1960000991821289\n",
      "Training batch loss 7795 0.1940598487854004\n",
      "Training batch loss 7815 0.20012931525707245\n",
      "Training batch loss 7835 0.20398621261119843\n",
      "Training batch loss 7855 0.19369077682495117\n",
      "Training batch loss 7875 0.20635288953781128\n",
      "Training batch loss 7895 0.19847333431243896\n",
      "Training batch loss 7915 0.20124462246894836\n",
      "Training batch loss 7935 0.18851247429847717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss 7955 0.19849780201911926\n",
      "Training batch loss 7975 0.2047310471534729\n",
      "Training batch loss 7995 0.19965028762817383\n",
      "Training batch loss 8015 0.20437711477279663\n",
      "Training batch loss 8035 0.1912769377231598\n",
      "Training batch loss 8055 0.1965130865573883\n",
      "Training batch loss 8075 0.19570137560367584\n",
      "Training batch loss 8095 0.19478954374790192\n",
      "Training batch loss 8115 0.2014407366514206\n",
      "Training epoch loss 5 0.19857195019721985\n",
      "Test loss during training 5 0.19766581058502197\n",
      "Test accuracy during training 5 0.0\n",
      "Training batch loss 8130 0.19170011579990387\n",
      "Training batch loss 8150 0.19589978456497192\n",
      "Training batch loss 8170 0.20131242275238037\n",
      "Training batch loss 8190 0.1983059048652649\n",
      "Training batch loss 8210 0.19917060434818268\n",
      "Training batch loss 8230 0.2009792923927307\n",
      "Training batch loss 8250 0.19805923104286194\n",
      "Training batch loss 8270 0.19715023040771484\n",
      "Training batch loss 8290 0.1926024705171585\n",
      "Training batch loss 8310 0.1988033652305603\n",
      "Training batch loss 8330 0.19632503390312195\n",
      "Training batch loss 8350 0.19872209429740906\n",
      "Training batch loss 8370 0.20111088454723358\n",
      "Training batch loss 8390 0.19334369897842407\n",
      "Training batch loss 8410 0.2011612057685852\n",
      "Training batch loss 8430 0.18688777089118958\n",
      "Training batch loss 8450 0.19738690555095673\n",
      "Training batch loss 8470 0.20211851596832275\n",
      "Training batch loss 8490 0.19543080031871796\n",
      "Training batch loss 8510 0.20585790276527405\n",
      "Training batch loss 8530 0.1972314417362213\n",
      "Training batch loss 8550 0.1904403567314148\n",
      "Training batch loss 8570 0.2028382122516632\n",
      "Training batch loss 8590 0.19655495882034302\n",
      "Training batch loss 8610 0.195566326379776\n",
      "Training batch loss 8630 0.20146487653255463\n",
      "Training batch loss 8650 0.19889996945858002\n",
      "Training batch loss 8670 0.198419451713562\n",
      "Training batch loss 8690 0.20389147102832794\n",
      "Training batch loss 8710 0.19226858019828796\n",
      "Training batch loss 8730 0.20353886485099792\n",
      "Training batch loss 8750 0.19257724285125732\n",
      "Training batch loss 8770 0.19313021004199982\n",
      "Training batch loss 8790 0.19947463274002075\n",
      "Training batch loss 8810 0.19628629088401794\n",
      "Training batch loss 8830 0.20586371421813965\n",
      "Training batch loss 8850 0.19683080911636353\n",
      "Training batch loss 8870 0.20596984028816223\n",
      "Training batch loss 8890 0.1997758448123932\n",
      "Training batch loss 8910 0.20147402584552765\n",
      "Training batch loss 8930 0.19950464367866516\n",
      "Training batch loss 8950 0.1979617029428482\n",
      "Training batch loss 8970 0.20617517828941345\n",
      "Training batch loss 8990 0.2074558138847351\n",
      "Training batch loss 9010 0.19798772037029266\n",
      "Training batch loss 9030 0.20131947100162506\n",
      "Training batch loss 9050 0.21275144815444946\n",
      "Training batch loss 9070 0.19735082983970642\n",
      "Training batch loss 9090 0.2057875394821167\n",
      "Training batch loss 9110 0.2038629651069641\n",
      "Training batch loss 9130 0.1960001289844513\n",
      "Training batch loss 9150 0.19405977427959442\n",
      "Training batch loss 9170 0.20012934505939484\n",
      "Training batch loss 9190 0.20398619771003723\n",
      "Training batch loss 9210 0.19369086623191833\n",
      "Training batch loss 9230 0.20635291934013367\n",
      "Training batch loss 9250 0.1984732747077942\n",
      "Training batch loss 9270 0.20124465227127075\n",
      "Training batch loss 9290 0.18851247429847717\n",
      "Training batch loss 9310 0.19849778711795807\n",
      "Training batch loss 9330 0.20473097264766693\n",
      "Training batch loss 9350 0.19965028762817383\n",
      "Training batch loss 9370 0.20437709987163544\n",
      "Training batch loss 9390 0.19127696752548218\n",
      "Training batch loss 9410 0.1965130865573883\n",
      "Training batch loss 9430 0.1957014501094818\n",
      "Training batch loss 9450 0.19478952884674072\n",
      "Training batch loss 9470 0.2014407068490982\n",
      "Training epoch loss 6 0.19857197999954224\n",
      "Test loss during training 6 0.19766570627689362\n",
      "Test accuracy during training 6 0.0\n",
      "Training batch loss 9485 0.19170010089874268\n",
      "Training batch loss 9505 0.19589972496032715\n",
      "Training batch loss 9525 0.2013126015663147\n",
      "Training batch loss 9545 0.1983058750629425\n",
      "Training batch loss 9565 0.1991705447435379\n",
      "Training batch loss 9585 0.2009793221950531\n",
      "Training batch loss 9605 0.19805927574634552\n",
      "Training batch loss 9625 0.19715021550655365\n",
      "Training batch loss 9645 0.19260242581367493\n",
      "Training batch loss 9665 0.19880343973636627\n",
      "Training batch loss 9685 0.19632498919963837\n",
      "Training batch loss 9705 0.19872212409973145\n",
      "Training batch loss 9725 0.2011108696460724\n",
      "Training batch loss 9745 0.19334368407726288\n",
      "Training batch loss 9765 0.201161190867424\n",
      "Training batch loss 9785 0.18688775599002838\n",
      "Training batch loss 9805 0.19738692045211792\n",
      "Training batch loss 9825 0.20211860537528992\n",
      "Training batch loss 9845 0.19543078541755676\n",
      "Training batch loss 9865 0.20585793256759644\n",
      "Training batch loss 9885 0.19723138213157654\n",
      "Training batch loss 9905 0.19044038653373718\n",
      "Training batch loss 9925 0.20283818244934082\n",
      "Training batch loss 9945 0.19655492901802063\n",
      "Training batch loss 9965 0.1955663561820984\n",
      "Training batch loss 9985 0.20146486163139343\n",
      "Training batch loss 10005 0.1988999843597412\n",
      "Training batch loss 10025 0.198419451713562\n",
      "Training batch loss 10045 0.20389148592948914\n",
      "Training batch loss 10065 0.19226861000061035\n",
      "Training batch loss 10085 0.2035389095544815\n",
      "Training batch loss 10105 0.19257721304893494\n",
      "Training batch loss 10125 0.193130224943161\n",
      "Training batch loss 10145 0.19947469234466553\n",
      "Training batch loss 10165 0.19628626108169556\n",
      "Training batch loss 10185 0.20586371421813965\n",
      "Training batch loss 10205 0.1968308389186859\n",
      "Training batch loss 10225 0.20596987009048462\n",
      "Training batch loss 10245 0.1997758150100708\n",
      "Training batch loss 10265 0.20147401094436646\n",
      "Training batch loss 10285 0.19950464367866516\n",
      "Training batch loss 10305 0.1979617178440094\n",
      "Training batch loss 10325 0.20617516338825226\n",
      "Training batch loss 10345 0.2074557989835739\n",
      "Training batch loss 10365 0.19798772037029266\n",
      "Training batch loss 10385 0.20131948590278625\n",
      "Training batch loss 10405 0.21275147795677185\n",
      "Training batch loss 10425 0.19735084474086761\n",
      "Training batch loss 10445 0.2057875394821167\n",
      "Training batch loss 10465 0.2038630098104477\n",
      "Training batch loss 10485 0.1960001289844513\n",
      "Training batch loss 10505 0.19405975937843323\n",
      "Training batch loss 10525 0.20012935996055603\n",
      "Training batch loss 10545 0.20398618280887604\n",
      "Training batch loss 10565 0.19369086623191833\n",
      "Training batch loss 10585 0.20635291934013367\n",
      "Training batch loss 10605 0.1984732449054718\n",
      "Training batch loss 10625 0.20124468207359314\n",
      "Training batch loss 10645 0.18851247429847717\n",
      "Training batch loss 10665 0.19849780201911926\n",
      "Training batch loss 10685 0.20473095774650574\n",
      "Training batch loss 10705 0.19965030252933502\n",
      "Training batch loss 10725 0.20437705516815186\n",
      "Training batch loss 10745 0.19127696752548218\n",
      "Training batch loss 10765 0.1965131014585495\n",
      "Training batch loss 10785 0.1957014501094818\n",
      "Training batch loss 10805 0.19478952884674072\n",
      "Training batch loss 10825 0.2014407068490982\n",
      "Training epoch loss 7 0.19857197999954224\n",
      "Test loss during training 7 0.1976657658815384\n",
      "Test accuracy during training 7 0.0\n",
      "Training batch loss 10840 0.19170010089874268\n",
      "Training batch loss 10860 0.19589969515800476\n",
      "Training batch loss 10880 0.20131264626979828\n",
      "Training batch loss 10900 0.19830584526062012\n",
      "Training batch loss 10920 0.1991705298423767\n",
      "Training batch loss 10940 0.2009793221950531\n",
      "Training batch loss 10960 0.1980592906475067\n",
      "Training batch loss 10980 0.19715023040771484\n",
      "Training batch loss 11000 0.19260242581367493\n",
      "Training batch loss 11020 0.19880345463752747\n",
      "Training batch loss 11040 0.19632497429847717\n",
      "Training batch loss 11060 0.19872212409973145\n",
      "Training batch loss 11080 0.2011108696460724\n",
      "Training batch loss 11100 0.19334366917610168\n",
      "Training batch loss 11120 0.2011612057685852\n",
      "Training batch loss 11140 0.18688777089118958\n",
      "Training batch loss 11160 0.19738690555095673\n",
      "Training batch loss 11180 0.20211859047412872\n",
      "Training batch loss 11200 0.19543078541755676\n",
      "Training batch loss 11220 0.20585793256759644\n",
      "Training batch loss 11240 0.19723136723041534\n",
      "Training batch loss 11260 0.19044040143489838\n",
      "Training batch loss 11280 0.20283819735050201\n",
      "Training batch loss 11300 0.19655489921569824\n",
      "Training batch loss 11320 0.1955663412809372\n",
      "Training batch loss 11340 0.20146483182907104\n",
      "Training batch loss 11360 0.1988999843597412\n",
      "Training batch loss 11380 0.19841943681240082\n",
      "Training batch loss 11400 0.20389148592948914\n",
      "Training batch loss 11420 0.19226861000061035\n",
      "Training batch loss 11440 0.2035389393568039\n",
      "Training batch loss 11460 0.19257719814777374\n",
      "Training batch loss 11480 0.193130224943161\n",
      "Training batch loss 11500 0.19947472214698792\n",
      "Training batch loss 11520 0.19628626108169556\n",
      "Training batch loss 11540 0.20586371421813965\n",
      "Training batch loss 11560 0.1968308389186859\n",
      "Training batch loss 11580 0.20596985518932343\n",
      "Training batch loss 11600 0.1997758150100708\n",
      "Training batch loss 11620 0.20147401094436646\n",
      "Training batch loss 11640 0.19950464367866516\n",
      "Training batch loss 11660 0.1979617178440094\n",
      "Training batch loss 11680 0.20617519319057465\n",
      "Training batch loss 11700 0.2074558138847351\n",
      "Training batch loss 11720 0.19798772037029266\n",
      "Training batch loss 11740 0.20131948590278625\n",
      "Training batch loss 11760 0.21275147795677185\n",
      "Training batch loss 11780 0.1973508596420288\n",
      "Training batch loss 11800 0.2057875394821167\n",
      "Training batch loss 11820 0.2038630247116089\n",
      "Training batch loss 11840 0.1960001289844513\n",
      "Training batch loss 11860 0.19405975937843323\n",
      "Training batch loss 11880 0.20012935996055603\n",
      "Training batch loss 11900 0.20398618280887604\n",
      "Training batch loss 11920 0.19369086623191833\n",
      "Training batch loss 11940 0.20635293424129486\n",
      "Training batch loss 11960 0.1984732449054718\n",
      "Training batch loss 11980 0.20124468207359314\n",
      "Training batch loss 12000 0.18851247429847717\n",
      "Training batch loss 12020 0.19849778711795807\n",
      "Training batch loss 12040 0.20473095774650574\n",
      "Training batch loss 12060 0.19965028762817383\n",
      "Training batch loss 12080 0.20437705516815186\n",
      "Training batch loss 12100 0.19127696752548218\n",
      "Training batch loss 12120 0.1965131014585495\n",
      "Training batch loss 12140 0.1957014501094818\n",
      "Training batch loss 12160 0.19478952884674072\n",
      "Training batch loss 12180 0.2014407068490982\n",
      "Training epoch loss 8 0.19857197999954224\n",
      "Test loss during training 8 0.1976657658815384\n",
      "Test accuracy during training 8 0.0\n",
      "Training batch loss 12195 0.19170010089874268\n",
      "Training batch loss 12215 0.19589971005916595\n",
      "Training batch loss 12235 0.20131264626979828\n",
      "Training batch loss 12255 0.1983058601617813\n",
      "Training batch loss 12275 0.1991705298423767\n",
      "Training batch loss 12295 0.2009793221950531\n",
      "Training batch loss 12315 0.1980593055486679\n",
      "Training batch loss 12335 0.19715023040771484\n",
      "Training batch loss 12355 0.19260241091251373\n",
      "Training batch loss 12375 0.19880345463752747\n",
      "Training batch loss 12395 0.19632497429847717\n",
      "Training batch loss 12415 0.19872212409973145\n",
      "Training batch loss 12435 0.2011108696460724\n",
      "Training batch loss 12455 0.19334366917610168\n",
      "Training batch loss 12475 0.2011612057685852\n",
      "Training batch loss 12495 0.18688777089118958\n",
      "Training batch loss 12515 0.19738690555095673\n",
      "Training batch loss 12535 0.20211859047412872\n",
      "Training batch loss 12555 0.19543080031871796\n",
      "Training batch loss 12575 0.20585793256759644\n",
      "Training batch loss 12595 0.19723135232925415\n",
      "Training batch loss 12615 0.19044041633605957\n",
      "Training batch loss 12635 0.20283818244934082\n",
      "Training batch loss 12655 0.19655489921569824\n",
      "Training batch loss 12675 0.1955663561820984\n",
      "Training batch loss 12695 0.20146483182907104\n",
      "Training batch loss 12715 0.1988999843597412\n",
      "Training batch loss 12735 0.19841943681240082\n",
      "Training batch loss 12755 0.20389148592948914\n",
      "Training batch loss 12775 0.19226862490177155\n",
      "Training batch loss 12795 0.2035389393568039\n",
      "Training batch loss 12815 0.19257719814777374\n",
      "Training batch loss 12835 0.193130224943161\n",
      "Training batch loss 12855 0.19947472214698792\n",
      "Training batch loss 12875 0.19628626108169556\n",
      "Training batch loss 12895 0.20586371421813965\n",
      "Training batch loss 12915 0.1968308389186859\n",
      "Training batch loss 12935 0.20596987009048462\n",
      "Training batch loss 12955 0.1997758150100708\n",
      "Training batch loss 12975 0.20147401094436646\n",
      "Training batch loss 12995 0.19950467348098755\n",
      "Training batch loss 13015 0.1979617178440094\n",
      "Training batch loss 13035 0.20617520809173584\n",
      "Training batch loss 13055 0.2074558138847351\n",
      "Training batch loss 13075 0.19798772037029266\n",
      "Training batch loss 13095 0.20131948590278625\n",
      "Training batch loss 13115 0.21275147795677185\n",
      "Training batch loss 13135 0.19735084474086761\n",
      "Training batch loss 13155 0.2057875394821167\n",
      "Training batch loss 13175 0.2038630247116089\n",
      "Training batch loss 13195 0.1960001289844513\n",
      "Training batch loss 13215 0.19405974447727203\n",
      "Training batch loss 13235 0.20012935996055603\n",
      "Training batch loss 13255 0.20398618280887604\n",
      "Training batch loss 13275 0.19369086623191833\n",
      "Training batch loss 13295 0.20635294914245605\n",
      "Training batch loss 13315 0.1984732449054718\n",
      "Training batch loss 13335 0.20124468207359314\n",
      "Training batch loss 13355 0.18851247429847717\n",
      "Training batch loss 13375 0.19849778711795807\n",
      "Training batch loss 13395 0.20473094284534454\n",
      "Training batch loss 13415 0.19965031743049622\n",
      "Training batch loss 13435 0.20437705516815186\n",
      "Training batch loss 13455 0.19127696752548218\n",
      "Training batch loss 13475 0.1965131014585495\n",
      "Training batch loss 13495 0.1957014501094818\n",
      "Training batch loss 13515 0.19478952884674072\n",
      "Training batch loss 13535 0.201440691947937\n",
      "Training epoch loss 9 0.19857197999954224\n",
      "Test loss during training 9 0.1976657658815384\n",
      "Test accuracy during training 9 0.0\n",
      "Training batch loss 13550 0.19170010089874268\n",
      "Training batch loss 13570 0.19589971005916595\n",
      "Training batch loss 13590 0.20131264626979828\n",
      "Training batch loss 13610 0.1983058601617813\n",
      "Training batch loss 13630 0.1991705298423767\n",
      "Training batch loss 13650 0.2009793221950531\n",
      "Training batch loss 13670 0.1980592906475067\n",
      "Training batch loss 13690 0.19715023040771484\n",
      "Training batch loss 13710 0.19260242581367493\n",
      "Training batch loss 13730 0.19880345463752747\n",
      "Training batch loss 13750 0.19632495939731598\n",
      "Training batch loss 13770 0.19872212409973145\n",
      "Training batch loss 13790 0.2011108696460724\n",
      "Training batch loss 13810 0.19334366917610168\n",
      "Training batch loss 13830 0.2011612057685852\n",
      "Training batch loss 13850 0.18688777089118958\n",
      "Training batch loss 13870 0.19738690555095673\n",
      "Training batch loss 13890 0.20211859047412872\n",
      "Training batch loss 13910 0.19543080031871796\n",
      "Training batch loss 13930 0.20585793256759644\n",
      "Training batch loss 13950 0.19723135232925415\n",
      "Training batch loss 13970 0.19044041633605957\n",
      "Training batch loss 13990 0.20283818244934082\n",
      "Training batch loss 14010 0.19655489921569824\n",
      "Training batch loss 14030 0.1955663561820984\n",
      "Training batch loss 14050 0.20146483182907104\n",
      "Training batch loss 14070 0.1988999843597412\n",
      "Training batch loss 14090 0.198419451713562\n",
      "Training batch loss 14110 0.20389148592948914\n",
      "Training batch loss 14130 0.19226862490177155\n",
      "Training batch loss 14150 0.2035389393568039\n",
      "Training batch loss 14170 0.19257719814777374\n",
      "Training batch loss 14190 0.1931302398443222\n",
      "Training batch loss 14210 0.19947472214698792\n",
      "Training batch loss 14230 0.19628626108169556\n",
      "Training batch loss 14250 0.20586371421813965\n",
      "Training batch loss 14270 0.1968308389186859\n",
      "Training batch loss 14290 0.20596987009048462\n",
      "Training batch loss 14310 0.1997758150100708\n",
      "Training batch loss 14330 0.20147401094436646\n",
      "Training batch loss 14350 0.19950464367866516\n",
      "Training batch loss 14370 0.1979617178440094\n",
      "Training batch loss 14390 0.20617517828941345\n",
      "Training batch loss 14410 0.2074558138847351\n",
      "Training batch loss 14430 0.19798772037029266\n",
      "Training batch loss 14450 0.20131948590278625\n",
      "Training batch loss 14470 0.21275147795677185\n",
      "Training batch loss 14490 0.1973508596420288\n",
      "Training batch loss 14510 0.2057875394821167\n",
      "Training batch loss 14530 0.2038630247116089\n",
      "Training batch loss 14550 0.1960001289844513\n",
      "Training batch loss 14570 0.19405975937843323\n",
      "Training batch loss 14590 0.20012935996055603\n",
      "Training batch loss 14610 0.20398616790771484\n",
      "Training batch loss 14630 0.19369086623191833\n",
      "Training batch loss 14650 0.20635294914245605\n",
      "Training batch loss 14670 0.1984732449054718\n",
      "Training batch loss 14690 0.20124468207359314\n",
      "Training batch loss 14710 0.18851247429847717\n",
      "Training batch loss 14730 0.19849780201911926\n",
      "Training batch loss 14750 0.20473094284534454\n",
      "Training batch loss 14770 0.19965031743049622\n",
      "Training batch loss 14790 0.20437705516815186\n",
      "Training batch loss 14810 0.19127696752548218\n",
      "Training batch loss 14830 0.1965131163597107\n",
      "Training batch loss 14850 0.1957014501094818\n",
      "Training batch loss 14870 0.19478952884674072\n",
      "Training batch loss 14890 0.201440691947937\n",
      "Training epoch loss 10 0.19857197999954224\n",
      "Test loss during training 10 0.1976657658815384\n",
      "Test accuracy during training 10 0.0\n",
      "Training batch loss 14905 0.19170010089874268\n",
      "Training batch loss 14925 0.19589971005916595\n",
      "Training batch loss 14945 0.20131266117095947\n",
      "Training batch loss 14965 0.1983058601617813\n",
      "Training batch loss 14985 0.1991705298423767\n",
      "Training batch loss 15005 0.2009793221950531\n",
      "Training batch loss 15025 0.1980592906475067\n",
      "Training batch loss 15045 0.19715023040771484\n",
      "Training batch loss 15065 0.19260242581367493\n",
      "Training batch loss 15085 0.19880345463752747\n",
      "Training batch loss 15105 0.19632495939731598\n",
      "Training batch loss 15125 0.19872212409973145\n",
      "Training batch loss 15145 0.2011108696460724\n",
      "Training batch loss 15165 0.19334366917610168\n",
      "Training batch loss 15185 0.2011612057685852\n",
      "Training batch loss 15205 0.18688777089118958\n",
      "Training batch loss 15225 0.19738690555095673\n",
      "Training batch loss 15245 0.20211857557296753\n",
      "Training batch loss 15265 0.19543080031871796\n",
      "Training batch loss 15285 0.20585793256759644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss 15305 0.19723135232925415\n",
      "Training batch loss 15325 0.19044041633605957\n",
      "Training batch loss 15345 0.20283818244934082\n",
      "Training batch loss 15365 0.19655489921569824\n",
      "Training batch loss 15385 0.1955663561820984\n",
      "Training batch loss 15405 0.20146483182907104\n",
      "Training batch loss 15425 0.1988999843597412\n",
      "Training batch loss 15445 0.19841943681240082\n",
      "Training batch loss 15465 0.20389148592948914\n",
      "Training batch loss 15485 0.19226862490177155\n",
      "Training batch loss 15505 0.2035389393568039\n",
      "Training batch loss 15525 0.19257719814777374\n",
      "Training batch loss 15545 0.1931302547454834\n",
      "Training batch loss 15565 0.19947472214698792\n",
      "Training batch loss 15585 0.19628626108169556\n",
      "Training batch loss 15605 0.20586371421813965\n",
      "Training batch loss 15625 0.1968308389186859\n",
      "Training batch loss 15645 0.20596987009048462\n",
      "Training batch loss 15665 0.1997758150100708\n",
      "Training batch loss 15685 0.20147401094436646\n",
      "Training batch loss 15705 0.19950464367866516\n",
      "Training batch loss 15725 0.1979617178440094\n",
      "Training batch loss 15745 0.20617517828941345\n",
      "Training batch loss 15765 0.2074558138847351\n",
      "Training batch loss 15785 0.19798772037029266\n",
      "Training batch loss 15805 0.20131948590278625\n",
      "Training batch loss 15825 0.21275147795677185\n",
      "Training batch loss 15845 0.19735084474086761\n",
      "Training batch loss 15865 0.2057875394821167\n",
      "Training batch loss 15885 0.2038630247116089\n",
      "Training batch loss 15905 0.1960001289844513\n",
      "Training batch loss 15925 0.19405975937843323\n",
      "Training batch loss 15945 0.20012935996055603\n",
      "Training batch loss 15965 0.20398616790771484\n",
      "Training batch loss 15985 0.19369086623191833\n",
      "Training batch loss 16005 0.20635294914245605\n",
      "Training batch loss 16025 0.1984732449054718\n",
      "Training batch loss 16045 0.20124468207359314\n",
      "Training batch loss 16065 0.18851247429847717\n",
      "Training batch loss 16085 0.19849780201911926\n",
      "Training batch loss 16105 0.20473094284534454\n",
      "Training batch loss 16125 0.19965031743049622\n",
      "Training batch loss 16145 0.20437705516815186\n",
      "Training batch loss 16165 0.19127696752548218\n",
      "Training batch loss 16185 0.1965131163597107\n",
      "Training batch loss 16205 0.1957014501094818\n",
      "Training batch loss 16225 0.19478952884674072\n",
      "Training batch loss 16245 0.201440691947937\n",
      "Training epoch loss 11 0.19857197999954224\n",
      "Test loss during training 11 0.1976657658815384\n",
      "Test accuracy during training 11 0.0\n",
      "Training batch loss 16260 0.19170010089874268\n",
      "Training batch loss 16280 0.19589971005916595\n",
      "Training batch loss 16300 0.20131266117095947\n",
      "Training batch loss 16320 0.1983058601617813\n",
      "Training batch loss 16340 0.1991705298423767\n",
      "Training batch loss 16360 0.2009793221950531\n",
      "Training batch loss 16380 0.1980592906475067\n",
      "Training batch loss 16400 0.19715023040771484\n",
      "Training batch loss 16420 0.19260242581367493\n",
      "Training batch loss 16440 0.19880345463752747\n",
      "Training batch loss 16460 0.19632495939731598\n",
      "Training batch loss 16480 0.19872212409973145\n",
      "Training batch loss 16500 0.2011108696460724\n",
      "Training batch loss 16520 0.19334366917610168\n",
      "Training batch loss 16540 0.2011612057685852\n",
      "Training batch loss 16560 0.18688777089118958\n",
      "Training batch loss 16580 0.19738690555095673\n",
      "Training batch loss 16600 0.20211857557296753\n",
      "Training batch loss 16620 0.19543081521987915\n",
      "Training batch loss 16640 0.20585793256759644\n",
      "Training batch loss 16660 0.19723135232925415\n",
      "Training batch loss 16680 0.19044041633605957\n",
      "Training batch loss 16700 0.20283818244934082\n",
      "Training batch loss 16720 0.19655489921569824\n",
      "Training batch loss 16740 0.1955663561820984\n",
      "Training batch loss 16760 0.20146483182907104\n",
      "Training batch loss 16780 0.1988999843597412\n",
      "Training batch loss 16800 0.19841943681240082\n",
      "Training batch loss 16820 0.20389148592948914\n",
      "Training batch loss 16840 0.19226862490177155\n",
      "Training batch loss 16860 0.2035389393568039\n",
      "Training batch loss 16880 0.19257719814777374\n",
      "Training batch loss 16900 0.1931302547454834\n",
      "Training batch loss 16920 0.19947472214698792\n",
      "Training batch loss 16940 0.19628626108169556\n",
      "Training batch loss 16960 0.20586371421813965\n",
      "Training batch loss 16980 0.1968308389186859\n",
      "Training batch loss 17000 0.20596987009048462\n",
      "Training batch loss 17020 0.1997758150100708\n",
      "Training batch loss 17040 0.20147401094436646\n",
      "Training batch loss 17060 0.19950464367866516\n",
      "Training batch loss 17080 0.1979617178440094\n",
      "Training batch loss 17100 0.20617517828941345\n",
      "Training batch loss 17120 0.2074558138847351\n",
      "Training batch loss 17140 0.19798772037029266\n",
      "Training batch loss 17160 0.20131948590278625\n",
      "Training batch loss 17180 0.21275147795677185\n",
      "Training batch loss 17200 0.1973508596420288\n",
      "Training batch loss 17220 0.2057875394821167\n",
      "Training batch loss 17240 0.2038630247116089\n",
      "Training batch loss 17260 0.1960001289844513\n",
      "Training batch loss 17280 0.19405975937843323\n",
      "Training batch loss 17300 0.20012935996055603\n",
      "Training batch loss 17320 0.20398616790771484\n",
      "Training batch loss 17340 0.19369086623191833\n",
      "Training batch loss 17360 0.20635294914245605\n",
      "Training batch loss 17380 0.1984732449054718\n",
      "Training batch loss 17400 0.20124468207359314\n",
      "Training batch loss 17420 0.18851247429847717\n",
      "Training batch loss 17440 0.19849780201911926\n",
      "Training batch loss 17460 0.20473094284534454\n",
      "Training batch loss 17480 0.19965031743049622\n",
      "Training batch loss 17500 0.20437705516815186\n",
      "Training batch loss 17520 0.19127696752548218\n",
      "Training batch loss 17540 0.1965131163597107\n",
      "Training batch loss 17560 0.195701465010643\n",
      "Training batch loss 17580 0.19478952884674072\n",
      "Training batch loss 17600 0.201440691947937\n",
      "Training epoch loss 12 0.19857197999954224\n",
      "Test loss during training 12 0.1976657658815384\n",
      "Test accuracy during training 12 0.0\n",
      "Training batch loss 17615 0.19170010089874268\n",
      "Training batch loss 17635 0.19589971005916595\n",
      "Training batch loss 17655 0.20131266117095947\n",
      "Training batch loss 17675 0.1983058601617813\n",
      "Training batch loss 17695 0.1991705298423767\n",
      "Training batch loss 17715 0.2009793221950531\n",
      "Training batch loss 17735 0.1980592906475067\n",
      "Training batch loss 17755 0.19715023040771484\n",
      "Training batch loss 17775 0.19260242581367493\n",
      "Training batch loss 17795 0.19880345463752747\n",
      "Training batch loss 17815 0.19632495939731598\n",
      "Training batch loss 17835 0.19872212409973145\n",
      "Training batch loss 17855 0.2011108696460724\n",
      "Training batch loss 17875 0.19334366917610168\n",
      "Training batch loss 17895 0.2011612057685852\n",
      "Training batch loss 17915 0.18688777089118958\n",
      "Training batch loss 17935 0.19738690555095673\n",
      "Training batch loss 17955 0.20211857557296753\n",
      "Training batch loss 17975 0.19543080031871796\n",
      "Training batch loss 17995 0.20585793256759644\n",
      "Training batch loss 18015 0.19723135232925415\n",
      "Training batch loss 18035 0.19044041633605957\n",
      "Training batch loss 18055 0.20283818244934082\n",
      "Training batch loss 18075 0.19655489921569824\n",
      "Training batch loss 18095 0.1955663561820984\n",
      "Training batch loss 18115 0.20146483182907104\n",
      "Training batch loss 18135 0.1988999843597412\n",
      "Training batch loss 18155 0.19841943681240082\n",
      "Training batch loss 18175 0.20389148592948914\n",
      "Training batch loss 18195 0.19226862490177155\n",
      "Training batch loss 18215 0.2035389244556427\n",
      "Training batch loss 18235 0.19257719814777374\n",
      "Training batch loss 18255 0.1931302547454834\n",
      "Training batch loss 18275 0.19947470724582672\n",
      "Training batch loss 18295 0.19628626108169556\n",
      "Training batch loss 18315 0.20586371421813965\n",
      "Training batch loss 18335 0.1968308389186859\n",
      "Training batch loss 18355 0.20596987009048462\n",
      "Training batch loss 18375 0.1997758150100708\n",
      "Training batch loss 18395 0.20147401094436646\n",
      "Training batch loss 18415 0.19950464367866516\n",
      "Training batch loss 18435 0.1979617178440094\n",
      "Training batch loss 18455 0.20617517828941345\n",
      "Training batch loss 18475 0.2074558138847351\n",
      "Training batch loss 18495 0.19798772037029266\n",
      "Training batch loss 18515 0.20131948590278625\n",
      "Training batch loss 18535 0.21275150775909424\n",
      "Training batch loss 18555 0.19735084474086761\n",
      "Training batch loss 18575 0.2057875394821167\n",
      "Training batch loss 18595 0.2038630247116089\n",
      "Training batch loss 18615 0.1960001289844513\n",
      "Training batch loss 18635 0.19405975937843323\n",
      "Training batch loss 18655 0.20012935996055603\n",
      "Training batch loss 18675 0.20398616790771484\n",
      "Training batch loss 18695 0.19369086623191833\n",
      "Training batch loss 18715 0.20635294914245605\n",
      "Training batch loss 18735 0.1984732449054718\n",
      "Training batch loss 18755 0.20124468207359314\n",
      "Training batch loss 18775 0.18851247429847717\n",
      "Training batch loss 18795 0.19849780201911926\n",
      "Training batch loss 18815 0.20473094284534454\n",
      "Training batch loss 18835 0.19965031743049622\n",
      "Training batch loss 18855 0.20437705516815186\n",
      "Training batch loss 18875 0.19127696752548218\n",
      "Training batch loss 18895 0.1965131163597107\n",
      "Training batch loss 18915 0.195701465010643\n",
      "Training batch loss 18935 0.19478952884674072\n",
      "Training batch loss 18955 0.201440691947937\n",
      "Training epoch loss 13 0.19857197999954224\n",
      "Test loss during training 13 0.1976657658815384\n",
      "Test accuracy during training 13 0.0\n",
      "Training batch loss 18970 0.19170010089874268\n",
      "Training batch loss 18990 0.19589971005916595\n",
      "Training batch loss 19010 0.20131264626979828\n",
      "Training batch loss 19030 0.1983058601617813\n",
      "Training batch loss 19050 0.1991705298423767\n",
      "Training batch loss 19070 0.2009793221950531\n",
      "Training batch loss 19090 0.1980592906475067\n",
      "Training batch loss 19110 0.19715023040771484\n",
      "Training batch loss 19130 0.19260242581367493\n",
      "Training batch loss 19150 0.19880345463752747\n",
      "Training batch loss 19170 0.19632495939731598\n",
      "Training batch loss 19190 0.19872212409973145\n",
      "Training batch loss 19210 0.2011108696460724\n",
      "Training batch loss 19230 0.19334366917610168\n",
      "Training batch loss 19250 0.2011612057685852\n",
      "Training batch loss 19270 0.18688777089118958\n",
      "Training batch loss 19290 0.19738690555095673\n",
      "Training batch loss 19310 0.20211857557296753\n",
      "Training batch loss 19330 0.19543080031871796\n",
      "Training batch loss 19350 0.20585793256759644\n",
      "Training batch loss 19370 0.19723135232925415\n",
      "Training batch loss 19390 0.19044041633605957\n",
      "Training batch loss 19410 0.20283818244934082\n",
      "Training batch loss 19430 0.19655489921569824\n",
      "Training batch loss 19450 0.1955663561820984\n",
      "Training batch loss 19470 0.20146483182907104\n",
      "Training batch loss 19490 0.1988999843597412\n",
      "Training batch loss 19510 0.19841943681240082\n",
      "Training batch loss 19530 0.20389148592948914\n",
      "Training batch loss 19550 0.19226862490177155\n",
      "Training batch loss 19570 0.2035389244556427\n",
      "Training batch loss 19590 0.19257719814777374\n",
      "Training batch loss 19610 0.1931302547454834\n",
      "Training batch loss 19630 0.19947470724582672\n",
      "Training batch loss 19650 0.19628626108169556\n",
      "Training batch loss 19670 0.20586371421813965\n",
      "Training batch loss 19690 0.1968308389186859\n",
      "Training batch loss 19710 0.20596987009048462\n",
      "Training batch loss 19730 0.1997758150100708\n",
      "Training batch loss 19750 0.20147401094436646\n",
      "Training batch loss 19770 0.19950464367866516\n",
      "Training batch loss 19790 0.1979617178440094\n",
      "Training batch loss 19810 0.20617517828941345\n",
      "Training batch loss 19830 0.2074558138847351\n",
      "Training batch loss 19850 0.19798772037029266\n",
      "Training batch loss 19870 0.20131948590278625\n",
      "Training batch loss 19890 0.21275150775909424\n",
      "Training batch loss 19910 0.19735084474086761\n",
      "Training batch loss 19930 0.2057875394821167\n",
      "Training batch loss 19950 0.2038630247116089\n",
      "Training batch loss 19970 0.1960001289844513\n",
      "Training batch loss 19990 0.19405975937843323\n",
      "Training batch loss 20010 0.20012935996055603\n",
      "Training batch loss 20030 0.20398616790771484\n",
      "Training batch loss 20050 0.19369086623191833\n",
      "Training batch loss 20070 0.20635294914245605\n",
      "Training batch loss 20090 0.1984732449054718\n",
      "Training batch loss 20110 0.20124468207359314\n",
      "Training batch loss 20130 0.18851247429847717\n",
      "Training batch loss 20150 0.19849780201911926\n",
      "Training batch loss 20170 0.20473094284534454\n",
      "Training batch loss 20190 0.19965031743049622\n",
      "Training batch loss 20210 0.20437705516815186\n",
      "Training batch loss 20230 0.19127696752548218\n",
      "Training batch loss 20250 0.1965131014585495\n",
      "Training batch loss 20270 0.195701465010643\n",
      "Training batch loss 20290 0.19478952884674072\n",
      "Training batch loss 20310 0.201440691947937\n",
      "Training epoch loss 14 0.19857197999954224\n",
      "Test loss during training 14 0.1976657658815384\n",
      "Test accuracy during training 14 0.0\n",
      "Training batch loss 20325 0.19170010089874268\n",
      "Training batch loss 20345 0.19589971005916595\n",
      "Training batch loss 20365 0.20131264626979828\n",
      "Training batch loss 20385 0.1983058601617813\n",
      "Training batch loss 20405 0.1991705298423767\n",
      "Training batch loss 20425 0.2009793221950531\n",
      "Training batch loss 20445 0.1980592906475067\n",
      "Training batch loss 20465 0.19715023040771484\n",
      "Training batch loss 20485 0.19260242581367493\n",
      "Training batch loss 20505 0.19880345463752747\n",
      "Training batch loss 20525 0.19632495939731598\n",
      "Training batch loss 20545 0.19872212409973145\n",
      "Training batch loss 20565 0.2011108696460724\n",
      "Training batch loss 20585 0.19334366917610168\n",
      "Training batch loss 20605 0.2011612057685852\n",
      "Training batch loss 20625 0.18688777089118958\n",
      "Training batch loss 20645 0.19738690555095673\n",
      "Training batch loss 20665 0.20211857557296753\n",
      "Training batch loss 20685 0.19543080031871796\n",
      "Training batch loss 20705 0.20585793256759644\n",
      "Training batch loss 20725 0.19723135232925415\n",
      "Training batch loss 20745 0.19044041633605957\n",
      "Training batch loss 20765 0.20283818244934082\n",
      "Training batch loss 20785 0.19655489921569824\n",
      "Training batch loss 20805 0.1955663561820984\n",
      "Training batch loss 20825 0.20146483182907104\n",
      "Training batch loss 20845 0.1988999843597412\n",
      "Training batch loss 20865 0.19841943681240082\n",
      "Training batch loss 20885 0.20389148592948914\n",
      "Training batch loss 20905 0.19226862490177155\n",
      "Training batch loss 20925 0.2035389244556427\n",
      "Training batch loss 20945 0.19257719814777374\n",
      "Training batch loss 20965 0.1931302547454834\n",
      "Training batch loss 20985 0.19947470724582672\n",
      "Training batch loss 21005 0.19628626108169556\n",
      "Training batch loss 21025 0.20586371421813965\n",
      "Training batch loss 21045 0.1968308389186859\n",
      "Training batch loss 21065 0.20596987009048462\n",
      "Training batch loss 21085 0.1997758150100708\n",
      "Training batch loss 21105 0.20147401094436646\n",
      "Training batch loss 21125 0.19950464367866516\n",
      "Training batch loss 21145 0.1979617178440094\n",
      "Training batch loss 21165 0.20617517828941345\n",
      "Training batch loss 21185 0.2074558138847351\n",
      "Training batch loss 21205 0.19798772037029266\n",
      "Training batch loss 21225 0.20131948590278625\n",
      "Training batch loss 21245 0.21275150775909424\n",
      "Training batch loss 21265 0.19735084474086761\n",
      "Training batch loss 21285 0.2057875394821167\n",
      "Training batch loss 21305 0.2038630247116089\n",
      "Training batch loss 21325 0.1960001289844513\n",
      "Training batch loss 21345 0.19405975937843323\n",
      "Training batch loss 21365 0.20012935996055603\n",
      "Training batch loss 21385 0.20398616790771484\n",
      "Training batch loss 21405 0.19369086623191833\n",
      "Training batch loss 21425 0.20635294914245605\n",
      "Training batch loss 21445 0.1984732449054718\n",
      "Training batch loss 21465 0.20124468207359314\n",
      "Training batch loss 21485 0.18851247429847717\n",
      "Training batch loss 21505 0.19849780201911926\n",
      "Training batch loss 21525 0.20473094284534454\n",
      "Training batch loss 21545 0.19965031743049622\n",
      "Training batch loss 21565 0.20437705516815186\n",
      "Training batch loss 21585 0.19127696752548218\n",
      "Training batch loss 21605 0.1965131014585495\n",
      "Training batch loss 21625 0.195701465010643\n",
      "Training batch loss 21645 0.19478952884674072\n",
      "Training batch loss 21665 0.201440691947937\n",
      "Training epoch loss 15 0.19857197999954224\n",
      "Test loss during training 15 0.1976657658815384\n",
      "Test accuracy during training 15 0.0\n",
      "Training batch loss 21680 0.19170010089874268\n",
      "Training batch loss 21700 0.19589971005916595\n",
      "Training batch loss 21720 0.20131264626979828\n",
      "Training batch loss 21740 0.1983058601617813\n",
      "Training batch loss 21760 0.1991705298423767\n",
      "Training batch loss 21780 0.2009793221950531\n",
      "Training batch loss 21800 0.1980592906475067\n",
      "Training batch loss 21820 0.19715023040771484\n",
      "Training batch loss 21840 0.19260242581367493\n",
      "Training batch loss 21860 0.19880345463752747\n",
      "Training batch loss 21880 0.19632495939731598\n",
      "Training batch loss 21900 0.19872212409973145\n",
      "Training batch loss 21920 0.2011108696460724\n",
      "Training batch loss 21940 0.19334366917610168\n",
      "Training batch loss 21960 0.2011612057685852\n",
      "Training batch loss 21980 0.18688777089118958\n",
      "Training batch loss 22000 0.19738690555095673\n",
      "Training batch loss 22020 0.20211857557296753\n",
      "Training batch loss 22040 0.19543080031871796\n",
      "Training batch loss 22060 0.20585793256759644\n",
      "Training batch loss 22080 0.19723135232925415\n",
      "Training batch loss 22100 0.19044041633605957\n",
      "Training batch loss 22120 0.20283818244934082\n",
      "Training batch loss 22140 0.19655489921569824\n",
      "Training batch loss 22160 0.1955663561820984\n",
      "Training batch loss 22180 0.20146483182907104\n",
      "Training batch loss 22200 0.1988999843597412\n",
      "Training batch loss 22220 0.19841943681240082\n",
      "Training batch loss 22240 0.20389148592948914\n",
      "Training batch loss 22260 0.19226862490177155\n",
      "Training batch loss 22280 0.2035389244556427\n",
      "Training batch loss 22300 0.19257719814777374\n",
      "Training batch loss 22320 0.1931302547454834\n",
      "Training batch loss 22340 0.19947470724582672\n",
      "Training batch loss 22360 0.19628626108169556\n",
      "Training batch loss 22380 0.20586371421813965\n",
      "Training batch loss 22400 0.1968308389186859\n",
      "Training batch loss 22420 0.20596987009048462\n",
      "Training batch loss 22440 0.1997758150100708\n",
      "Training batch loss 22460 0.20147401094436646\n",
      "Training batch loss 22480 0.19950464367866516\n",
      "Training batch loss 22500 0.1979617178440094\n",
      "Training batch loss 22520 0.20617517828941345\n",
      "Training batch loss 22540 0.2074558138847351\n",
      "Training batch loss 22560 0.19798772037029266\n",
      "Training batch loss 22580 0.20131948590278625\n",
      "Training batch loss 22600 0.21275150775909424\n",
      "Training batch loss 22620 0.19735084474086761\n",
      "Training batch loss 22640 0.2057875394821167\n",
      "Training batch loss 22660 0.2038630247116089\n",
      "Training batch loss 22680 0.1960001289844513\n",
      "Training batch loss 22700 0.19405975937843323\n",
      "Training batch loss 22720 0.20012935996055603\n",
      "Training batch loss 22740 0.20398616790771484\n",
      "Training batch loss 22760 0.19369086623191833\n",
      "Training batch loss 22780 0.20635294914245605\n",
      "Training batch loss 22800 0.1984732449054718\n",
      "Training batch loss 22820 0.20124468207359314\n",
      "Training batch loss 22840 0.18851247429847717\n",
      "Training batch loss 22860 0.19849780201911926\n",
      "Training batch loss 22880 0.20473094284534454\n",
      "Training batch loss 22900 0.19965031743049622\n",
      "Training batch loss 22920 0.20437705516815186\n",
      "Training batch loss 22940 0.19127696752548218\n",
      "Training batch loss 22960 0.1965131014585495\n",
      "Training batch loss 22980 0.195701465010643\n",
      "Training batch loss 23000 0.19478952884674072\n",
      "Training batch loss 23020 0.201440691947937\n",
      "Training epoch loss 16 0.19857197999954224\n",
      "Test loss during training 16 0.1976657658815384\n",
      "Test accuracy during training 16 0.0\n",
      "Training batch loss 23035 0.19170010089874268\n",
      "Training batch loss 23055 0.19589971005916595\n",
      "Training batch loss 23075 0.20131264626979828\n",
      "Training batch loss 23095 0.1983058601617813\n",
      "Training batch loss 23115 0.1991705298423767\n",
      "Training batch loss 23135 0.2009793221950531\n",
      "Training batch loss 23155 0.1980592906475067\n",
      "Training batch loss 23175 0.19715023040771484\n",
      "Training batch loss 23195 0.19260242581367493\n",
      "Training batch loss 23215 0.19880345463752747\n",
      "Training batch loss 23235 0.19632495939731598\n",
      "Training batch loss 23255 0.19872212409973145\n",
      "Training batch loss 23275 0.2011108696460724\n",
      "Training batch loss 23295 0.19334366917610168\n",
      "Training batch loss 23315 0.2011612057685852\n",
      "Training batch loss 23335 0.18688777089118958\n",
      "Training batch loss 23355 0.19738690555095673\n",
      "Training batch loss 23375 0.20211857557296753\n",
      "Training batch loss 23395 0.19543080031871796\n",
      "Training batch loss 23415 0.20585793256759644\n",
      "Training batch loss 23435 0.19723135232925415\n",
      "Training batch loss 23455 0.19044041633605957\n",
      "Training batch loss 23475 0.20283818244934082\n",
      "Training batch loss 23495 0.19655489921569824\n",
      "Training batch loss 23515 0.1955663561820984\n",
      "Training batch loss 23535 0.20146483182907104\n",
      "Training batch loss 23555 0.1988999843597412\n",
      "Training batch loss 23575 0.19841943681240082\n",
      "Training batch loss 23595 0.20389148592948914\n",
      "Training batch loss 23615 0.19226862490177155\n",
      "Training batch loss 23635 0.2035389244556427\n",
      "Training batch loss 23655 0.19257719814777374\n",
      "Training batch loss 23675 0.1931302547454834\n",
      "Training batch loss 23695 0.19947470724582672\n",
      "Training batch loss 23715 0.19628626108169556\n",
      "Training batch loss 23735 0.20586371421813965\n",
      "Training batch loss 23755 0.1968308389186859\n",
      "Training batch loss 23775 0.20596987009048462\n",
      "Training batch loss 23795 0.1997758150100708\n",
      "Training batch loss 23815 0.20147401094436646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss 23835 0.19950464367866516\n",
      "Training batch loss 23855 0.1979617178440094\n",
      "Training batch loss 23875 0.20617517828941345\n",
      "Training batch loss 23895 0.2074558138847351\n",
      "Training batch loss 23915 0.19798772037029266\n",
      "Training batch loss 23935 0.20131948590278625\n",
      "Training batch loss 23955 0.21275150775909424\n",
      "Training batch loss 23975 0.19735084474086761\n",
      "Training batch loss 23995 0.2057875394821167\n",
      "Training batch loss 24015 0.2038630247116089\n",
      "Training batch loss 24035 0.1960001289844513\n",
      "Training batch loss 24055 0.19405975937843323\n",
      "Training batch loss 24075 0.20012935996055603\n",
      "Training batch loss 24095 0.20398616790771484\n",
      "Training batch loss 24115 0.19369086623191833\n",
      "Training batch loss 24135 0.20635294914245605\n",
      "Training batch loss 24155 0.1984732449054718\n",
      "Training batch loss 24175 0.20124468207359314\n",
      "Training batch loss 24195 0.18851247429847717\n",
      "Training batch loss 24215 0.19849780201911926\n",
      "Training batch loss 24235 0.20473094284534454\n",
      "Training batch loss 24255 0.19965031743049622\n",
      "Training batch loss 24275 0.20437705516815186\n",
      "Training batch loss 24295 0.19127696752548218\n",
      "Training batch loss 24315 0.1965131014585495\n",
      "Training batch loss 24335 0.195701465010643\n",
      "Training batch loss 24355 0.19478952884674072\n",
      "Training batch loss 24375 0.201440691947937\n",
      "Training epoch loss 17 0.19857197999954224\n",
      "Test loss during training 17 0.1976657658815384\n",
      "Test accuracy during training 17 0.0\n",
      "Training batch loss 24390 0.19170010089874268\n",
      "Training batch loss 24410 0.19589971005916595\n",
      "Training batch loss 24430 0.20131264626979828\n",
      "Training batch loss 24450 0.1983058601617813\n",
      "Training batch loss 24470 0.1991705298423767\n",
      "Training batch loss 24490 0.2009793221950531\n",
      "Training batch loss 24510 0.1980592906475067\n",
      "Training batch loss 24530 0.19715023040771484\n",
      "Training batch loss 24550 0.19260242581367493\n",
      "Training batch loss 24570 0.19880345463752747\n",
      "Training batch loss 24590 0.19632495939731598\n",
      "Training batch loss 24610 0.19872212409973145\n",
      "Training batch loss 24630 0.2011108696460724\n",
      "Training batch loss 24650 0.19334366917610168\n",
      "Training batch loss 24670 0.2011612057685852\n",
      "Training batch loss 24690 0.18688777089118958\n",
      "Training batch loss 24710 0.19738690555095673\n",
      "Training batch loss 24730 0.20211857557296753\n",
      "Training batch loss 24750 0.19543080031871796\n",
      "Training batch loss 24770 0.20585793256759644\n",
      "Training batch loss 24790 0.19723135232925415\n",
      "Training batch loss 24810 0.19044041633605957\n",
      "Training batch loss 24830 0.20283818244934082\n",
      "Training batch loss 24850 0.19655489921569824\n",
      "Training batch loss 24870 0.1955663561820984\n",
      "Training batch loss 24890 0.20146483182907104\n",
      "Training batch loss 24910 0.1988999843597412\n",
      "Training batch loss 24930 0.19841943681240082\n",
      "Training batch loss 24950 0.20389148592948914\n",
      "Training batch loss 24970 0.19226862490177155\n",
      "Training batch loss 24990 0.2035389244556427\n",
      "Training batch loss 25010 0.19257719814777374\n",
      "Training batch loss 25030 0.1931302547454834\n",
      "Training batch loss 25050 0.19947470724582672\n",
      "Training batch loss 25070 0.19628626108169556\n",
      "Training batch loss 25090 0.20586371421813965\n",
      "Training batch loss 25110 0.1968308389186859\n",
      "Training batch loss 25130 0.20596987009048462\n",
      "Training batch loss 25150 0.1997758150100708\n",
      "Training batch loss 25170 0.20147401094436646\n",
      "Training batch loss 25190 0.19950464367866516\n",
      "Training batch loss 25210 0.1979617178440094\n",
      "Training batch loss 25230 0.20617517828941345\n",
      "Training batch loss 25250 0.2074558138847351\n",
      "Training batch loss 25270 0.19798772037029266\n",
      "Training batch loss 25290 0.20131948590278625\n",
      "Training batch loss 25310 0.21275150775909424\n",
      "Training batch loss 25330 0.19735084474086761\n",
      "Training batch loss 25350 0.2057875394821167\n",
      "Training batch loss 25370 0.2038630247116089\n",
      "Training batch loss 25390 0.1960001289844513\n",
      "Training batch loss 25410 0.19405975937843323\n",
      "Training batch loss 25430 0.20012935996055603\n",
      "Training batch loss 25450 0.20398616790771484\n",
      "Training batch loss 25470 0.19369086623191833\n",
      "Training batch loss 25490 0.20635294914245605\n",
      "Training batch loss 25510 0.1984732449054718\n",
      "Training batch loss 25530 0.20124468207359314\n",
      "Training batch loss 25550 0.18851247429847717\n",
      "Training batch loss 25570 0.19849780201911926\n",
      "Training batch loss 25590 0.20473094284534454\n",
      "Training batch loss 25610 0.19965031743049622\n",
      "Training batch loss 25630 0.20437705516815186\n",
      "Training batch loss 25650 0.19127696752548218\n",
      "Training batch loss 25670 0.1965131014585495\n",
      "Training batch loss 25690 0.195701465010643\n",
      "Training batch loss 25710 0.19478952884674072\n",
      "Training batch loss 25730 0.201440691947937\n",
      "Training epoch loss 18 0.19857197999954224\n",
      "Test loss during training 18 0.19766581058502197\n",
      "Test accuracy during training 18 0.0\n",
      "Training batch loss 25745 0.19170010089874268\n",
      "Training batch loss 25765 0.19589971005916595\n",
      "Training batch loss 25785 0.20131264626979828\n",
      "Training batch loss 25805 0.1983058601617813\n",
      "Training batch loss 25825 0.1991705298423767\n",
      "Training batch loss 25845 0.2009793221950531\n",
      "Training batch loss 25865 0.1980592906475067\n",
      "Training batch loss 25885 0.19715023040771484\n",
      "Training batch loss 25905 0.19260242581367493\n",
      "Training batch loss 25925 0.19880345463752747\n",
      "Training batch loss 25945 0.19632495939731598\n",
      "Training batch loss 25965 0.19872212409973145\n",
      "Training batch loss 25985 0.2011108696460724\n",
      "Training batch loss 26005 0.19334366917610168\n",
      "Training batch loss 26025 0.2011612057685852\n",
      "Training batch loss 26045 0.18688777089118958\n",
      "Training batch loss 26065 0.19738690555095673\n",
      "Training batch loss 26085 0.20211857557296753\n",
      "Training batch loss 26105 0.19543080031871796\n",
      "Training batch loss 26125 0.20585793256759644\n",
      "Training batch loss 26145 0.19723135232925415\n",
      "Training batch loss 26165 0.19044041633605957\n",
      "Training batch loss 26185 0.20283818244934082\n",
      "Training batch loss 26205 0.19655489921569824\n",
      "Training batch loss 26225 0.1955663561820984\n",
      "Training batch loss 26245 0.20146483182907104\n",
      "Training batch loss 26265 0.1988999843597412\n",
      "Training batch loss 26285 0.19841943681240082\n",
      "Training batch loss 26305 0.20389148592948914\n",
      "Training batch loss 26325 0.19226862490177155\n",
      "Training batch loss 26345 0.2035389244556427\n",
      "Training batch loss 26365 0.19257719814777374\n",
      "Training batch loss 26385 0.1931302547454834\n",
      "Training batch loss 26405 0.19947470724582672\n",
      "Training batch loss 26425 0.19628626108169556\n",
      "Training batch loss 26445 0.20586371421813965\n",
      "Training batch loss 26465 0.1968308389186859\n",
      "Training batch loss 26485 0.20596987009048462\n",
      "Training batch loss 26505 0.1997758150100708\n",
      "Training batch loss 26525 0.20147401094436646\n",
      "Training batch loss 26545 0.19950464367866516\n",
      "Training batch loss 26565 0.1979617178440094\n",
      "Training batch loss 26585 0.20617517828941345\n",
      "Training batch loss 26605 0.2074558138847351\n",
      "Training batch loss 26625 0.19798772037029266\n",
      "Training batch loss 26645 0.20131948590278625\n",
      "Training batch loss 26665 0.21275150775909424\n",
      "Training batch loss 26685 0.19735084474086761\n",
      "Training batch loss 26705 0.2057875394821167\n",
      "Training batch loss 26725 0.2038630247116089\n",
      "Training batch loss 26745 0.1960001289844513\n",
      "Training batch loss 26765 0.19405975937843323\n",
      "Training batch loss 26785 0.20012935996055603\n",
      "Training batch loss 26805 0.20398616790771484\n",
      "Training batch loss 26825 0.19369086623191833\n",
      "Training batch loss 26845 0.20635294914245605\n",
      "Training batch loss 26865 0.1984732449054718\n",
      "Training batch loss 26885 0.20124468207359314\n",
      "Training batch loss 26905 0.18851247429847717\n",
      "Training batch loss 26925 0.19849780201911926\n",
      "Training batch loss 26945 0.20473094284534454\n",
      "Training batch loss 26965 0.19965031743049622\n",
      "Training batch loss 26985 0.20437705516815186\n",
      "Training batch loss 27005 0.19127696752548218\n",
      "Training batch loss 27025 0.1965131014585495\n",
      "Training batch loss 27045 0.195701465010643\n",
      "Training batch loss 27065 0.19478952884674072\n",
      "Training batch loss 27085 0.201440691947937\n",
      "Training epoch loss 19 0.19857197999954224\n",
      "Test loss during training 19 0.1976657658815384\n",
      "Test accuracy during training 19 0.0\n",
      "Training batch loss 27100 0.19170010089874268\n",
      "Training batch loss 27120 0.19589971005916595\n",
      "Training batch loss 27140 0.20131264626979828\n",
      "Training batch loss 27160 0.1983058601617813\n",
      "Training batch loss 27180 0.1991705298423767\n",
      "Training batch loss 27200 0.2009793221950531\n",
      "Training batch loss 27220 0.1980592906475067\n",
      "Training batch loss 27240 0.19715023040771484\n",
      "Training batch loss 27260 0.19260242581367493\n",
      "Training batch loss 27280 0.19880345463752747\n",
      "Training batch loss 27300 0.19632495939731598\n",
      "Training batch loss 27320 0.19872212409973145\n",
      "Training batch loss 27340 0.2011108696460724\n",
      "Training batch loss 27360 0.19334366917610168\n",
      "Training batch loss 27380 0.2011612057685852\n",
      "Training batch loss 27400 0.18688777089118958\n",
      "Training batch loss 27420 0.19738690555095673\n",
      "Training batch loss 27440 0.20211857557296753\n",
      "Training batch loss 27460 0.19543080031871796\n",
      "Training batch loss 27480 0.20585793256759644\n",
      "Training batch loss 27500 0.19723135232925415\n",
      "Training batch loss 27520 0.19044041633605957\n",
      "Training batch loss 27540 0.20283818244934082\n",
      "Training batch loss 27560 0.19655489921569824\n",
      "Training batch loss 27580 0.1955663561820984\n",
      "Training batch loss 27600 0.20146483182907104\n",
      "Training batch loss 27620 0.1988999843597412\n",
      "Training batch loss 27640 0.19841943681240082\n",
      "Training batch loss 27660 0.20389148592948914\n",
      "Training batch loss 27680 0.19226862490177155\n",
      "Training batch loss 27700 0.2035389244556427\n",
      "Training batch loss 27720 0.19257719814777374\n",
      "Training batch loss 27740 0.1931302547454834\n",
      "Training batch loss 27760 0.19947470724582672\n",
      "Training batch loss 27780 0.19628626108169556\n",
      "Training batch loss 27800 0.20586371421813965\n",
      "Training batch loss 27820 0.1968308389186859\n",
      "Training batch loss 27840 0.20596987009048462\n",
      "Training batch loss 27860 0.1997758150100708\n",
      "Training batch loss 27880 0.20147401094436646\n",
      "Training batch loss 27900 0.19950464367866516\n",
      "Training batch loss 27920 0.1979617178440094\n",
      "Training batch loss 27940 0.20617517828941345\n",
      "Training batch loss 27960 0.2074558138847351\n",
      "Training batch loss 27980 0.19798772037029266\n",
      "Training batch loss 28000 0.20131948590278625\n",
      "Training batch loss 28020 0.21275150775909424\n",
      "Training batch loss 28040 0.19735084474086761\n",
      "Training batch loss 28060 0.2057875394821167\n",
      "Training batch loss 28080 0.2038630247116089\n",
      "Training batch loss 28100 0.1960001289844513\n",
      "Training batch loss 28120 0.19405975937843323\n",
      "Training batch loss 28140 0.20012935996055603\n",
      "Training batch loss 28160 0.20398616790771484\n",
      "Training batch loss 28180 0.19369086623191833\n",
      "Training batch loss 28200 0.20635294914245605\n",
      "Training batch loss 28220 0.1984732449054718\n",
      "Training batch loss 28240 0.20124468207359314\n",
      "Training batch loss 28260 0.18851247429847717\n",
      "Training batch loss 28280 0.19849780201911926\n",
      "Training batch loss 28300 0.20473094284534454\n",
      "Training batch loss 28320 0.19965031743049622\n",
      "Training batch loss 28340 0.20437705516815186\n",
      "Training batch loss 28360 0.19127696752548218\n",
      "Training batch loss 28380 0.1965131014585495\n",
      "Training batch loss 28400 0.195701465010643\n",
      "Training batch loss 28420 0.19478952884674072\n",
      "Training batch loss 28440 0.201440691947937\n",
      "Training epoch loss 20 0.19857197999954224\n",
      "Test loss during training 20 0.1976657658815384\n",
      "Test accuracy during training 20 0.0\n",
      "Training batch loss 28455 0.19170010089874268\n",
      "Training batch loss 28475 0.19589971005916595\n",
      "Training batch loss 28495 0.20131264626979828\n",
      "Training batch loss 28515 0.1983058601617813\n",
      "Training batch loss 28535 0.1991705298423767\n",
      "Training batch loss 28555 0.2009793221950531\n",
      "Training batch loss 28575 0.1980592906475067\n",
      "Training batch loss 28595 0.19715023040771484\n",
      "Training batch loss 28615 0.19260242581367493\n",
      "Training batch loss 28635 0.19880345463752747\n",
      "Training batch loss 28655 0.19632495939731598\n",
      "Training batch loss 28675 0.19872212409973145\n",
      "Training batch loss 28695 0.2011108696460724\n",
      "Training batch loss 28715 0.19334366917610168\n",
      "Training batch loss 28735 0.2011612057685852\n",
      "Training batch loss 28755 0.18688777089118958\n",
      "Training batch loss 28775 0.19738690555095673\n",
      "Training batch loss 28795 0.20211857557296753\n",
      "Training batch loss 28815 0.19543080031871796\n",
      "Training batch loss 28835 0.20585793256759644\n",
      "Training batch loss 28855 0.19723135232925415\n",
      "Training batch loss 28875 0.19044041633605957\n",
      "Training batch loss 28895 0.20283818244934082\n",
      "Training batch loss 28915 0.19655489921569824\n",
      "Training batch loss 28935 0.1955663561820984\n",
      "Training batch loss 28955 0.20146483182907104\n",
      "Training batch loss 28975 0.1988999843597412\n",
      "Training batch loss 28995 0.19841943681240082\n",
      "Training batch loss 29015 0.20389148592948914\n",
      "Training batch loss 29035 0.19226862490177155\n",
      "Training batch loss 29055 0.2035389244556427\n",
      "Training batch loss 29075 0.19257719814777374\n",
      "Training batch loss 29095 0.1931302547454834\n",
      "Training batch loss 29115 0.19947470724582672\n",
      "Training batch loss 29135 0.19628626108169556\n",
      "Training batch loss 29155 0.20586371421813965\n",
      "Training batch loss 29175 0.1968308389186859\n",
      "Training batch loss 29195 0.20596987009048462\n",
      "Training batch loss 29215 0.1997758150100708\n",
      "Training batch loss 29235 0.20147401094436646\n",
      "Training batch loss 29255 0.19950464367866516\n",
      "Training batch loss 29275 0.1979617178440094\n",
      "Training batch loss 29295 0.20617517828941345\n",
      "Training batch loss 29315 0.2074558138847351\n",
      "Training batch loss 29335 0.19798772037029266\n",
      "Training batch loss 29355 0.20131948590278625\n",
      "Training batch loss 29375 0.21275150775909424\n",
      "Training batch loss 29395 0.19735084474086761\n",
      "Training batch loss 29415 0.2057875394821167\n",
      "Training batch loss 29435 0.2038630247116089\n",
      "Training batch loss 29455 0.1960001289844513\n",
      "Training batch loss 29475 0.19405975937843323\n",
      "Training batch loss 29495 0.20012935996055603\n",
      "Training batch loss 29515 0.20398616790771484\n",
      "Training batch loss 29535 0.19369086623191833\n",
      "Training batch loss 29555 0.20635294914245605\n",
      "Training batch loss 29575 0.1984732449054718\n",
      "Training batch loss 29595 0.20124468207359314\n",
      "Training batch loss 29615 0.18851247429847717\n",
      "Training batch loss 29635 0.19849780201911926\n",
      "Training batch loss 29655 0.20473094284534454\n",
      "Training batch loss 29675 0.19965031743049622\n",
      "Training batch loss 29695 0.20437705516815186\n",
      "Training batch loss 29715 0.19127696752548218\n",
      "Training batch loss 29735 0.1965131014585495\n",
      "Training batch loss 29755 0.195701465010643\n",
      "Training batch loss 29775 0.19478952884674072\n",
      "Training batch loss 29795 0.201440691947937\n",
      "Training epoch loss 21 0.19857197999954224\n",
      "Test loss during training 21 0.1976657658815384\n",
      "Test accuracy during training 21 0.0\n",
      "Training batch loss 29810 0.19170010089874268\n",
      "Training batch loss 29830 0.19589971005916595\n",
      "Training batch loss 29850 0.20131264626979828\n",
      "Training batch loss 29870 0.1983058601617813\n",
      "Training batch loss 29890 0.1991705298423767\n",
      "Training batch loss 29910 0.2009793221950531\n",
      "Training batch loss 29930 0.1980592906475067\n",
      "Training batch loss 29950 0.19715023040771484\n",
      "Training batch loss 29970 0.19260242581367493\n",
      "Training batch loss 29990 0.19880345463752747\n",
      "Training batch loss 30010 0.19632495939731598\n",
      "Training batch loss 30030 0.19872212409973145\n",
      "Training batch loss 30050 0.2011108696460724\n",
      "Training batch loss 30070 0.19334366917610168\n",
      "Training batch loss 30090 0.2011612057685852\n",
      "Training batch loss 30110 0.18688777089118958\n",
      "Training batch loss 30130 0.19738690555095673\n",
      "Training batch loss 30150 0.20211857557296753\n",
      "Training batch loss 30170 0.19543080031871796\n",
      "Training batch loss 30190 0.20585793256759644\n",
      "Training batch loss 30210 0.19723135232925415\n",
      "Training batch loss 30230 0.19044041633605957\n",
      "Training batch loss 30250 0.20283818244934082\n",
      "Training batch loss 30270 0.19655489921569824\n",
      "Training batch loss 30290 0.1955663561820984\n",
      "Training batch loss 30310 0.20146483182907104\n",
      "Training batch loss 30330 0.1988999843597412\n",
      "Training batch loss 30350 0.19841943681240082\n",
      "Training batch loss 30370 0.20389148592948914\n",
      "Training batch loss 30390 0.19226862490177155\n",
      "Training batch loss 30410 0.2035389244556427\n",
      "Training batch loss 30430 0.19257719814777374\n",
      "Training batch loss 30450 0.1931302547454834\n",
      "Training batch loss 30470 0.19947470724582672\n",
      "Training batch loss 30490 0.19628626108169556\n",
      "Training batch loss 30510 0.20586371421813965\n",
      "Training batch loss 30530 0.1968308389186859\n",
      "Training batch loss 30550 0.20596987009048462\n",
      "Training batch loss 30570 0.1997758150100708\n",
      "Training batch loss 30590 0.20147401094436646\n",
      "Training batch loss 30610 0.19950464367866516\n",
      "Training batch loss 30630 0.1979617178440094\n",
      "Training batch loss 30650 0.20617517828941345\n",
      "Training batch loss 30670 0.2074558138847351\n",
      "Training batch loss 30690 0.19798772037029266\n",
      "Training batch loss 30710 0.20131948590278625\n",
      "Training batch loss 30730 0.21275150775909424\n",
      "Training batch loss 30750 0.19735084474086761\n",
      "Training batch loss 30770 0.2057875394821167\n",
      "Training batch loss 30790 0.2038630247116089\n",
      "Training batch loss 30810 0.1960001289844513\n",
      "Training batch loss 30830 0.19405975937843323\n",
      "Training batch loss 30850 0.20012935996055603\n",
      "Training batch loss 30870 0.20398616790771484\n",
      "Training batch loss 30890 0.19369086623191833\n",
      "Training batch loss 30910 0.20635294914245605\n",
      "Training batch loss 30930 0.1984732449054718\n",
      "Training batch loss 30950 0.20124468207359314\n",
      "Training batch loss 30970 0.18851247429847717\n",
      "Training batch loss 30990 0.19849780201911926\n",
      "Training batch loss 31010 0.20473094284534454\n",
      "Training batch loss 31030 0.19965031743049622\n",
      "Training batch loss 31050 0.20437705516815186\n",
      "Training batch loss 31070 0.19127696752548218\n",
      "Training batch loss 31090 0.1965131014585495\n",
      "Training batch loss 31110 0.195701465010643\n",
      "Training batch loss 31130 0.19478952884674072\n",
      "Training batch loss 31150 0.201440691947937\n",
      "Training epoch loss 22 0.19857197999954224\n",
      "Test loss during training 22 0.1976657658815384\n",
      "Test accuracy during training 22 0.0\n",
      "Training batch loss 31165 0.19170010089874268\n",
      "Training batch loss 31185 0.19589971005916595\n",
      "Training batch loss 31205 0.20131264626979828\n",
      "Training batch loss 31225 0.1983058601617813\n",
      "Training batch loss 31245 0.1991705298423767\n",
      "Training batch loss 31265 0.2009793221950531\n",
      "Training batch loss 31285 0.1980592906475067\n",
      "Training batch loss 31305 0.19715023040771484\n",
      "Training batch loss 31325 0.19260242581367493\n",
      "Training batch loss 31345 0.19880345463752747\n",
      "Training batch loss 31365 0.19632495939731598\n",
      "Training batch loss 31385 0.19872212409973145\n",
      "Training batch loss 31405 0.2011108696460724\n",
      "Training batch loss 31425 0.19334366917610168\n",
      "Training batch loss 31445 0.2011612057685852\n",
      "Training batch loss 31465 0.18688777089118958\n",
      "Training batch loss 31485 0.19738690555095673\n",
      "Training batch loss 31505 0.20211857557296753\n",
      "Training batch loss 31525 0.19543080031871796\n",
      "Training batch loss 31545 0.20585793256759644\n",
      "Training batch loss 31565 0.19723135232925415\n",
      "Training batch loss 31585 0.19044041633605957\n",
      "Training batch loss 31605 0.20283818244934082\n",
      "Training batch loss 31625 0.19655489921569824\n",
      "Training batch loss 31645 0.1955663561820984\n",
      "Training batch loss 31665 0.20146483182907104\n",
      "Training batch loss 31685 0.1988999843597412\n",
      "Training batch loss 31705 0.19841943681240082\n",
      "Training batch loss 31725 0.20389148592948914\n",
      "Training batch loss 31745 0.19226862490177155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss 31765 0.2035389244556427\n",
      "Training batch loss 31785 0.19257719814777374\n",
      "Training batch loss 31805 0.1931302547454834\n",
      "Training batch loss 31825 0.19947470724582672\n",
      "Training batch loss 31845 0.19628626108169556\n",
      "Training batch loss 31865 0.20586371421813965\n",
      "Training batch loss 31885 0.1968308389186859\n",
      "Training batch loss 31905 0.20596987009048462\n",
      "Training batch loss 31925 0.1997758150100708\n",
      "Training batch loss 31945 0.20147401094436646\n",
      "Training batch loss 31965 0.19950464367866516\n",
      "Training batch loss 31985 0.1979617178440094\n",
      "Training batch loss 32005 0.20617517828941345\n",
      "Training batch loss 32025 0.2074558138847351\n",
      "Training batch loss 32045 0.19798772037029266\n",
      "Training batch loss 32065 0.20131948590278625\n",
      "Training batch loss 32085 0.21275150775909424\n",
      "Training batch loss 32105 0.19735084474086761\n",
      "Training batch loss 32125 0.2057875394821167\n",
      "Training batch loss 32145 0.2038630247116089\n",
      "Training batch loss 32165 0.1960001289844513\n",
      "Training batch loss 32185 0.19405975937843323\n",
      "Training batch loss 32205 0.20012935996055603\n",
      "Training batch loss 32225 0.20398616790771484\n",
      "Training batch loss 32245 0.19369086623191833\n",
      "Training batch loss 32265 0.20635294914245605\n",
      "Training batch loss 32285 0.1984732449054718\n",
      "Training batch loss 32305 0.20124468207359314\n",
      "Training batch loss 32325 0.18851247429847717\n",
      "Training batch loss 32345 0.19849780201911926\n",
      "Training batch loss 32365 0.20473094284534454\n",
      "Training batch loss 32385 0.19965031743049622\n",
      "Training batch loss 32405 0.20437705516815186\n",
      "Training batch loss 32425 0.19127696752548218\n",
      "Training batch loss 32445 0.1965131014585495\n",
      "Training batch loss 32465 0.195701465010643\n",
      "Training batch loss 32485 0.19478952884674072\n",
      "Training batch loss 32505 0.201440691947937\n",
      "Training epoch loss 23 0.19857197999954224\n",
      "Test loss during training 23 0.1976657658815384\n",
      "Test accuracy during training 23 0.0\n",
      "Training batch loss 32520 0.19170010089874268\n",
      "Training batch loss 32540 0.19589971005916595\n",
      "Training batch loss 32560 0.20131264626979828\n",
      "Training batch loss 32580 0.1983058601617813\n",
      "Training batch loss 32600 0.1991705298423767\n",
      "Training batch loss 32620 0.2009793221950531\n",
      "Training batch loss 32640 0.1980592906475067\n",
      "Training batch loss 32660 0.19715023040771484\n",
      "Training batch loss 32680 0.19260242581367493\n",
      "Training batch loss 32700 0.19880345463752747\n",
      "Training batch loss 32720 0.19632495939731598\n",
      "Training batch loss 32740 0.19872212409973145\n",
      "Training batch loss 32760 0.2011108696460724\n",
      "Training batch loss 32780 0.19334366917610168\n",
      "Training batch loss 32800 0.2011612057685852\n",
      "Training batch loss 32820 0.18688777089118958\n",
      "Training batch loss 32840 0.19738690555095673\n",
      "Training batch loss 32860 0.20211857557296753\n",
      "Training batch loss 32880 0.19543080031871796\n",
      "Training batch loss 32900 0.20585793256759644\n",
      "Training batch loss 32920 0.19723135232925415\n",
      "Training batch loss 32940 0.19044041633605957\n",
      "Training batch loss 32960 0.20283818244934082\n",
      "Training batch loss 32980 0.19655489921569824\n",
      "Training batch loss 33000 0.1955663561820984\n",
      "Training batch loss 33020 0.20146483182907104\n",
      "Training batch loss 33040 0.1988999843597412\n",
      "Training batch loss 33060 0.19841943681240082\n",
      "Training batch loss 33080 0.20389148592948914\n",
      "Training batch loss 33100 0.19226862490177155\n",
      "Training batch loss 33120 0.2035389244556427\n",
      "Training batch loss 33140 0.19257719814777374\n",
      "Training batch loss 33160 0.1931302547454834\n",
      "Training batch loss 33180 0.19947470724582672\n",
      "Training batch loss 33200 0.19628626108169556\n",
      "Training batch loss 33220 0.20586371421813965\n",
      "Training batch loss 33240 0.1968308389186859\n",
      "Training batch loss 33260 0.20596987009048462\n",
      "Training batch loss 33280 0.1997758150100708\n",
      "Training batch loss 33300 0.20147401094436646\n",
      "Training batch loss 33320 0.19950464367866516\n",
      "Training batch loss 33340 0.1979617178440094\n",
      "Training batch loss 33360 0.20617517828941345\n",
      "Training batch loss 33380 0.2074558138847351\n",
      "Training batch loss 33400 0.19798772037029266\n",
      "Training batch loss 33420 0.20131948590278625\n",
      "Training batch loss 33440 0.21275150775909424\n",
      "Training batch loss 33460 0.19735084474086761\n",
      "Training batch loss 33480 0.2057875394821167\n",
      "Training batch loss 33500 0.2038630247116089\n",
      "Training batch loss 33520 0.1960001289844513\n",
      "Training batch loss 33540 0.19405975937843323\n",
      "Training batch loss 33560 0.20012935996055603\n",
      "Training batch loss 33580 0.20398616790771484\n",
      "Training batch loss 33600 0.19369086623191833\n",
      "Training batch loss 33620 0.20635294914245605\n",
      "Training batch loss 33640 0.1984732449054718\n",
      "Training batch loss 33660 0.20124468207359314\n",
      "Training batch loss 33680 0.18851247429847717\n",
      "Training batch loss 33700 0.19849780201911926\n",
      "Training batch loss 33720 0.20473094284534454\n",
      "Training batch loss 33740 0.19965031743049622\n",
      "Training batch loss 33760 0.20437705516815186\n",
      "Training batch loss 33780 0.19127696752548218\n",
      "Training batch loss 33800 0.1965131014585495\n",
      "Training batch loss 33820 0.195701465010643\n",
      "Training batch loss 33840 0.19478952884674072\n",
      "Training batch loss 33860 0.201440691947937\n",
      "Training epoch loss 24 0.19857197999954224\n",
      "Test loss during training 24 0.1976657658815384\n",
      "Test accuracy during training 24 0.0\n",
      "Training batch loss 33875 0.19170010089874268\n",
      "Training batch loss 33895 0.19589971005916595\n",
      "Training batch loss 33915 0.20131264626979828\n",
      "Training batch loss 33935 0.1983058601617813\n",
      "Training batch loss 33955 0.1991705298423767\n",
      "Training batch loss 33975 0.2009793221950531\n",
      "Training batch loss 33995 0.1980592906475067\n",
      "Training batch loss 34015 0.19715023040771484\n",
      "Training batch loss 34035 0.19260242581367493\n",
      "Training batch loss 34055 0.19880345463752747\n",
      "Training batch loss 34075 0.19632495939731598\n",
      "Training batch loss 34095 0.19872212409973145\n",
      "Training batch loss 34115 0.2011108696460724\n",
      "Training batch loss 34135 0.19334366917610168\n",
      "Training batch loss 34155 0.2011612057685852\n",
      "Training batch loss 34175 0.18688777089118958\n",
      "Training batch loss 34195 0.19738690555095673\n",
      "Training batch loss 34215 0.20211857557296753\n",
      "Training batch loss 34235 0.19543080031871796\n",
      "Training batch loss 34255 0.20585793256759644\n",
      "Training batch loss 34275 0.19723135232925415\n",
      "Training batch loss 34295 0.19044041633605957\n",
      "Training batch loss 34315 0.20283818244934082\n",
      "Training batch loss 34335 0.19655489921569824\n",
      "Training batch loss 34355 0.1955663561820984\n",
      "Training batch loss 34375 0.20146483182907104\n",
      "Training batch loss 34395 0.1988999843597412\n",
      "Training batch loss 34415 0.19841943681240082\n",
      "Training batch loss 34435 0.20389148592948914\n",
      "Training batch loss 34455 0.19226862490177155\n",
      "Training batch loss 34475 0.2035389244556427\n",
      "Training batch loss 34495 0.19257719814777374\n",
      "Training batch loss 34515 0.1931302547454834\n",
      "Training batch loss 34535 0.19947470724582672\n",
      "Training batch loss 34555 0.19628626108169556\n",
      "Training batch loss 34575 0.20586371421813965\n",
      "Training batch loss 34595 0.1968308389186859\n",
      "Training batch loss 34615 0.20596987009048462\n",
      "Training batch loss 34635 0.1997758150100708\n",
      "Training batch loss 34655 0.20147401094436646\n",
      "Training batch loss 34675 0.19950464367866516\n",
      "Training batch loss 34695 0.1979617178440094\n",
      "Training batch loss 34715 0.20617517828941345\n",
      "Training batch loss 34735 0.2074558138847351\n",
      "Training batch loss 34755 0.19798772037029266\n",
      "Training batch loss 34775 0.20131948590278625\n",
      "Training batch loss 34795 0.21275150775909424\n",
      "Training batch loss 34815 0.19735084474086761\n",
      "Training batch loss 34835 0.2057875394821167\n",
      "Training batch loss 34855 0.2038630247116089\n",
      "Training batch loss 34875 0.1960001289844513\n",
      "Training batch loss 34895 0.19405975937843323\n",
      "Training batch loss 34915 0.20012935996055603\n",
      "Training batch loss 34935 0.20398616790771484\n",
      "Training batch loss 34955 0.19369086623191833\n",
      "Training batch loss 34975 0.20635294914245605\n",
      "Training batch loss 34995 0.1984732449054718\n",
      "Training batch loss 35015 0.20124468207359314\n",
      "Training batch loss 35035 0.18851247429847717\n",
      "Training batch loss 35055 0.19849780201911926\n",
      "Training batch loss 35075 0.20473094284534454\n",
      "Training batch loss 35095 0.19965031743049622\n",
      "Training batch loss 35115 0.20437705516815186\n",
      "Training batch loss 35135 0.19127696752548218\n",
      "Training batch loss 35155 0.1965131014585495\n",
      "Training batch loss 35175 0.195701465010643\n",
      "Training batch loss 35195 0.19478952884674072\n",
      "Training batch loss 35215 0.201440691947937\n",
      "Training epoch loss 25 0.19857197999954224\n",
      "Test loss during training 25 0.1976657658815384\n",
      "Test accuracy during training 25 0.0\n",
      "Training batch loss 35230 0.19170010089874268\n",
      "Training batch loss 35250 0.19589971005916595\n",
      "Training batch loss 35270 0.20131264626979828\n",
      "Training batch loss 35290 0.1983058601617813\n",
      "Training batch loss 35310 0.1991705298423767\n",
      "Training batch loss 35330 0.2009793221950531\n",
      "Training batch loss 35350 0.1980592906475067\n",
      "Training batch loss 35370 0.19715023040771484\n",
      "Training batch loss 35390 0.19260242581367493\n",
      "Training batch loss 35410 0.19880345463752747\n",
      "Training batch loss 35430 0.19632495939731598\n",
      "Training batch loss 35450 0.19872212409973145\n",
      "Training batch loss 35470 0.2011108696460724\n",
      "Training batch loss 35490 0.19334366917610168\n",
      "Training batch loss 35510 0.2011612057685852\n",
      "Training batch loss 35530 0.18688777089118958\n",
      "Training batch loss 35550 0.19738690555095673\n",
      "Training batch loss 35570 0.20211857557296753\n",
      "Training batch loss 35590 0.19543080031871796\n",
      "Training batch loss 35610 0.20585793256759644\n",
      "Training batch loss 35630 0.19723135232925415\n",
      "Training batch loss 35650 0.19044041633605957\n",
      "Training batch loss 35670 0.20283818244934082\n",
      "Training batch loss 35690 0.19655489921569824\n",
      "Training batch loss 35710 0.1955663561820984\n",
      "Training batch loss 35730 0.20146483182907104\n",
      "Training batch loss 35750 0.1988999843597412\n",
      "Training batch loss 35770 0.19841943681240082\n",
      "Training batch loss 35790 0.20389148592948914\n",
      "Training batch loss 35810 0.19226862490177155\n",
      "Training batch loss 35830 0.2035389244556427\n",
      "Training batch loss 35850 0.19257719814777374\n",
      "Training batch loss 35870 0.1931302547454834\n",
      "Training batch loss 35890 0.19947470724582672\n",
      "Training batch loss 35910 0.19628626108169556\n",
      "Training batch loss 35930 0.20586371421813965\n",
      "Training batch loss 35950 0.1968308389186859\n",
      "Training batch loss 35970 0.20596987009048462\n",
      "Training batch loss 35990 0.1997758150100708\n",
      "Training batch loss 36010 0.20147401094436646\n",
      "Training batch loss 36030 0.19950464367866516\n",
      "Training batch loss 36050 0.1979617178440094\n",
      "Training batch loss 36070 0.20617517828941345\n",
      "Training batch loss 36090 0.2074558138847351\n",
      "Training batch loss 36110 0.19798772037029266\n",
      "Training batch loss 36130 0.20131948590278625\n",
      "Training batch loss 36150 0.21275150775909424\n",
      "Training batch loss 36170 0.19735084474086761\n",
      "Training batch loss 36190 0.2057875394821167\n",
      "Training batch loss 36210 0.2038630247116089\n",
      "Training batch loss 36230 0.1960001289844513\n",
      "Training batch loss 36250 0.19405975937843323\n",
      "Training batch loss 36270 0.20012935996055603\n",
      "Training batch loss 36290 0.20398616790771484\n",
      "Training batch loss 36310 0.19369086623191833\n",
      "Training batch loss 36330 0.20635294914245605\n",
      "Training batch loss 36350 0.1984732449054718\n",
      "Training batch loss 36370 0.20124468207359314\n",
      "Training batch loss 36390 0.18851247429847717\n",
      "Training batch loss 36410 0.19849780201911926\n",
      "Training batch loss 36430 0.20473094284534454\n",
      "Training batch loss 36450 0.19965031743049622\n",
      "Training batch loss 36470 0.20437705516815186\n",
      "Training batch loss 36490 0.19127696752548218\n",
      "Training batch loss 36510 0.1965131014585495\n",
      "Training batch loss 36530 0.195701465010643\n",
      "Training batch loss 36550 0.19478952884674072\n",
      "Training batch loss 36570 0.201440691947937\n",
      "Training epoch loss 26 0.19857197999954224\n",
      "Test loss during training 26 0.1976657658815384\n",
      "Test accuracy during training 26 0.0\n",
      "Training batch loss 36585 0.19170010089874268\n",
      "Training batch loss 36605 0.19589971005916595\n",
      "Training batch loss 36625 0.20131264626979828\n",
      "Training batch loss 36645 0.1983058601617813\n",
      "Training batch loss 36665 0.1991705298423767\n",
      "Training batch loss 36685 0.2009793221950531\n",
      "Training batch loss 36705 0.1980592906475067\n",
      "Training batch loss 36725 0.19715023040771484\n",
      "Training batch loss 36745 0.19260242581367493\n",
      "Training batch loss 36765 0.19880345463752747\n",
      "Training batch loss 36785 0.19632495939731598\n",
      "Training batch loss 36805 0.19872212409973145\n",
      "Training batch loss 36825 0.2011108696460724\n",
      "Training batch loss 36845 0.19334366917610168\n",
      "Training batch loss 36865 0.2011612057685852\n",
      "Training batch loss 36885 0.18688777089118958\n",
      "Training batch loss 36905 0.19738690555095673\n",
      "Training batch loss 36925 0.20211857557296753\n",
      "Training batch loss 36945 0.19543080031871796\n",
      "Training batch loss 36965 0.20585793256759644\n",
      "Training batch loss 36985 0.19723135232925415\n",
      "Training batch loss 37005 0.19044041633605957\n",
      "Training batch loss 37025 0.20283818244934082\n",
      "Training batch loss 37045 0.19655489921569824\n",
      "Training batch loss 37065 0.1955663561820984\n",
      "Training batch loss 37085 0.20146483182907104\n",
      "Training batch loss 37105 0.1988999843597412\n",
      "Training batch loss 37125 0.19841943681240082\n",
      "Training batch loss 37145 0.20389148592948914\n",
      "Training batch loss 37165 0.19226862490177155\n",
      "Training batch loss 37185 0.2035389244556427\n",
      "Training batch loss 37205 0.19257719814777374\n",
      "Training batch loss 37225 0.1931302547454834\n",
      "Training batch loss 37245 0.19947470724582672\n",
      "Training batch loss 37265 0.19628626108169556\n",
      "Training batch loss 37285 0.20586371421813965\n",
      "Training batch loss 37305 0.1968308389186859\n",
      "Training batch loss 37325 0.20596987009048462\n",
      "Training batch loss 37345 0.1997758150100708\n",
      "Training batch loss 37365 0.20147401094436646\n",
      "Training batch loss 37385 0.19950464367866516\n",
      "Training batch loss 37405 0.1979617178440094\n",
      "Training batch loss 37425 0.20617517828941345\n",
      "Training batch loss 37445 0.2074558138847351\n",
      "Training batch loss 37465 0.19798772037029266\n",
      "Training batch loss 37485 0.20131948590278625\n",
      "Training batch loss 37505 0.21275150775909424\n",
      "Training batch loss 37525 0.19735084474086761\n",
      "Training batch loss 37545 0.2057875394821167\n",
      "Training batch loss 37565 0.2038630247116089\n",
      "Training batch loss 37585 0.1960001289844513\n",
      "Training batch loss 37605 0.19405975937843323\n",
      "Training batch loss 37625 0.20012935996055603\n",
      "Training batch loss 37645 0.20398616790771484\n",
      "Training batch loss 37665 0.19369086623191833\n",
      "Training batch loss 37685 0.20635294914245605\n",
      "Training batch loss 37705 0.1984732449054718\n",
      "Training batch loss 37725 0.20124468207359314\n",
      "Training batch loss 37745 0.18851247429847717\n",
      "Training batch loss 37765 0.19849780201911926\n",
      "Training batch loss 37785 0.20473094284534454\n",
      "Training batch loss 37805 0.19965031743049622\n",
      "Training batch loss 37825 0.20437705516815186\n",
      "Training batch loss 37845 0.19127696752548218\n",
      "Training batch loss 37865 0.1965131014585495\n",
      "Training batch loss 37885 0.195701465010643\n",
      "Training batch loss 37905 0.19478952884674072\n",
      "Training batch loss 37925 0.201440691947937\n",
      "Training epoch loss 27 0.19857197999954224\n",
      "Test loss during training 27 0.1976657658815384\n",
      "Test accuracy during training 27 0.0\n",
      "Training batch loss 37940 0.19170010089874268\n",
      "Training batch loss 37960 0.19589971005916595\n",
      "Training batch loss 37980 0.20131264626979828\n",
      "Training batch loss 38000 0.1983058601617813\n",
      "Training batch loss 38020 0.1991705298423767\n",
      "Training batch loss 38040 0.2009793221950531\n",
      "Training batch loss 38060 0.1980592906475067\n",
      "Training batch loss 38080 0.19715023040771484\n",
      "Training batch loss 38100 0.19260242581367493\n",
      "Training batch loss 38120 0.19880345463752747\n",
      "Training batch loss 38140 0.19632495939731598\n",
      "Training batch loss 38160 0.19872212409973145\n",
      "Training batch loss 38180 0.2011108696460724\n",
      "Training batch loss 38200 0.19334366917610168\n",
      "Training batch loss 38220 0.2011612057685852\n",
      "Training batch loss 38240 0.18688777089118958\n",
      "Training batch loss 38260 0.19738690555095673\n",
      "Training batch loss 38280 0.20211857557296753\n",
      "Training batch loss 38300 0.19543080031871796\n",
      "Training batch loss 38320 0.20585793256759644\n",
      "Training batch loss 38340 0.19723135232925415\n",
      "Training batch loss 38360 0.19044041633605957\n",
      "Training batch loss 38380 0.20283818244934082\n",
      "Training batch loss 38400 0.19655489921569824\n",
      "Training batch loss 38420 0.1955663561820984\n",
      "Training batch loss 38440 0.20146483182907104\n",
      "Training batch loss 38460 0.1988999843597412\n",
      "Training batch loss 38480 0.19841943681240082\n",
      "Training batch loss 38500 0.20389148592948914\n",
      "Training batch loss 38520 0.19226862490177155\n",
      "Training batch loss 38540 0.2035389244556427\n",
      "Training batch loss 38560 0.19257719814777374\n",
      "Training batch loss 38580 0.1931302547454834\n",
      "Training batch loss 38600 0.19947470724582672\n",
      "Training batch loss 38620 0.19628626108169556\n",
      "Training batch loss 38640 0.20586371421813965\n",
      "Training batch loss 38660 0.1968308389186859\n",
      "Training batch loss 38680 0.20596987009048462\n",
      "Training batch loss 38700 0.1997758150100708\n",
      "Training batch loss 38720 0.20147401094436646\n",
      "Training batch loss 38740 0.19950464367866516\n",
      "Training batch loss 38760 0.1979617178440094\n",
      "Training batch loss 38780 0.20617517828941345\n",
      "Training batch loss 38800 0.2074558138847351\n",
      "Training batch loss 38820 0.19798772037029266\n",
      "Training batch loss 38840 0.20131948590278625\n",
      "Training batch loss 38860 0.21275150775909424\n",
      "Training batch loss 38880 0.19735084474086761\n",
      "Training batch loss 38900 0.2057875394821167\n",
      "Training batch loss 38920 0.2038630247116089\n",
      "Training batch loss 38940 0.1960001289844513\n",
      "Training batch loss 38960 0.19405975937843323\n",
      "Training batch loss 38980 0.20012935996055603\n",
      "Training batch loss 39000 0.20398616790771484\n",
      "Training batch loss 39020 0.19369086623191833\n",
      "Training batch loss 39040 0.20635294914245605\n",
      "Training batch loss 39060 0.1984732449054718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss 39080 0.20124468207359314\n",
      "Training batch loss 39100 0.18851247429847717\n",
      "Training batch loss 39120 0.19849780201911926\n",
      "Training batch loss 39140 0.20473094284534454\n",
      "Training batch loss 39160 0.19965031743049622\n",
      "Training batch loss 39180 0.20437705516815186\n",
      "Training batch loss 39200 0.19127696752548218\n",
      "Training batch loss 39220 0.1965131014585495\n",
      "Training batch loss 39240 0.195701465010643\n",
      "Training batch loss 39260 0.19478952884674072\n",
      "Training batch loss 39280 0.201440691947937\n",
      "Training epoch loss 28 0.19857197999954224\n",
      "Test loss during training 28 0.1976657658815384\n",
      "Test accuracy during training 28 0.0\n",
      "Training batch loss 39295 0.19170010089874268\n",
      "Training batch loss 39315 0.19589971005916595\n",
      "Training batch loss 39335 0.20131264626979828\n",
      "Training batch loss 39355 0.1983058601617813\n",
      "Training batch loss 39375 0.1991705298423767\n",
      "Training batch loss 39395 0.2009793221950531\n",
      "Training batch loss 39415 0.1980592906475067\n",
      "Training batch loss 39435 0.19715023040771484\n",
      "Training batch loss 39455 0.19260242581367493\n",
      "Training batch loss 39475 0.19880345463752747\n",
      "Training batch loss 39495 0.19632495939731598\n",
      "Training batch loss 39515 0.19872212409973145\n",
      "Training batch loss 39535 0.2011108696460724\n",
      "Training batch loss 39555 0.19334366917610168\n",
      "Training batch loss 39575 0.2011612057685852\n",
      "Training batch loss 39595 0.18688777089118958\n",
      "Training batch loss 39615 0.19738690555095673\n",
      "Training batch loss 39635 0.20211857557296753\n",
      "Training batch loss 39655 0.19543080031871796\n",
      "Training batch loss 39675 0.20585793256759644\n",
      "Training batch loss 39695 0.19723135232925415\n",
      "Training batch loss 39715 0.19044041633605957\n",
      "Training batch loss 39735 0.20283818244934082\n",
      "Training batch loss 39755 0.19655489921569824\n",
      "Training batch loss 39775 0.1955663561820984\n",
      "Training batch loss 39795 0.20146483182907104\n",
      "Training batch loss 39815 0.1988999843597412\n",
      "Training batch loss 39835 0.19841943681240082\n",
      "Training batch loss 39855 0.20389148592948914\n",
      "Training batch loss 39875 0.19226862490177155\n",
      "Training batch loss 39895 0.2035389244556427\n",
      "Training batch loss 39915 0.19257719814777374\n",
      "Training batch loss 39935 0.1931302547454834\n",
      "Training batch loss 39955 0.19947470724582672\n",
      "Training batch loss 39975 0.19628626108169556\n",
      "Training batch loss 39995 0.20586371421813965\n",
      "Training batch loss 40015 0.1968308389186859\n",
      "Training batch loss 40035 0.20596987009048462\n",
      "Training batch loss 40055 0.1997758150100708\n",
      "Training batch loss 40075 0.20147401094436646\n",
      "Training batch loss 40095 0.19950464367866516\n",
      "Training batch loss 40115 0.1979617178440094\n",
      "Training batch loss 40135 0.20617517828941345\n",
      "Training batch loss 40155 0.2074558138847351\n",
      "Training batch loss 40175 0.19798772037029266\n",
      "Training batch loss 40195 0.20131948590278625\n",
      "Training batch loss 40215 0.21275150775909424\n",
      "Training batch loss 40235 0.19735084474086761\n",
      "Training batch loss 40255 0.2057875394821167\n",
      "Training batch loss 40275 0.2038630247116089\n",
      "Training batch loss 40295 0.1960001289844513\n",
      "Training batch loss 40315 0.19405975937843323\n",
      "Training batch loss 40335 0.20012935996055603\n",
      "Training batch loss 40355 0.20398616790771484\n",
      "Training batch loss 40375 0.19369086623191833\n",
      "Training batch loss 40395 0.20635294914245605\n",
      "Training batch loss 40415 0.1984732449054718\n",
      "Training batch loss 40435 0.20124468207359314\n",
      "Training batch loss 40455 0.18851247429847717\n",
      "Training batch loss 40475 0.19849780201911926\n",
      "Training batch loss 40495 0.20473094284534454\n",
      "Training batch loss 40515 0.19965031743049622\n",
      "Training batch loss 40535 0.20437705516815186\n",
      "Training batch loss 40555 0.19127696752548218\n",
      "Training batch loss 40575 0.1965131014585495\n",
      "Training batch loss 40595 0.195701465010643\n",
      "Training batch loss 40615 0.19478952884674072\n",
      "Training batch loss 40635 0.201440691947937\n",
      "Training epoch loss 29 0.19857197999954224\n",
      "Test loss during training 29 0.1976657658815384\n",
      "Test accuracy during training 29 0.0\n",
      "Training batch loss 40650 0.19170010089874268\n",
      "Training batch loss 40670 0.19589971005916595\n",
      "Training batch loss 40690 0.20131264626979828\n",
      "Training batch loss 40710 0.1983058601617813\n",
      "Training batch loss 40730 0.1991705298423767\n",
      "Training batch loss 40750 0.2009793221950531\n",
      "Training batch loss 40770 0.1980592906475067\n",
      "Training batch loss 40790 0.19715023040771484\n",
      "Training batch loss 40810 0.19260242581367493\n",
      "Training batch loss 40830 0.19880345463752747\n",
      "Training batch loss 40850 0.19632495939731598\n",
      "Training batch loss 40870 0.19872212409973145\n",
      "Training batch loss 40890 0.2011108696460724\n",
      "Training batch loss 40910 0.19334366917610168\n",
      "Training batch loss 40930 0.2011612057685852\n",
      "Training batch loss 40950 0.18688777089118958\n",
      "Training batch loss 40970 0.19738690555095673\n",
      "Training batch loss 40990 0.20211857557296753\n",
      "Training batch loss 41010 0.19543080031871796\n",
      "Training batch loss 41030 0.20585793256759644\n",
      "Training batch loss 41050 0.19723135232925415\n",
      "Training batch loss 41070 0.19044041633605957\n",
      "Training batch loss 41090 0.20283818244934082\n",
      "Training batch loss 41110 0.19655489921569824\n",
      "Training batch loss 41130 0.1955663561820984\n",
      "Training batch loss 41150 0.20146483182907104\n",
      "Training batch loss 41170 0.1988999843597412\n",
      "Training batch loss 41190 0.19841943681240082\n",
      "Training batch loss 41210 0.20389148592948914\n",
      "Training batch loss 41230 0.19226862490177155\n",
      "Training batch loss 41250 0.2035389244556427\n",
      "Training batch loss 41270 0.19257719814777374\n",
      "Training batch loss 41290 0.1931302547454834\n",
      "Training batch loss 41310 0.19947470724582672\n",
      "Training batch loss 41330 0.19628626108169556\n",
      "Training batch loss 41350 0.20586371421813965\n",
      "Training batch loss 41370 0.1968308389186859\n",
      "Training batch loss 41390 0.20596987009048462\n",
      "Training batch loss 41410 0.1997758150100708\n",
      "Training batch loss 41430 0.20147401094436646\n",
      "Training batch loss 41450 0.19950464367866516\n",
      "Training batch loss 41470 0.1979617178440094\n",
      "Training batch loss 41490 0.20617517828941345\n",
      "Training batch loss 41510 0.2074558138847351\n",
      "Training batch loss 41530 0.19798772037029266\n",
      "Training batch loss 41550 0.20131948590278625\n",
      "Training batch loss 41570 0.21275150775909424\n",
      "Training batch loss 41590 0.19735084474086761\n",
      "Training batch loss 41610 0.2057875394821167\n",
      "Training batch loss 41630 0.2038630247116089\n",
      "Training batch loss 41650 0.1960001289844513\n",
      "Training batch loss 41670 0.19405975937843323\n",
      "Training batch loss 41690 0.20012935996055603\n",
      "Training batch loss 41710 0.20398616790771484\n",
      "Training batch loss 41730 0.19369086623191833\n",
      "Training batch loss 41750 0.20635294914245605\n",
      "Training batch loss 41770 0.1984732449054718\n",
      "Training batch loss 41790 0.20124468207359314\n",
      "Training batch loss 41810 0.18851247429847717\n",
      "Training batch loss 41830 0.19849780201911926\n",
      "Training batch loss 41850 0.20473094284534454\n",
      "Training batch loss 41870 0.19965031743049622\n",
      "Training batch loss 41890 0.20437705516815186\n",
      "Training batch loss 41910 0.19127696752548218\n",
      "Training batch loss 41930 0.1965131014585495\n",
      "Training batch loss 41950 0.195701465010643\n",
      "Training batch loss 41970 0.19478952884674072\n",
      "Training batch loss 41990 0.201440691947937\n",
      "Training epoch loss 30 0.19857197999954224\n",
      "Test loss during training 30 0.1976657658815384\n",
      "Test accuracy during training 30 0.0\n",
      "Training batch loss 42005 0.19170010089874268\n",
      "Training batch loss 42025 0.19589971005916595\n",
      "Training batch loss 42045 0.20131264626979828\n",
      "Training batch loss 42065 0.1983058601617813\n",
      "Training batch loss 42085 0.1991705298423767\n",
      "Training batch loss 42105 0.2009793221950531\n",
      "Training batch loss 42125 0.1980592906475067\n",
      "Training batch loss 42145 0.19715023040771484\n",
      "Training batch loss 42165 0.19260242581367493\n",
      "Training batch loss 42185 0.19880345463752747\n",
      "Training batch loss 42205 0.19632495939731598\n",
      "Training batch loss 42225 0.19872212409973145\n",
      "Training batch loss 42245 0.2011108696460724\n",
      "Training batch loss 42265 0.19334366917610168\n",
      "Training batch loss 42285 0.2011612057685852\n",
      "Training batch loss 42305 0.18688777089118958\n",
      "Training batch loss 42325 0.19738690555095673\n",
      "Training batch loss 42345 0.20211857557296753\n",
      "Training batch loss 42365 0.19543080031871796\n",
      "Training batch loss 42385 0.20585793256759644\n",
      "Training batch loss 42405 0.19723135232925415\n",
      "Training batch loss 42425 0.19044041633605957\n",
      "Training batch loss 42445 0.20283818244934082\n",
      "Training batch loss 42465 0.19655489921569824\n",
      "Training batch loss 42485 0.1955663561820984\n",
      "Training batch loss 42505 0.20146483182907104\n",
      "Training batch loss 42525 0.1988999843597412\n",
      "Training batch loss 42545 0.19841943681240082\n",
      "Training batch loss 42565 0.20389148592948914\n",
      "Training batch loss 42585 0.19226862490177155\n",
      "Training batch loss 42605 0.2035389244556427\n",
      "Training batch loss 42625 0.19257719814777374\n",
      "Training batch loss 42645 0.1931302547454834\n",
      "Training batch loss 42665 0.19947470724582672\n",
      "Training batch loss 42685 0.19628626108169556\n",
      "Training batch loss 42705 0.20586371421813965\n",
      "Training batch loss 42725 0.1968308389186859\n",
      "Training batch loss 42745 0.20596987009048462\n",
      "Training batch loss 42765 0.1997758150100708\n",
      "Training batch loss 42785 0.20147401094436646\n",
      "Training batch loss 42805 0.19950464367866516\n",
      "Training batch loss 42825 0.1979617178440094\n",
      "Training batch loss 42845 0.20617517828941345\n",
      "Training batch loss 42865 0.2074558138847351\n",
      "Training batch loss 42885 0.19798772037029266\n",
      "Training batch loss 42905 0.20131948590278625\n",
      "Training batch loss 42925 0.21275150775909424\n",
      "Training batch loss 42945 0.19735084474086761\n",
      "Training batch loss 42965 0.2057875394821167\n",
      "Training batch loss 42985 0.2038630247116089\n",
      "Training batch loss 43005 0.1960001289844513\n",
      "Training batch loss 43025 0.19405975937843323\n",
      "Training batch loss 43045 0.20012935996055603\n",
      "Training batch loss 43065 0.20398616790771484\n",
      "Training batch loss 43085 0.19369086623191833\n",
      "Training batch loss 43105 0.20635294914245605\n",
      "Training batch loss 43125 0.1984732449054718\n",
      "Training batch loss 43145 0.20124468207359314\n",
      "Training batch loss 43165 0.18851247429847717\n",
      "Training batch loss 43185 0.19849780201911926\n",
      "Training batch loss 43205 0.20473094284534454\n",
      "Training batch loss 43225 0.19965031743049622\n",
      "Training batch loss 43245 0.20437705516815186\n",
      "Training batch loss 43265 0.19127696752548218\n",
      "Training batch loss 43285 0.1965131014585495\n",
      "Training batch loss 43305 0.195701465010643\n",
      "Training batch loss 43325 0.19478952884674072\n",
      "Training batch loss 43345 0.201440691947937\n",
      "Training epoch loss 31 0.19857197999954224\n",
      "Test loss during training 31 0.19766592979431152\n",
      "Test accuracy during training 31 0.0\n",
      "Training batch loss 43360 0.19170010089874268\n",
      "Training batch loss 43380 0.19589971005916595\n",
      "Training batch loss 43400 0.20131264626979828\n",
      "Training batch loss 43420 0.1983058601617813\n",
      "Training batch loss 43440 0.1991705298423767\n",
      "Training batch loss 43460 0.2009793221950531\n",
      "Training batch loss 43480 0.1980592906475067\n",
      "Training batch loss 43500 0.19715023040771484\n",
      "Training batch loss 43520 0.19260242581367493\n",
      "Training batch loss 43540 0.19880345463752747\n",
      "Training batch loss 43560 0.19632495939731598\n",
      "Training batch loss 43580 0.19872212409973145\n",
      "Training batch loss 43600 0.2011108696460724\n",
      "Training batch loss 43620 0.19334366917610168\n",
      "Training batch loss 43640 0.2011612057685852\n",
      "Training batch loss 43660 0.18688777089118958\n",
      "Training batch loss 43680 0.19738690555095673\n",
      "Training batch loss 43700 0.20211857557296753\n",
      "Training batch loss 43720 0.19543080031871796\n",
      "Training batch loss 43740 0.20585793256759644\n",
      "Training batch loss 43760 0.19723135232925415\n",
      "Training batch loss 43780 0.19044041633605957\n",
      "Training batch loss 43800 0.20283818244934082\n",
      "Training batch loss 43820 0.19655489921569824\n",
      "Training batch loss 43840 0.1955663561820984\n",
      "Training batch loss 43860 0.20146483182907104\n",
      "Training batch loss 43880 0.1988999843597412\n",
      "Training batch loss 43900 0.19841943681240082\n",
      "Training batch loss 43920 0.20389148592948914\n",
      "Training batch loss 43940 0.19226862490177155\n",
      "Training batch loss 43960 0.2035389244556427\n",
      "Training batch loss 43980 0.19257719814777374\n",
      "Training batch loss 44000 0.1931302547454834\n",
      "Training batch loss 44020 0.19947470724582672\n",
      "Training batch loss 44040 0.19628626108169556\n",
      "Training batch loss 44060 0.20586371421813965\n",
      "Training batch loss 44080 0.1968308389186859\n",
      "Training batch loss 44100 0.20596987009048462\n",
      "Training batch loss 44120 0.1997758150100708\n",
      "Training batch loss 44140 0.20147401094436646\n",
      "Training batch loss 44160 0.19950464367866516\n",
      "Training batch loss 44180 0.1979617178440094\n",
      "Training batch loss 44200 0.20617517828941345\n",
      "Training batch loss 44220 0.2074558138847351\n",
      "Training batch loss 44240 0.19798772037029266\n",
      "Training batch loss 44260 0.20131948590278625\n",
      "Training batch loss 44280 0.21275150775909424\n",
      "Training batch loss 44300 0.19735084474086761\n",
      "Training batch loss 44320 0.2057875394821167\n",
      "Training batch loss 44340 0.2038630247116089\n",
      "Training batch loss 44360 0.1960001289844513\n",
      "Training batch loss 44380 0.19405975937843323\n",
      "Training batch loss 44400 0.20012935996055603\n",
      "Training batch loss 44420 0.20398616790771484\n",
      "Training batch loss 44440 0.19369086623191833\n",
      "Training batch loss 44460 0.20635294914245605\n",
      "Training batch loss 44480 0.1984732449054718\n",
      "Training batch loss 44500 0.20124468207359314\n",
      "Training batch loss 44520 0.18851247429847717\n",
      "Training batch loss 44540 0.19849780201911926\n",
      "Training batch loss 44560 0.20473094284534454\n",
      "Training batch loss 44580 0.19965031743049622\n",
      "Training batch loss 44600 0.20437705516815186\n",
      "Training batch loss 44620 0.19127696752548218\n",
      "Training batch loss 44640 0.1965131014585495\n",
      "Training batch loss 44660 0.195701465010643\n",
      "Training batch loss 44680 0.19478952884674072\n",
      "Training batch loss 44700 0.201440691947937\n",
      "Training epoch loss 32 0.19857197999954224\n",
      "Test loss during training 32 0.1976657658815384\n",
      "Test accuracy during training 32 0.0\n",
      "Training batch loss 44715 0.19170010089874268\n",
      "Training batch loss 44735 0.19589971005916595\n",
      "Training batch loss 44755 0.20131264626979828\n",
      "Training batch loss 44775 0.1983058601617813\n",
      "Training batch loss 44795 0.1991705298423767\n",
      "Training batch loss 44815 0.2009793221950531\n",
      "Training batch loss 44835 0.1980592906475067\n",
      "Training batch loss 44855 0.19715023040771484\n",
      "Training batch loss 44875 0.19260242581367493\n",
      "Training batch loss 44895 0.19880345463752747\n",
      "Training batch loss 44915 0.19632495939731598\n",
      "Training batch loss 44935 0.19872212409973145\n",
      "Training batch loss 44955 0.2011108696460724\n",
      "Training batch loss 44975 0.19334366917610168\n",
      "Training batch loss 44995 0.2011612057685852\n",
      "Training batch loss 45015 0.18688777089118958\n",
      "Training batch loss 45035 0.19738690555095673\n",
      "Training batch loss 45055 0.20211857557296753\n",
      "Training batch loss 45075 0.19543080031871796\n",
      "Training batch loss 45095 0.20585793256759644\n",
      "Training batch loss 45115 0.19723135232925415\n",
      "Training batch loss 45135 0.19044041633605957\n",
      "Training batch loss 45155 0.20283818244934082\n",
      "Training batch loss 45175 0.19655489921569824\n",
      "Training batch loss 45195 0.1955663561820984\n",
      "Training batch loss 45215 0.20146483182907104\n",
      "Training batch loss 45235 0.1988999843597412\n",
      "Training batch loss 45255 0.19841943681240082\n",
      "Training batch loss 45275 0.20389148592948914\n",
      "Training batch loss 45295 0.19226862490177155\n",
      "Training batch loss 45315 0.2035389244556427\n",
      "Training batch loss 45335 0.19257719814777374\n",
      "Training batch loss 45355 0.1931302547454834\n",
      "Training batch loss 45375 0.19947470724582672\n",
      "Training batch loss 45395 0.19628626108169556\n",
      "Training batch loss 45415 0.20586371421813965\n",
      "Training batch loss 45435 0.1968308389186859\n",
      "Training batch loss 45455 0.20596987009048462\n",
      "Training batch loss 45475 0.1997758150100708\n",
      "Training batch loss 45495 0.20147401094436646\n",
      "Training batch loss 45515 0.19950464367866516\n",
      "Training batch loss 45535 0.1979617178440094\n",
      "Training batch loss 45555 0.20617517828941345\n",
      "Training batch loss 45575 0.2074558138847351\n",
      "Training batch loss 45595 0.19798772037029266\n",
      "Training batch loss 45615 0.20131948590278625\n",
      "Training batch loss 45635 0.21275150775909424\n",
      "Training batch loss 45655 0.19735084474086761\n",
      "Training batch loss 45675 0.2057875394821167\n",
      "Training batch loss 45695 0.2038630247116089\n",
      "Training batch loss 45715 0.1960001289844513\n",
      "Training batch loss 45735 0.19405975937843323\n",
      "Training batch loss 45755 0.20012935996055603\n",
      "Training batch loss 45775 0.20398616790771484\n",
      "Training batch loss 45795 0.19369086623191833\n",
      "Training batch loss 45815 0.20635294914245605\n",
      "Training batch loss 45835 0.1984732449054718\n",
      "Training batch loss 45855 0.20124468207359314\n",
      "Training batch loss 45875 0.18851247429847717\n",
      "Training batch loss 45895 0.19849780201911926\n",
      "Training batch loss 45915 0.20473094284534454\n",
      "Training batch loss 45935 0.19965031743049622\n",
      "Training batch loss 45955 0.20437705516815186\n",
      "Training batch loss 45975 0.19127696752548218\n",
      "Training batch loss 45995 0.1965131014585495\n",
      "Training batch loss 46015 0.195701465010643\n",
      "Training batch loss 46035 0.19478952884674072\n",
      "Training batch loss 46055 0.201440691947937\n",
      "Training epoch loss 33 0.19857197999954224\n",
      "Test loss during training 33 0.1976657658815384\n",
      "Test accuracy during training 33 0.0\n",
      "Training batch loss 46070 0.19170010089874268\n",
      "Training batch loss 46090 0.19589971005916595\n",
      "Training batch loss 46110 0.20131264626979828\n",
      "Training batch loss 46130 0.1983058601617813\n",
      "Training batch loss 46150 0.1991705298423767\n",
      "Training batch loss 46170 0.2009793221950531\n",
      "Training batch loss 46190 0.1980592906475067\n",
      "Training batch loss 46210 0.19715023040771484\n",
      "Training batch loss 46230 0.19260242581367493\n",
      "Training batch loss 46250 0.19880345463752747\n",
      "Training batch loss 46270 0.19632495939731598\n",
      "Training batch loss 46290 0.19872212409973145\n",
      "Training batch loss 46310 0.2011108696460724\n",
      "Training batch loss 46330 0.19334366917610168\n",
      "Training batch loss 46350 0.2011612057685852\n",
      "Training batch loss 46370 0.18688777089118958\n",
      "Training batch loss 46390 0.19738690555095673\n",
      "Training batch loss 46410 0.20211857557296753\n",
      "Training batch loss 46430 0.19543080031871796\n",
      "Training batch loss 46450 0.20585793256759644\n",
      "Training batch loss 46470 0.19723135232925415\n",
      "Training batch loss 46490 0.19044041633605957\n",
      "Training batch loss 46510 0.20283818244934082\n",
      "Training batch loss 46530 0.19655489921569824\n",
      "Training batch loss 46550 0.1955663561820984\n",
      "Training batch loss 46570 0.20146483182907104\n",
      "Training batch loss 46590 0.1988999843597412\n",
      "Training batch loss 46610 0.19841943681240082\n",
      "Training batch loss 46630 0.20389148592948914\n",
      "Training batch loss 46650 0.19226862490177155\n",
      "Training batch loss 46670 0.2035389244556427\n",
      "Training batch loss 46690 0.19257719814777374\n",
      "Training batch loss 46710 0.1931302547454834\n",
      "Training batch loss 46730 0.19947470724582672\n",
      "Training batch loss 46750 0.19628626108169556\n",
      "Training batch loss 46770 0.20586371421813965\n",
      "Training batch loss 46790 0.1968308389186859\n",
      "Training batch loss 46810 0.20596987009048462\n",
      "Training batch loss 46830 0.1997758150100708\n",
      "Training batch loss 46850 0.20147401094436646\n",
      "Training batch loss 46870 0.19950464367866516\n",
      "Training batch loss 46890 0.1979617178440094\n",
      "Training batch loss 46910 0.20617517828941345\n",
      "Training batch loss 46930 0.2074558138847351\n",
      "Training batch loss 46950 0.19798772037029266\n",
      "Training batch loss 46970 0.20131948590278625\n",
      "Training batch loss 46990 0.21275150775909424\n",
      "Training batch loss 47010 0.19735084474086761\n",
      "Training batch loss 47030 0.2057875394821167\n",
      "Training batch loss 47050 0.2038630247116089\n",
      "Training batch loss 47070 0.1960001289844513\n",
      "Training batch loss 47090 0.19405975937843323\n",
      "Training batch loss 47110 0.20012935996055603\n",
      "Training batch loss 47130 0.20398616790771484\n",
      "Training batch loss 47150 0.19369086623191833\n",
      "Training batch loss 47170 0.20635294914245605\n",
      "Training batch loss 47190 0.1984732449054718\n",
      "Training batch loss 47210 0.20124468207359314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss 47230 0.18851247429847717\n",
      "Training batch loss 47250 0.19849780201911926\n",
      "Training batch loss 47270 0.20473094284534454\n",
      "Training batch loss 47290 0.19965031743049622\n",
      "Training batch loss 47310 0.20437705516815186\n",
      "Training batch loss 47330 0.19127696752548218\n",
      "Training batch loss 47350 0.1965131014585495\n",
      "Training batch loss 47370 0.195701465010643\n",
      "Training batch loss 47390 0.19478952884674072\n",
      "Training batch loss 47410 0.201440691947937\n",
      "Training epoch loss 34 0.19857197999954224\n",
      "Test loss during training 34 0.1976657658815384\n",
      "Test accuracy during training 34 0.0\n",
      "Training batch loss 47425 0.19170010089874268\n",
      "Training batch loss 47445 0.19589971005916595\n",
      "Training batch loss 47465 0.20131264626979828\n",
      "Training batch loss 47485 0.1983058601617813\n",
      "Training batch loss 47505 0.1991705298423767\n",
      "Training batch loss 47525 0.2009793221950531\n",
      "Training batch loss 47545 0.1980592906475067\n",
      "Training batch loss 47565 0.19715023040771484\n",
      "Training batch loss 47585 0.19260242581367493\n",
      "Training batch loss 47605 0.19880345463752747\n",
      "Training batch loss 47625 0.19632495939731598\n",
      "Training batch loss 47645 0.19872212409973145\n",
      "Training batch loss 47665 0.2011108696460724\n",
      "Training batch loss 47685 0.19334366917610168\n",
      "Training batch loss 47705 0.2011612057685852\n",
      "Training batch loss 47725 0.18688777089118958\n",
      "Training batch loss 47745 0.19738690555095673\n",
      "Training batch loss 47765 0.20211857557296753\n",
      "Training batch loss 47785 0.19543080031871796\n",
      "Training batch loss 47805 0.20585793256759644\n",
      "Training batch loss 47825 0.19723135232925415\n",
      "Training batch loss 47845 0.19044041633605957\n",
      "Training batch loss 47865 0.20283818244934082\n",
      "Training batch loss 47885 0.19655489921569824\n",
      "Training batch loss 47905 0.1955663561820984\n",
      "Training batch loss 47925 0.20146483182907104\n",
      "Training batch loss 47945 0.1988999843597412\n",
      "Training batch loss 47965 0.19841943681240082\n",
      "Training batch loss 47985 0.20389148592948914\n",
      "Training batch loss 48005 0.19226862490177155\n",
      "Training batch loss 48025 0.2035389244556427\n",
      "Training batch loss 48045 0.19257719814777374\n",
      "Training batch loss 48065 0.1931302547454834\n",
      "Training batch loss 48085 0.19947470724582672\n",
      "Training batch loss 48105 0.19628626108169556\n",
      "Training batch loss 48125 0.20586371421813965\n",
      "Training batch loss 48145 0.1968308389186859\n",
      "Training batch loss 48165 0.20596987009048462\n",
      "Training batch loss 48185 0.1997758150100708\n",
      "Training batch loss 48205 0.20147401094436646\n",
      "Training batch loss 48225 0.19950464367866516\n",
      "Training batch loss 48245 0.1979617178440094\n",
      "Training batch loss 48265 0.20617517828941345\n",
      "Training batch loss 48285 0.2074558138847351\n",
      "Training batch loss 48305 0.19798772037029266\n",
      "Training batch loss 48325 0.20131948590278625\n",
      "Training batch loss 48345 0.21275150775909424\n",
      "Training batch loss 48365 0.19735084474086761\n",
      "Training batch loss 48385 0.2057875394821167\n",
      "Training batch loss 48405 0.2038630247116089\n",
      "Training batch loss 48425 0.1960001289844513\n",
      "Training batch loss 48445 0.19405975937843323\n",
      "Training batch loss 48465 0.20012935996055603\n",
      "Training batch loss 48485 0.20398616790771484\n",
      "Training batch loss 48505 0.19369086623191833\n",
      "Training batch loss 48525 0.20635294914245605\n",
      "Training batch loss 48545 0.1984732449054718\n",
      "Training batch loss 48565 0.20124468207359314\n",
      "Training batch loss 48585 0.18851247429847717\n",
      "Training batch loss 48605 0.19849780201911926\n",
      "Training batch loss 48625 0.20473094284534454\n",
      "Training batch loss 48645 0.19965031743049622\n",
      "Training batch loss 48665 0.20437705516815186\n",
      "Training batch loss 48685 0.19127696752548218\n",
      "Training batch loss 48705 0.1965131014585495\n",
      "Training batch loss 48725 0.195701465010643\n",
      "Training batch loss 48745 0.19478952884674072\n",
      "Training batch loss 48765 0.201440691947937\n",
      "Training epoch loss 35 0.19857197999954224\n",
      "Test loss during training 35 0.1976657658815384\n",
      "Test accuracy during training 35 0.0\n",
      "Training batch loss 48780 0.19170010089874268\n",
      "Training batch loss 48800 0.19589971005916595\n",
      "Training batch loss 48820 0.20131264626979828\n",
      "Training batch loss 48840 0.1983058601617813\n",
      "Training batch loss 48860 0.1991705298423767\n",
      "Training batch loss 48880 0.2009793221950531\n",
      "Training batch loss 48900 0.1980592906475067\n",
      "Training batch loss 48920 0.19715023040771484\n",
      "Training batch loss 48940 0.19260242581367493\n",
      "Training batch loss 48960 0.19880345463752747\n",
      "Training batch loss 48980 0.19632495939731598\n",
      "Training batch loss 49000 0.19872212409973145\n",
      "Training batch loss 49020 0.2011108696460724\n",
      "Training batch loss 49040 0.19334366917610168\n",
      "Training batch loss 49060 0.2011612057685852\n",
      "Training batch loss 49080 0.18688777089118958\n",
      "Training batch loss 49100 0.19738690555095673\n",
      "Training batch loss 49120 0.20211857557296753\n",
      "Training batch loss 49140 0.19543080031871796\n",
      "Training batch loss 49160 0.20585793256759644\n",
      "Training batch loss 49180 0.19723135232925415\n",
      "Training batch loss 49200 0.19044041633605957\n",
      "Training batch loss 49220 0.20283818244934082\n",
      "Training batch loss 49240 0.19655489921569824\n",
      "Training batch loss 49260 0.1955663561820984\n",
      "Training batch loss 49280 0.20146483182907104\n",
      "Training batch loss 49300 0.1988999843597412\n",
      "Training batch loss 49320 0.19841943681240082\n",
      "Training batch loss 49340 0.20389148592948914\n",
      "Training batch loss 49360 0.19226862490177155\n",
      "Training batch loss 49380 0.2035389244556427\n",
      "Training batch loss 49400 0.19257719814777374\n",
      "Training batch loss 49420 0.1931302547454834\n",
      "Training batch loss 49440 0.19947470724582672\n",
      "Training batch loss 49460 0.19628626108169556\n",
      "Training batch loss 49480 0.20586371421813965\n",
      "Training batch loss 49500 0.1968308389186859\n",
      "Training batch loss 49520 0.20596987009048462\n",
      "Training batch loss 49540 0.1997758150100708\n",
      "Training batch loss 49560 0.20147401094436646\n",
      "Training batch loss 49580 0.19950464367866516\n",
      "Training batch loss 49600 0.1979617178440094\n",
      "Training batch loss 49620 0.20617517828941345\n",
      "Training batch loss 49640 0.2074558138847351\n",
      "Training batch loss 49660 0.19798772037029266\n",
      "Training batch loss 49680 0.20131948590278625\n",
      "Training batch loss 49700 0.21275150775909424\n",
      "Training batch loss 49720 0.19735084474086761\n",
      "Training batch loss 49740 0.2057875394821167\n",
      "Training batch loss 49760 0.2038630247116089\n",
      "Training batch loss 49780 0.1960001289844513\n",
      "Training batch loss 49800 0.19405975937843323\n",
      "Training batch loss 49820 0.20012935996055603\n",
      "Training batch loss 49840 0.20398616790771484\n",
      "Training batch loss 49860 0.19369086623191833\n",
      "Training batch loss 49880 0.20635294914245605\n",
      "Training batch loss 49900 0.1984732449054718\n",
      "Training batch loss 49920 0.20124468207359314\n",
      "Training batch loss 49940 0.18851247429847717\n",
      "Training batch loss 49960 0.19849780201911926\n",
      "Training batch loss 49980 0.20473094284534454\n",
      "Training batch loss 50000 0.19965031743049622\n",
      "Training batch loss 50020 0.20437705516815186\n",
      "Training batch loss 50040 0.19127696752548218\n",
      "Training batch loss 50060 0.1965131014585495\n",
      "Training batch loss 50080 0.195701465010643\n",
      "Training batch loss 50100 0.19478952884674072\n",
      "Training batch loss 50120 0.201440691947937\n",
      "Training epoch loss 36 0.19857197999954224\n",
      "Test loss during training 36 0.1976657658815384\n",
      "Test accuracy during training 36 0.0\n",
      "Training batch loss 50135 0.19170010089874268\n",
      "Training batch loss 50155 0.19589971005916595\n",
      "Training batch loss 50175 0.20131264626979828\n",
      "Training batch loss 50195 0.1983058601617813\n",
      "Training batch loss 50215 0.1991705298423767\n",
      "Training batch loss 50235 0.2009793221950531\n",
      "Training batch loss 50255 0.1980592906475067\n",
      "Training batch loss 50275 0.19715023040771484\n",
      "Training batch loss 50295 0.19260242581367493\n",
      "Training batch loss 50315 0.19880345463752747\n",
      "Training batch loss 50335 0.19632495939731598\n",
      "Training batch loss 50355 0.19872212409973145\n",
      "Training batch loss 50375 0.2011108696460724\n",
      "Training batch loss 50395 0.19334366917610168\n",
      "Training batch loss 50415 0.2011612057685852\n",
      "Training batch loss 50435 0.18688777089118958\n",
      "Training batch loss 50455 0.19738690555095673\n",
      "Training batch loss 50475 0.20211857557296753\n",
      "Training batch loss 50495 0.19543080031871796\n",
      "Training batch loss 50515 0.20585793256759644\n",
      "Training batch loss 50535 0.19723135232925415\n",
      "Training batch loss 50555 0.19044041633605957\n",
      "Training batch loss 50575 0.20283818244934082\n",
      "Training batch loss 50595 0.19655489921569824\n",
      "Training batch loss 50615 0.1955663561820984\n",
      "Training batch loss 50635 0.20146483182907104\n",
      "Training batch loss 50655 0.1988999843597412\n",
      "Training batch loss 50675 0.19841943681240082\n",
      "Training batch loss 50695 0.20389148592948914\n",
      "Training batch loss 50715 0.19226862490177155\n",
      "Training batch loss 50735 0.2035389244556427\n",
      "Training batch loss 50755 0.19257719814777374\n",
      "Training batch loss 50775 0.1931302547454834\n",
      "Training batch loss 50795 0.19947470724582672\n",
      "Training batch loss 50815 0.19628626108169556\n",
      "Training batch loss 50835 0.20586371421813965\n",
      "Training batch loss 50855 0.1968308389186859\n",
      "Training batch loss 50875 0.20596987009048462\n",
      "Training batch loss 50895 0.1997758150100708\n",
      "Training batch loss 50915 0.20147401094436646\n",
      "Training batch loss 50935 0.19950464367866516\n",
      "Training batch loss 50955 0.1979617178440094\n",
      "Training batch loss 50975 0.20617517828941345\n",
      "Training batch loss 50995 0.2074558138847351\n",
      "Training batch loss 51015 0.19798772037029266\n",
      "Training batch loss 51035 0.20131948590278625\n",
      "Training batch loss 51055 0.21275150775909424\n",
      "Training batch loss 51075 0.19735084474086761\n",
      "Training batch loss 51095 0.2057875394821167\n",
      "Training batch loss 51115 0.2038630247116089\n",
      "Training batch loss 51135 0.1960001289844513\n",
      "Training batch loss 51155 0.19405975937843323\n",
      "Training batch loss 51175 0.20012935996055603\n",
      "Training batch loss 51195 0.20398616790771484\n",
      "Training batch loss 51215 0.19369086623191833\n",
      "Training batch loss 51235 0.20635294914245605\n",
      "Training batch loss 51255 0.1984732449054718\n",
      "Training batch loss 51275 0.20124468207359314\n",
      "Training batch loss 51295 0.18851247429847717\n",
      "Training batch loss 51315 0.19849780201911926\n",
      "Training batch loss 51335 0.20473094284534454\n",
      "Training batch loss 51355 0.19965031743049622\n",
      "Training batch loss 51375 0.20437705516815186\n",
      "Training batch loss 51395 0.19127696752548218\n",
      "Training batch loss 51415 0.1965131014585495\n",
      "Training batch loss 51435 0.195701465010643\n",
      "Training batch loss 51455 0.19478952884674072\n",
      "Training batch loss 51475 0.201440691947937\n",
      "Training epoch loss 37 0.19857197999954224\n",
      "Test loss during training 37 0.1976657658815384\n",
      "Test accuracy during training 37 0.0\n",
      "Training batch loss 51490 0.19170010089874268\n",
      "Training batch loss 51510 0.19589971005916595\n",
      "Training batch loss 51530 0.20131264626979828\n",
      "Training batch loss 51550 0.1983058601617813\n",
      "Training batch loss 51570 0.1991705298423767\n",
      "Training batch loss 51590 0.2009793221950531\n",
      "Training batch loss 51610 0.1980592906475067\n",
      "Training batch loss 51630 0.19715023040771484\n",
      "Training batch loss 51650 0.19260242581367493\n",
      "Training batch loss 51670 0.19880345463752747\n",
      "Training batch loss 51690 0.19632495939731598\n",
      "Training batch loss 51710 0.19872212409973145\n",
      "Training batch loss 51730 0.2011108696460724\n",
      "Training batch loss 51750 0.19334366917610168\n",
      "Training batch loss 51770 0.2011612057685852\n",
      "Training batch loss 51790 0.18688777089118958\n",
      "Training batch loss 51810 0.19738690555095673\n",
      "Training batch loss 51830 0.20211857557296753\n",
      "Training batch loss 51850 0.19543080031871796\n",
      "Training batch loss 51870 0.20585793256759644\n",
      "Training batch loss 51890 0.19723135232925415\n",
      "Training batch loss 51910 0.19044041633605957\n",
      "Training batch loss 51930 0.20283818244934082\n",
      "Training batch loss 51950 0.19655489921569824\n",
      "Training batch loss 51970 0.1955663561820984\n",
      "Training batch loss 51990 0.20146483182907104\n",
      "Training batch loss 52010 0.1988999843597412\n",
      "Training batch loss 52030 0.19841943681240082\n",
      "Training batch loss 52050 0.20389148592948914\n",
      "Training batch loss 52070 0.19226862490177155\n",
      "Training batch loss 52090 0.2035389244556427\n",
      "Training batch loss 52110 0.19257719814777374\n",
      "Training batch loss 52130 0.1931302547454834\n",
      "Training batch loss 52150 0.19947470724582672\n",
      "Training batch loss 52170 0.19628626108169556\n",
      "Training batch loss 52190 0.20586371421813965\n",
      "Training batch loss 52210 0.1968308389186859\n",
      "Training batch loss 52230 0.20596987009048462\n",
      "Training batch loss 52250 0.1997758150100708\n",
      "Training batch loss 52270 0.20147401094436646\n",
      "Training batch loss 52290 0.19950464367866516\n",
      "Training batch loss 52310 0.1979617178440094\n",
      "Training batch loss 52330 0.20617517828941345\n",
      "Training batch loss 52350 0.2074558138847351\n",
      "Training batch loss 52370 0.19798772037029266\n",
      "Training batch loss 52390 0.20131948590278625\n",
      "Training batch loss 52410 0.21275150775909424\n",
      "Training batch loss 52430 0.19735084474086761\n",
      "Training batch loss 52450 0.2057875394821167\n",
      "Training batch loss 52470 0.2038630247116089\n",
      "Training batch loss 52490 0.1960001289844513\n",
      "Training batch loss 52510 0.19405975937843323\n",
      "Training batch loss 52530 0.20012935996055603\n",
      "Training batch loss 52550 0.20398616790771484\n",
      "Training batch loss 52570 0.19369086623191833\n",
      "Training batch loss 52590 0.20635294914245605\n",
      "Training batch loss 52610 0.1984732449054718\n",
      "Training batch loss 52630 0.20124468207359314\n",
      "Training batch loss 52650 0.18851247429847717\n",
      "Training batch loss 52670 0.19849780201911926\n",
      "Training batch loss 52690 0.20473094284534454\n",
      "Training batch loss 52710 0.19965031743049622\n",
      "Training batch loss 52730 0.20437705516815186\n",
      "Training batch loss 52750 0.19127696752548218\n",
      "Training batch loss 52770 0.1965131014585495\n",
      "Training batch loss 52790 0.195701465010643\n",
      "Training batch loss 52810 0.19478952884674072\n",
      "Training batch loss 52830 0.201440691947937\n",
      "Training epoch loss 38 0.19857197999954224\n",
      "Test loss during training 38 0.1976657658815384\n",
      "Test accuracy during training 38 0.0\n",
      "Training batch loss 52845 0.19170010089874268\n",
      "Training batch loss 52865 0.19589971005916595\n",
      "Training batch loss 52885 0.20131264626979828\n",
      "Training batch loss 52905 0.1983058601617813\n",
      "Training batch loss 52925 0.1991705298423767\n",
      "Training batch loss 52945 0.2009793221950531\n",
      "Training batch loss 52965 0.1980592906475067\n",
      "Training batch loss 52985 0.19715023040771484\n",
      "Training batch loss 53005 0.19260242581367493\n",
      "Training batch loss 53025 0.19880345463752747\n",
      "Training batch loss 53045 0.19632495939731598\n",
      "Training batch loss 53065 0.19872212409973145\n",
      "Training batch loss 53085 0.2011108696460724\n",
      "Training batch loss 53105 0.19334366917610168\n",
      "Training batch loss 53125 0.2011612057685852\n",
      "Training batch loss 53145 0.18688777089118958\n",
      "Training batch loss 53165 0.19738690555095673\n",
      "Training batch loss 53185 0.20211857557296753\n",
      "Training batch loss 53205 0.19543080031871796\n",
      "Training batch loss 53225 0.20585793256759644\n",
      "Training batch loss 53245 0.19723135232925415\n",
      "Training batch loss 53265 0.19044041633605957\n",
      "Training batch loss 53285 0.20283818244934082\n",
      "Training batch loss 53305 0.19655489921569824\n",
      "Training batch loss 53325 0.1955663561820984\n",
      "Training batch loss 53345 0.20146483182907104\n",
      "Training batch loss 53365 0.1988999843597412\n",
      "Training batch loss 53385 0.19841943681240082\n",
      "Training batch loss 53405 0.20389148592948914\n",
      "Training batch loss 53425 0.19226862490177155\n",
      "Training batch loss 53445 0.2035389244556427\n",
      "Training batch loss 53465 0.19257719814777374\n",
      "Training batch loss 53485 0.1931302547454834\n",
      "Training batch loss 53505 0.19947470724582672\n",
      "Training batch loss 53525 0.19628626108169556\n",
      "Training batch loss 53545 0.20586371421813965\n",
      "Training batch loss 53565 0.1968308389186859\n",
      "Training batch loss 53585 0.20596987009048462\n",
      "Training batch loss 53605 0.1997758150100708\n",
      "Training batch loss 53625 0.20147401094436646\n",
      "Training batch loss 53645 0.19950464367866516\n",
      "Training batch loss 53665 0.1979617178440094\n",
      "Training batch loss 53685 0.20617517828941345\n",
      "Training batch loss 53705 0.2074558138847351\n",
      "Training batch loss 53725 0.19798772037029266\n",
      "Training batch loss 53745 0.20131948590278625\n",
      "Training batch loss 53765 0.21275150775909424\n",
      "Training batch loss 53785 0.19735084474086761\n",
      "Training batch loss 53805 0.2057875394821167\n",
      "Training batch loss 53825 0.2038630247116089\n",
      "Training batch loss 53845 0.1960001289844513\n",
      "Training batch loss 53865 0.19405975937843323\n",
      "Training batch loss 53885 0.20012935996055603\n",
      "Training batch loss 53905 0.20398616790771484\n",
      "Training batch loss 53925 0.19369086623191833\n",
      "Training batch loss 53945 0.20635294914245605\n",
      "Training batch loss 53965 0.1984732449054718\n",
      "Training batch loss 53985 0.20124468207359314\n",
      "Training batch loss 54005 0.18851247429847717\n",
      "Training batch loss 54025 0.19849780201911926\n",
      "Training batch loss 54045 0.20473094284534454\n",
      "Training batch loss 54065 0.19965031743049622\n",
      "Training batch loss 54085 0.20437705516815186\n",
      "Training batch loss 54105 0.19127696752548218\n",
      "Training batch loss 54125 0.1965131014585495\n",
      "Training batch loss 54145 0.195701465010643\n",
      "Training batch loss 54165 0.19478952884674072\n",
      "Training batch loss 54185 0.201440691947937\n",
      "Training epoch loss 39 0.19857197999954224\n",
      "Test loss during training 39 0.1976657658815384\n",
      "Test accuracy during training 39 0.0\n",
      "Training batch loss 54200 0.19170010089874268\n",
      "Training batch loss 54220 0.19589971005916595\n",
      "Training batch loss 54240 0.20131264626979828\n",
      "Training batch loss 54260 0.1983058601617813\n",
      "Training batch loss 54280 0.1991705298423767\n",
      "Training batch loss 54300 0.2009793221950531\n",
      "Training batch loss 54320 0.1980592906475067\n",
      "Training batch loss 54340 0.19715023040771484\n",
      "Training batch loss 54360 0.19260242581367493\n",
      "Training batch loss 54380 0.19880345463752747\n",
      "Training batch loss 54400 0.19632495939731598\n",
      "Training batch loss 54420 0.19872212409973145\n",
      "Training batch loss 54440 0.2011108696460724\n",
      "Training batch loss 54460 0.19334366917610168\n",
      "Training batch loss 54480 0.2011612057685852\n",
      "Training batch loss 54500 0.18688777089118958\n",
      "Training batch loss 54520 0.19738690555095673\n",
      "Training batch loss 54540 0.20211857557296753\n",
      "Training batch loss 54560 0.19543080031871796\n",
      "Training batch loss 54580 0.20585793256759644\n",
      "Training batch loss 54600 0.19723135232925415\n",
      "Training batch loss 54620 0.19044041633605957\n",
      "Training batch loss 54640 0.20283818244934082\n",
      "Training batch loss 54660 0.19655489921569824\n",
      "Training batch loss 54680 0.1955663561820984\n",
      "Training batch loss 54700 0.20146483182907104\n",
      "Training batch loss 54720 0.1988999843597412\n",
      "Training batch loss 54740 0.19841943681240082\n",
      "Training batch loss 54760 0.20389148592948914\n",
      "Training batch loss 54780 0.19226862490177155\n",
      "Training batch loss 54800 0.2035389244556427\n",
      "Training batch loss 54820 0.19257719814777374\n",
      "Training batch loss 54840 0.1931302547454834\n",
      "Training batch loss 54860 0.19947470724582672\n",
      "Training batch loss 54880 0.19628626108169556\n",
      "Training batch loss 54900 0.20586371421813965\n",
      "Training batch loss 54920 0.1968308389186859\n",
      "Training batch loss 54940 0.20596987009048462\n",
      "Training batch loss 54960 0.1997758150100708\n",
      "Training batch loss 54980 0.20147401094436646\n",
      "Training batch loss 55000 0.19950464367866516\n",
      "Training batch loss 55020 0.1979617178440094\n",
      "Training batch loss 55040 0.20617517828941345\n",
      "Training batch loss 55060 0.2074558138847351\n",
      "Training batch loss 55080 0.19798772037029266\n",
      "Training batch loss 55100 0.20131948590278625\n",
      "Training batch loss 55120 0.21275150775909424\n",
      "Training batch loss 55140 0.19735084474086761\n",
      "Training batch loss 55160 0.2057875394821167\n",
      "Training batch loss 55180 0.2038630247116089\n",
      "Training batch loss 55200 0.1960001289844513\n",
      "Training batch loss 55220 0.19405975937843323\n",
      "Training batch loss 55240 0.20012935996055603\n",
      "Training batch loss 55260 0.20398616790771484\n",
      "Training batch loss 55280 0.19369086623191833\n",
      "Training batch loss 55300 0.20635294914245605\n",
      "Training batch loss 55320 0.1984732449054718\n",
      "Training batch loss 55340 0.20124468207359314\n",
      "Training batch loss 55360 0.18851247429847717\n",
      "Training batch loss 55380 0.19849780201911926\n",
      "Training batch loss 55400 0.20473094284534454\n",
      "Training batch loss 55420 0.19965031743049622\n",
      "Training batch loss 55440 0.20437705516815186\n",
      "Training batch loss 55460 0.19127696752548218\n",
      "Training batch loss 55480 0.1965131014585495\n",
      "Training batch loss 55500 0.195701465010643\n",
      "Training batch loss 55520 0.19478952884674072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss 55540 0.201440691947937\n",
      "Training epoch loss 40 0.19857197999954224\n",
      "Test loss during training 40 0.1976657658815384\n",
      "Test accuracy during training 40 0.0\n",
      "Training batch loss 55555 0.19170010089874268\n",
      "Training batch loss 55575 0.19589971005916595\n",
      "Training batch loss 55595 0.20131264626979828\n",
      "Training batch loss 55615 0.1983058601617813\n",
      "Training batch loss 55635 0.1991705298423767\n",
      "Training batch loss 55655 0.2009793221950531\n",
      "Training batch loss 55675 0.1980592906475067\n",
      "Training batch loss 55695 0.19715023040771484\n",
      "Training batch loss 55715 0.19260242581367493\n",
      "Training batch loss 55735 0.19880345463752747\n",
      "Training batch loss 55755 0.19632495939731598\n",
      "Training batch loss 55775 0.19872212409973145\n",
      "Training batch loss 55795 0.2011108696460724\n",
      "Training batch loss 55815 0.19334366917610168\n",
      "Training batch loss 55835 0.2011612057685852\n",
      "Training batch loss 55855 0.18688777089118958\n",
      "Training batch loss 55875 0.19738690555095673\n",
      "Training batch loss 55895 0.20211857557296753\n",
      "Training batch loss 55915 0.19543080031871796\n",
      "Training batch loss 55935 0.20585793256759644\n",
      "Training batch loss 55955 0.19723135232925415\n",
      "Training batch loss 55975 0.19044041633605957\n",
      "Training batch loss 55995 0.20283818244934082\n",
      "Training batch loss 56015 0.19655489921569824\n",
      "Training batch loss 56035 0.1955663561820984\n",
      "Training batch loss 56055 0.20146483182907104\n",
      "Training batch loss 56075 0.1988999843597412\n",
      "Training batch loss 56095 0.19841943681240082\n",
      "Training batch loss 56115 0.20389148592948914\n",
      "Training batch loss 56135 0.19226862490177155\n",
      "Training batch loss 56155 0.2035389244556427\n",
      "Training batch loss 56175 0.19257719814777374\n",
      "Training batch loss 56195 0.1931302547454834\n",
      "Training batch loss 56215 0.19947470724582672\n",
      "Training batch loss 56235 0.19628626108169556\n",
      "Training batch loss 56255 0.20586371421813965\n",
      "Training batch loss 56275 0.1968308389186859\n",
      "Training batch loss 56295 0.20596987009048462\n",
      "Training batch loss 56315 0.1997758150100708\n",
      "Training batch loss 56335 0.20147401094436646\n",
      "Training batch loss 56355 0.19950464367866516\n",
      "Training batch loss 56375 0.1979617178440094\n",
      "Training batch loss 56395 0.20617517828941345\n",
      "Training batch loss 56415 0.2074558138847351\n",
      "Training batch loss 56435 0.19798772037029266\n",
      "Training batch loss 56455 0.20131948590278625\n",
      "Training batch loss 56475 0.21275150775909424\n",
      "Training batch loss 56495 0.19735084474086761\n",
      "Training batch loss 56515 0.2057875394821167\n",
      "Training batch loss 56535 0.2038630247116089\n",
      "Training batch loss 56555 0.1960001289844513\n",
      "Training batch loss 56575 0.19405975937843323\n",
      "Training batch loss 56595 0.20012935996055603\n",
      "Training batch loss 56615 0.20398616790771484\n",
      "Training batch loss 56635 0.19369086623191833\n",
      "Training batch loss 56655 0.20635294914245605\n",
      "Training batch loss 56675 0.1984732449054718\n",
      "Training batch loss 56695 0.20124468207359314\n",
      "Training batch loss 56715 0.18851247429847717\n",
      "Training batch loss 56735 0.19849780201911926\n",
      "Training batch loss 56755 0.20473094284534454\n",
      "Training batch loss 56775 0.19965031743049622\n",
      "Training batch loss 56795 0.20437705516815186\n",
      "Training batch loss 56815 0.19127696752548218\n",
      "Training batch loss 56835 0.1965131014585495\n",
      "Training batch loss 56855 0.195701465010643\n",
      "Training batch loss 56875 0.19478952884674072\n",
      "Training batch loss 56895 0.201440691947937\n",
      "Training epoch loss 41 0.19857197999954224\n",
      "Test loss during training 41 0.1976657658815384\n",
      "Test accuracy during training 41 0.0\n",
      "Training batch loss 56910 0.19170010089874268\n",
      "Training batch loss 56930 0.19589971005916595\n",
      "Training batch loss 56950 0.20131264626979828\n",
      "Training batch loss 56970 0.1983058601617813\n",
      "Training batch loss 56990 0.1991705298423767\n",
      "Training batch loss 57010 0.2009793221950531\n",
      "Training batch loss 57030 0.1980592906475067\n",
      "Training batch loss 57050 0.19715023040771484\n",
      "Training batch loss 57070 0.19260242581367493\n",
      "Training batch loss 57090 0.19880345463752747\n",
      "Training batch loss 57110 0.19632495939731598\n",
      "Training batch loss 57130 0.19872212409973145\n",
      "Training batch loss 57150 0.2011108696460724\n",
      "Training batch loss 57170 0.19334366917610168\n",
      "Training batch loss 57190 0.2011612057685852\n",
      "Training batch loss 57210 0.18688777089118958\n",
      "Training batch loss 57230 0.19738690555095673\n",
      "Training batch loss 57250 0.20211857557296753\n",
      "Training batch loss 57270 0.19543080031871796\n",
      "Training batch loss 57290 0.20585793256759644\n",
      "Training batch loss 57310 0.19723135232925415\n",
      "Training batch loss 57330 0.19044041633605957\n",
      "Training batch loss 57350 0.20283818244934082\n",
      "Training batch loss 57370 0.19655489921569824\n",
      "Training batch loss 57390 0.1955663561820984\n",
      "Training batch loss 57410 0.20146483182907104\n",
      "Training batch loss 57430 0.1988999843597412\n",
      "Training batch loss 57450 0.19841943681240082\n",
      "Training batch loss 57470 0.20389148592948914\n",
      "Training batch loss 57490 0.19226862490177155\n",
      "Training batch loss 57510 0.2035389244556427\n",
      "Training batch loss 57530 0.19257719814777374\n",
      "Training batch loss 57550 0.1931302547454834\n",
      "Training batch loss 57570 0.19947470724582672\n",
      "Training batch loss 57590 0.19628626108169556\n",
      "Training batch loss 57610 0.20586371421813965\n",
      "Training batch loss 57630 0.1968308389186859\n",
      "Training batch loss 57650 0.20596987009048462\n",
      "Training batch loss 57670 0.1997758150100708\n",
      "Training batch loss 57690 0.20147401094436646\n",
      "Training batch loss 57710 0.19950464367866516\n",
      "Training batch loss 57730 0.1979617178440094\n",
      "Training batch loss 57750 0.20617517828941345\n",
      "Training batch loss 57770 0.2074558138847351\n",
      "Training batch loss 57790 0.19798772037029266\n",
      "Training batch loss 57810 0.20131948590278625\n",
      "Training batch loss 57830 0.21275150775909424\n",
      "Training batch loss 57850 0.19735084474086761\n",
      "Training batch loss 57870 0.2057875394821167\n",
      "Training batch loss 57890 0.2038630247116089\n",
      "Training batch loss 57910 0.1960001289844513\n",
      "Training batch loss 57930 0.19405975937843323\n",
      "Training batch loss 57950 0.20012935996055603\n",
      "Training batch loss 57970 0.20398616790771484\n",
      "Training batch loss 57990 0.19369086623191833\n",
      "Training batch loss 58010 0.20635294914245605\n",
      "Training batch loss 58030 0.1984732449054718\n",
      "Training batch loss 58050 0.20124468207359314\n",
      "Training batch loss 58070 0.18851247429847717\n",
      "Training batch loss 58090 0.19849780201911926\n",
      "Training batch loss 58110 0.20473094284534454\n",
      "Training batch loss 58130 0.19965031743049622\n",
      "Training batch loss 58150 0.20437705516815186\n",
      "Training batch loss 58170 0.19127696752548218\n",
      "Training batch loss 58190 0.1965131014585495\n",
      "Training batch loss 58210 0.195701465010643\n",
      "Training batch loss 58230 0.19478952884674072\n",
      "Training batch loss 58250 0.201440691947937\n",
      "Training epoch loss 42 0.19857197999954224\n",
      "Test loss during training 42 0.1976657658815384\n",
      "Test accuracy during training 42 0.0\n",
      "Training batch loss 58265 0.19170010089874268\n",
      "Training batch loss 58285 0.19589971005916595\n",
      "Training batch loss 58305 0.20131264626979828\n",
      "Training batch loss 58325 0.1983058601617813\n",
      "Training batch loss 58345 0.1991705298423767\n",
      "Training batch loss 58365 0.2009793221950531\n",
      "Training batch loss 58385 0.1980592906475067\n",
      "Training batch loss 58405 0.19715023040771484\n",
      "Training batch loss 58425 0.19260242581367493\n",
      "Training batch loss 58445 0.19880345463752747\n",
      "Training batch loss 58465 0.19632495939731598\n",
      "Training batch loss 58485 0.19872212409973145\n",
      "Training batch loss 58505 0.2011108696460724\n",
      "Training batch loss 58525 0.19334366917610168\n",
      "Training batch loss 58545 0.2011612057685852\n",
      "Training batch loss 58565 0.18688777089118958\n",
      "Training batch loss 58585 0.19738690555095673\n",
      "Training batch loss 58605 0.20211857557296753\n",
      "Training batch loss 58625 0.19543080031871796\n",
      "Training batch loss 58645 0.20585793256759644\n",
      "Training batch loss 58665 0.19723135232925415\n",
      "Training batch loss 58685 0.19044041633605957\n",
      "Training batch loss 58705 0.20283818244934082\n",
      "Training batch loss 58725 0.19655489921569824\n",
      "Training batch loss 58745 0.1955663561820984\n",
      "Training batch loss 58765 0.20146483182907104\n",
      "Training batch loss 58785 0.1988999843597412\n",
      "Training batch loss 58805 0.19841943681240082\n",
      "Training batch loss 58825 0.20389148592948914\n",
      "Training batch loss 58845 0.19226862490177155\n",
      "Training batch loss 58865 0.2035389244556427\n",
      "Training batch loss 58885 0.19257719814777374\n",
      "Training batch loss 58905 0.1931302547454834\n",
      "Training batch loss 58925 0.19947470724582672\n",
      "Training batch loss 58945 0.19628626108169556\n",
      "Training batch loss 58965 0.20586371421813965\n",
      "Training batch loss 58985 0.1968308389186859\n",
      "Training batch loss 59005 0.20596987009048462\n",
      "Training batch loss 59025 0.1997758150100708\n",
      "Training batch loss 59045 0.20147401094436646\n",
      "Training batch loss 59065 0.19950464367866516\n",
      "Training batch loss 59085 0.1979617178440094\n",
      "Training batch loss 59105 0.20617517828941345\n",
      "Training batch loss 59125 0.2074558138847351\n",
      "Training batch loss 59145 0.19798772037029266\n",
      "Training batch loss 59165 0.20131948590278625\n",
      "Training batch loss 59185 0.21275150775909424\n",
      "Training batch loss 59205 0.19735084474086761\n",
      "Training batch loss 59225 0.2057875394821167\n",
      "Training batch loss 59245 0.2038630247116089\n",
      "Training batch loss 59265 0.1960001289844513\n",
      "Training batch loss 59285 0.19405975937843323\n",
      "Training batch loss 59305 0.20012935996055603\n",
      "Training batch loss 59325 0.20398616790771484\n",
      "Training batch loss 59345 0.19369086623191833\n",
      "Training batch loss 59365 0.20635294914245605\n",
      "Training batch loss 59385 0.1984732449054718\n",
      "Training batch loss 59405 0.20124468207359314\n",
      "Training batch loss 59425 0.18851247429847717\n",
      "Training batch loss 59445 0.19849780201911926\n",
      "Training batch loss 59465 0.20473094284534454\n",
      "Training batch loss 59485 0.19965031743049622\n",
      "Training batch loss 59505 0.20437705516815186\n",
      "Training batch loss 59525 0.19127696752548218\n",
      "Training batch loss 59545 0.1965131014585495\n",
      "Training batch loss 59565 0.195701465010643\n",
      "Training batch loss 59585 0.19478952884674072\n",
      "Training batch loss 59605 0.201440691947937\n",
      "Training epoch loss 43 0.19857197999954224\n",
      "Test loss during training 43 0.1976657658815384\n",
      "Test accuracy during training 43 0.0\n",
      "Training batch loss 59620 0.19170010089874268\n",
      "Training batch loss 59640 0.19589971005916595\n",
      "Training batch loss 59660 0.20131264626979828\n",
      "Training batch loss 59680 0.1983058601617813\n",
      "Training batch loss 59700 0.1991705298423767\n",
      "Training batch loss 59720 0.2009793221950531\n",
      "Training batch loss 59740 0.1980592906475067\n",
      "Training batch loss 59760 0.19715023040771484\n",
      "Training batch loss 59780 0.19260242581367493\n",
      "Training batch loss 59800 0.19880345463752747\n",
      "Training batch loss 59820 0.19632495939731598\n",
      "Training batch loss 59840 0.19872212409973145\n",
      "Training batch loss 59860 0.2011108696460724\n",
      "Training batch loss 59880 0.19334366917610168\n",
      "Training batch loss 59900 0.2011612057685852\n",
      "Training batch loss 59920 0.18688777089118958\n",
      "Training batch loss 59940 0.19738690555095673\n",
      "Training batch loss 59960 0.20211857557296753\n",
      "Training batch loss 59980 0.19543080031871796\n",
      "Training batch loss 60000 0.20585793256759644\n",
      "Training batch loss 60020 0.19723135232925415\n",
      "Training batch loss 60040 0.19044041633605957\n",
      "Training batch loss 60060 0.20283818244934082\n",
      "Training batch loss 60080 0.19655489921569824\n",
      "Training batch loss 60100 0.1955663561820984\n",
      "Training batch loss 60120 0.20146483182907104\n",
      "Training batch loss 60140 0.1988999843597412\n",
      "Training batch loss 60160 0.19841943681240082\n",
      "Training batch loss 60180 0.20389148592948914\n",
      "Training batch loss 60200 0.19226862490177155\n",
      "Training batch loss 60220 0.2035389244556427\n",
      "Training batch loss 60240 0.19257719814777374\n",
      "Training batch loss 60260 0.1931302547454834\n",
      "Training batch loss 60280 0.19947470724582672\n",
      "Training batch loss 60300 0.19628626108169556\n",
      "Training batch loss 60320 0.20586371421813965\n",
      "Training batch loss 60340 0.1968308389186859\n",
      "Training batch loss 60360 0.20596987009048462\n",
      "Training batch loss 60380 0.1997758150100708\n",
      "Training batch loss 60400 0.20147401094436646\n",
      "Training batch loss 60420 0.19950464367866516\n",
      "Training batch loss 60440 0.1979617178440094\n",
      "Training batch loss 60460 0.20617517828941345\n",
      "Training batch loss 60480 0.2074558138847351\n",
      "Training batch loss 60500 0.19798772037029266\n",
      "Training batch loss 60520 0.20131948590278625\n",
      "Training batch loss 60540 0.21275150775909424\n",
      "Training batch loss 60560 0.19735084474086761\n",
      "Training batch loss 60580 0.2057875394821167\n",
      "Training batch loss 60600 0.2038630247116089\n",
      "Training batch loss 60620 0.1960001289844513\n",
      "Training batch loss 60640 0.19405975937843323\n",
      "Training batch loss 60660 0.20012935996055603\n",
      "Training batch loss 60680 0.20398616790771484\n",
      "Training batch loss 60700 0.19369086623191833\n",
      "Training batch loss 60720 0.20635294914245605\n",
      "Training batch loss 60740 0.1984732449054718\n",
      "Training batch loss 60760 0.20124468207359314\n",
      "Training batch loss 60780 0.18851247429847717\n",
      "Training batch loss 60800 0.19849780201911926\n",
      "Training batch loss 60820 0.20473094284534454\n",
      "Training batch loss 60840 0.19965031743049622\n",
      "Training batch loss 60860 0.20437705516815186\n",
      "Training batch loss 60880 0.19127696752548218\n",
      "Training batch loss 60900 0.1965131014585495\n",
      "Training batch loss 60920 0.195701465010643\n",
      "Training batch loss 60940 0.19478952884674072\n",
      "Training batch loss 60960 0.201440691947937\n",
      "Training epoch loss 44 0.19857197999954224\n",
      "Test loss during training 44 0.1976657658815384\n",
      "Test accuracy during training 44 0.0\n",
      "Training batch loss 60975 0.19170010089874268\n",
      "Training batch loss 60995 0.19589971005916595\n",
      "Training batch loss 61015 0.20131264626979828\n",
      "Training batch loss 61035 0.1983058601617813\n",
      "Training batch loss 61055 0.1991705298423767\n",
      "Training batch loss 61075 0.2009793221950531\n",
      "Training batch loss 61095 0.1980592906475067\n",
      "Training batch loss 61115 0.19715023040771484\n",
      "Training batch loss 61135 0.19260242581367493\n",
      "Training batch loss 61155 0.19880345463752747\n",
      "Training batch loss 61175 0.19632495939731598\n",
      "Training batch loss 61195 0.19872212409973145\n",
      "Training batch loss 61215 0.2011108696460724\n",
      "Training batch loss 61235 0.19334366917610168\n",
      "Training batch loss 61255 0.2011612057685852\n",
      "Training batch loss 61275 0.18688777089118958\n",
      "Training batch loss 61295 0.19738690555095673\n",
      "Training batch loss 61315 0.20211857557296753\n",
      "Training batch loss 61335 0.19543080031871796\n",
      "Training batch loss 61355 0.20585793256759644\n",
      "Training batch loss 61375 0.19723135232925415\n",
      "Training batch loss 61395 0.19044041633605957\n",
      "Training batch loss 61415 0.20283818244934082\n",
      "Training batch loss 61435 0.19655489921569824\n",
      "Training batch loss 61455 0.1955663561820984\n",
      "Training batch loss 61475 0.20146483182907104\n",
      "Training batch loss 61495 0.1988999843597412\n",
      "Training batch loss 61515 0.19841943681240082\n",
      "Training batch loss 61535 0.20389148592948914\n",
      "Training batch loss 61555 0.19226862490177155\n",
      "Training batch loss 61575 0.2035389244556427\n",
      "Training batch loss 61595 0.19257719814777374\n",
      "Training batch loss 61615 0.1931302547454834\n",
      "Training batch loss 61635 0.19947470724582672\n",
      "Training batch loss 61655 0.19628626108169556\n",
      "Training batch loss 61675 0.20586371421813965\n",
      "Training batch loss 61695 0.1968308389186859\n",
      "Training batch loss 61715 0.20596987009048462\n",
      "Training batch loss 61735 0.1997758150100708\n",
      "Training batch loss 61755 0.20147401094436646\n",
      "Training batch loss 61775 0.19950464367866516\n",
      "Training batch loss 61795 0.1979617178440094\n",
      "Training batch loss 61815 0.20617517828941345\n",
      "Training batch loss 61835 0.2074558138847351\n",
      "Training batch loss 61855 0.19798772037029266\n",
      "Training batch loss 61875 0.20131948590278625\n",
      "Training batch loss 61895 0.21275150775909424\n",
      "Training batch loss 61915 0.19735084474086761\n",
      "Training batch loss 61935 0.2057875394821167\n",
      "Training batch loss 61955 0.2038630247116089\n",
      "Training batch loss 61975 0.1960001289844513\n",
      "Training batch loss 61995 0.19405975937843323\n",
      "Training batch loss 62015 0.20012935996055603\n",
      "Training batch loss 62035 0.20398616790771484\n",
      "Training batch loss 62055 0.19369086623191833\n",
      "Training batch loss 62075 0.20635294914245605\n",
      "Training batch loss 62095 0.1984732449054718\n",
      "Training batch loss 62115 0.20124468207359314\n",
      "Training batch loss 62135 0.18851247429847717\n",
      "Training batch loss 62155 0.19849780201911926\n",
      "Training batch loss 62175 0.20473094284534454\n",
      "Training batch loss 62195 0.19965031743049622\n",
      "Training batch loss 62215 0.20437705516815186\n",
      "Training batch loss 62235 0.19127696752548218\n",
      "Training batch loss 62255 0.1965131014585495\n",
      "Training batch loss 62275 0.195701465010643\n",
      "Training batch loss 62295 0.19478952884674072\n",
      "Training batch loss 62315 0.201440691947937\n",
      "Training epoch loss 45 0.19857197999954224\n",
      "Test loss during training 45 0.1976657658815384\n",
      "Test accuracy during training 45 0.0\n",
      "Training batch loss 62330 0.19170010089874268\n",
      "Training batch loss 62350 0.19589971005916595\n",
      "Training batch loss 62370 0.20131264626979828\n",
      "Training batch loss 62390 0.1983058601617813\n",
      "Training batch loss 62410 0.1991705298423767\n",
      "Training batch loss 62430 0.2009793221950531\n",
      "Training batch loss 62450 0.1980592906475067\n",
      "Training batch loss 62470 0.19715023040771484\n",
      "Training batch loss 62490 0.19260242581367493\n",
      "Training batch loss 62510 0.19880345463752747\n",
      "Training batch loss 62530 0.19632495939731598\n",
      "Training batch loss 62550 0.19872212409973145\n",
      "Training batch loss 62570 0.2011108696460724\n",
      "Training batch loss 62590 0.19334366917610168\n",
      "Training batch loss 62610 0.2011612057685852\n",
      "Training batch loss 62630 0.18688777089118958\n",
      "Training batch loss 62650 0.19738690555095673\n",
      "Training batch loss 62670 0.20211857557296753\n",
      "Training batch loss 62690 0.19543080031871796\n",
      "Training batch loss 62710 0.20585793256759644\n",
      "Training batch loss 62730 0.19723135232925415\n",
      "Training batch loss 62750 0.19044041633605957\n",
      "Training batch loss 62770 0.20283818244934082\n",
      "Training batch loss 62790 0.19655489921569824\n",
      "Training batch loss 62810 0.1955663561820984\n",
      "Training batch loss 62830 0.20146483182907104\n",
      "Training batch loss 62850 0.1988999843597412\n",
      "Training batch loss 62870 0.19841943681240082\n",
      "Training batch loss 62890 0.20389148592948914\n",
      "Training batch loss 62910 0.19226862490177155\n",
      "Training batch loss 62930 0.2035389244556427\n",
      "Training batch loss 62950 0.19257719814777374\n",
      "Training batch loss 62970 0.1931302547454834\n",
      "Training batch loss 62990 0.19947470724582672\n",
      "Training batch loss 63010 0.19628626108169556\n",
      "Training batch loss 63030 0.20586371421813965\n",
      "Training batch loss 63050 0.1968308389186859\n",
      "Training batch loss 63070 0.20596987009048462\n",
      "Training batch loss 63090 0.1997758150100708\n",
      "Training batch loss 63110 0.20147401094436646\n",
      "Training batch loss 63130 0.19950464367866516\n",
      "Training batch loss 63150 0.1979617178440094\n",
      "Training batch loss 63170 0.20617517828941345\n",
      "Training batch loss 63190 0.2074558138847351\n",
      "Training batch loss 63210 0.19798772037029266\n",
      "Training batch loss 63230 0.20131948590278625\n",
      "Training batch loss 63250 0.21275150775909424\n",
      "Training batch loss 63270 0.19735084474086761\n",
      "Training batch loss 63290 0.2057875394821167\n",
      "Training batch loss 63310 0.2038630247116089\n",
      "Training batch loss 63330 0.1960001289844513\n",
      "Training batch loss 63350 0.19405975937843323\n",
      "Training batch loss 63370 0.20012935996055603\n",
      "Training batch loss 63390 0.20398616790771484\n",
      "Training batch loss 63410 0.19369086623191833\n",
      "Training batch loss 63430 0.20635294914245605\n",
      "Training batch loss 63450 0.1984732449054718\n",
      "Training batch loss 63470 0.20124468207359314\n",
      "Training batch loss 63490 0.18851247429847717\n",
      "Training batch loss 63510 0.19849780201911926\n",
      "Training batch loss 63530 0.20473094284534454\n",
      "Training batch loss 63550 0.19965031743049622\n",
      "Training batch loss 63570 0.20437705516815186\n",
      "Training batch loss 63590 0.19127696752548218\n",
      "Training batch loss 63610 0.1965131014585495\n",
      "Training batch loss 63630 0.195701465010643\n",
      "Training batch loss 63650 0.19478952884674072\n",
      "Training batch loss 63670 0.201440691947937\n",
      "Training epoch loss 46 0.19857197999954224\n",
      "Test loss during training 46 0.1976657658815384\n",
      "Test accuracy during training 46 0.0\n",
      "Training batch loss 63685 0.19170010089874268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss 63705 0.19589971005916595\n",
      "Training batch loss 63725 0.20131264626979828\n",
      "Training batch loss 63745 0.1983058601617813\n",
      "Training batch loss 63765 0.1991705298423767\n",
      "Training batch loss 63785 0.2009793221950531\n",
      "Training batch loss 63805 0.1980592906475067\n",
      "Training batch loss 63825 0.19715023040771484\n",
      "Training batch loss 63845 0.19260242581367493\n",
      "Training batch loss 63865 0.19880345463752747\n",
      "Training batch loss 63885 0.19632495939731598\n",
      "Training batch loss 63905 0.19872212409973145\n",
      "Training batch loss 63925 0.2011108696460724\n",
      "Training batch loss 63945 0.19334366917610168\n",
      "Training batch loss 63965 0.2011612057685852\n",
      "Training batch loss 63985 0.18688777089118958\n",
      "Training batch loss 64005 0.19738690555095673\n",
      "Training batch loss 64025 0.20211857557296753\n",
      "Training batch loss 64045 0.19543080031871796\n",
      "Training batch loss 64065 0.20585793256759644\n",
      "Training batch loss 64085 0.19723135232925415\n",
      "Training batch loss 64105 0.19044041633605957\n",
      "Training batch loss 64125 0.20283818244934082\n",
      "Training batch loss 64145 0.19655489921569824\n",
      "Training batch loss 64165 0.1955663561820984\n",
      "Training batch loss 64185 0.20146483182907104\n",
      "Training batch loss 64205 0.1988999843597412\n",
      "Training batch loss 64225 0.19841943681240082\n",
      "Training batch loss 64245 0.20389148592948914\n",
      "Training batch loss 64265 0.19226862490177155\n",
      "Training batch loss 64285 0.2035389244556427\n",
      "Training batch loss 64305 0.19257719814777374\n",
      "Training batch loss 64325 0.1931302547454834\n",
      "Training batch loss 64345 0.19947470724582672\n",
      "Training batch loss 64365 0.19628626108169556\n",
      "Training batch loss 64385 0.20586371421813965\n",
      "Training batch loss 64405 0.1968308389186859\n",
      "Training batch loss 64425 0.20596987009048462\n",
      "Training batch loss 64445 0.1997758150100708\n",
      "Training batch loss 64465 0.20147401094436646\n",
      "Training batch loss 64485 0.19950464367866516\n",
      "Training batch loss 64505 0.1979617178440094\n",
      "Training batch loss 64525 0.20617517828941345\n",
      "Training batch loss 64545 0.2074558138847351\n",
      "Training batch loss 64565 0.19798772037029266\n",
      "Training batch loss 64585 0.20131948590278625\n",
      "Training batch loss 64605 0.21275150775909424\n",
      "Training batch loss 64625 0.19735084474086761\n",
      "Training batch loss 64645 0.2057875394821167\n",
      "Training batch loss 64665 0.2038630247116089\n",
      "Training batch loss 64685 0.1960001289844513\n",
      "Training batch loss 64705 0.19405975937843323\n",
      "Training batch loss 64725 0.20012935996055603\n",
      "Training batch loss 64745 0.20398616790771484\n",
      "Training batch loss 64765 0.19369086623191833\n",
      "Training batch loss 64785 0.20635294914245605\n",
      "Training batch loss 64805 0.1984732449054718\n",
      "Training batch loss 64825 0.20124468207359314\n",
      "Training batch loss 64845 0.18851247429847717\n",
      "Training batch loss 64865 0.19849780201911926\n",
      "Training batch loss 64885 0.20473094284534454\n",
      "Training batch loss 64905 0.19965031743049622\n",
      "Training batch loss 64925 0.20437705516815186\n",
      "Training batch loss 64945 0.19127696752548218\n",
      "Training batch loss 64965 0.1965131014585495\n",
      "Training batch loss 64985 0.195701465010643\n",
      "Training batch loss 65005 0.19478952884674072\n",
      "Training batch loss 65025 0.201440691947937\n",
      "Training epoch loss 47 0.19857197999954224\n",
      "Test loss during training 47 0.1976657658815384\n",
      "Test accuracy during training 47 0.0\n",
      "Training batch loss 65040 0.19170010089874268\n",
      "Training batch loss 65060 0.19589971005916595\n",
      "Training batch loss 65080 0.20131264626979828\n",
      "Training batch loss 65100 0.1983058601617813\n",
      "Training batch loss 65120 0.1991705298423767\n",
      "Training batch loss 65140 0.2009793221950531\n",
      "Training batch loss 65160 0.1980592906475067\n",
      "Training batch loss 65180 0.19715023040771484\n",
      "Training batch loss 65200 0.19260242581367493\n",
      "Training batch loss 65220 0.19880345463752747\n",
      "Training batch loss 65240 0.19632495939731598\n",
      "Training batch loss 65260 0.19872212409973145\n",
      "Training batch loss 65280 0.2011108696460724\n",
      "Training batch loss 65300 0.19334366917610168\n",
      "Training batch loss 65320 0.2011612057685852\n",
      "Training batch loss 65340 0.18688777089118958\n",
      "Training batch loss 65360 0.19738690555095673\n",
      "Training batch loss 65380 0.20211857557296753\n",
      "Training batch loss 65400 0.19543080031871796\n",
      "Training batch loss 65420 0.20585793256759644\n",
      "Training batch loss 65440 0.19723135232925415\n",
      "Training batch loss 65460 0.19044041633605957\n",
      "Training batch loss 65480 0.20283818244934082\n",
      "Training batch loss 65500 0.19655489921569824\n",
      "Training batch loss 65520 0.1955663561820984\n",
      "Training batch loss 65540 0.20146483182907104\n",
      "Training batch loss 65560 0.1988999843597412\n",
      "Training batch loss 65580 0.19841943681240082\n",
      "Training batch loss 65600 0.20389148592948914\n",
      "Training batch loss 65620 0.19226862490177155\n",
      "Training batch loss 65640 0.2035389244556427\n",
      "Training batch loss 65660 0.19257719814777374\n",
      "Training batch loss 65680 0.1931302547454834\n",
      "Training batch loss 65700 0.19947470724582672\n",
      "Training batch loss 65720 0.19628626108169556\n",
      "Training batch loss 65740 0.20586371421813965\n",
      "Training batch loss 65760 0.1968308389186859\n",
      "Training batch loss 65780 0.20596987009048462\n",
      "Training batch loss 65800 0.1997758150100708\n",
      "Training batch loss 65820 0.20147401094436646\n",
      "Training batch loss 65840 0.19950464367866516\n",
      "Training batch loss 65860 0.1979617178440094\n",
      "Training batch loss 65880 0.20617517828941345\n",
      "Training batch loss 65900 0.2074558138847351\n",
      "Training batch loss 65920 0.19798772037029266\n",
      "Training batch loss 65940 0.20131948590278625\n",
      "Training batch loss 65960 0.21275150775909424\n",
      "Training batch loss 65980 0.19735084474086761\n",
      "Training batch loss 66000 0.2057875394821167\n",
      "Training batch loss 66020 0.2038630247116089\n",
      "Training batch loss 66040 0.1960001289844513\n",
      "Training batch loss 66060 0.19405975937843323\n",
      "Training batch loss 66080 0.20012935996055603\n",
      "Training batch loss 66100 0.20398616790771484\n",
      "Training batch loss 66120 0.19369086623191833\n",
      "Training batch loss 66140 0.20635294914245605\n",
      "Training batch loss 66160 0.1984732449054718\n",
      "Training batch loss 66180 0.20124468207359314\n",
      "Training batch loss 66200 0.18851247429847717\n",
      "Training batch loss 66220 0.19849780201911926\n",
      "Training batch loss 66240 0.20473094284534454\n",
      "Training batch loss 66260 0.19965031743049622\n",
      "Training batch loss 66280 0.20437705516815186\n",
      "Training batch loss 66300 0.19127696752548218\n",
      "Training batch loss 66320 0.1965131014585495\n",
      "Training batch loss 66340 0.195701465010643\n",
      "Training batch loss 66360 0.19478952884674072\n",
      "Training batch loss 66380 0.201440691947937\n",
      "Training epoch loss 48 0.19857197999954224\n",
      "Test loss during training 48 0.1976657658815384\n",
      "Test accuracy during training 48 0.0\n",
      "Training batch loss 66395 0.19170010089874268\n",
      "Training batch loss 66415 0.19589971005916595\n",
      "Training batch loss 66435 0.20131264626979828\n",
      "Training batch loss 66455 0.1983058601617813\n",
      "Training batch loss 66475 0.1991705298423767\n",
      "Training batch loss 66495 0.2009793221950531\n",
      "Training batch loss 66515 0.1980592906475067\n",
      "Training batch loss 66535 0.19715023040771484\n",
      "Training batch loss 66555 0.19260242581367493\n",
      "Training batch loss 66575 0.19880345463752747\n",
      "Training batch loss 66595 0.19632495939731598\n",
      "Training batch loss 66615 0.19872212409973145\n",
      "Training batch loss 66635 0.2011108696460724\n",
      "Training batch loss 66655 0.19334366917610168\n",
      "Training batch loss 66675 0.2011612057685852\n",
      "Training batch loss 66695 0.18688777089118958\n",
      "Training batch loss 66715 0.19738690555095673\n",
      "Training batch loss 66735 0.20211857557296753\n",
      "Training batch loss 66755 0.19543080031871796\n",
      "Training batch loss 66775 0.20585793256759644\n",
      "Training batch loss 66795 0.19723135232925415\n",
      "Training batch loss 66815 0.19044041633605957\n",
      "Training batch loss 66835 0.20283818244934082\n",
      "Training batch loss 66855 0.19655489921569824\n",
      "Training batch loss 66875 0.1955663561820984\n",
      "Training batch loss 66895 0.20146483182907104\n",
      "Training batch loss 66915 0.1988999843597412\n",
      "Training batch loss 66935 0.19841943681240082\n",
      "Training batch loss 66955 0.20389148592948914\n",
      "Training batch loss 66975 0.19226862490177155\n",
      "Training batch loss 66995 0.2035389244556427\n",
      "Training batch loss 67015 0.19257719814777374\n",
      "Training batch loss 67035 0.1931302547454834\n",
      "Training batch loss 67055 0.19947470724582672\n",
      "Training batch loss 67075 0.19628626108169556\n",
      "Training batch loss 67095 0.20586371421813965\n",
      "Training batch loss 67115 0.1968308389186859\n",
      "Training batch loss 67135 0.20596987009048462\n",
      "Training batch loss 67155 0.1997758150100708\n",
      "Training batch loss 67175 0.20147401094436646\n",
      "Training batch loss 67195 0.19950464367866516\n",
      "Training batch loss 67215 0.1979617178440094\n",
      "Training batch loss 67235 0.20617517828941345\n",
      "Training batch loss 67255 0.2074558138847351\n",
      "Training batch loss 67275 0.19798772037029266\n",
      "Training batch loss 67295 0.20131948590278625\n",
      "Training batch loss 67315 0.21275150775909424\n",
      "Training batch loss 67335 0.19735084474086761\n",
      "Training batch loss 67355 0.2057875394821167\n",
      "Training batch loss 67375 0.2038630247116089\n",
      "Training batch loss 67395 0.1960001289844513\n",
      "Training batch loss 67415 0.19405975937843323\n",
      "Training batch loss 67435 0.20012935996055603\n",
      "Training batch loss 67455 0.20398616790771484\n",
      "Training batch loss 67475 0.19369086623191833\n",
      "Training batch loss 67495 0.20635294914245605\n",
      "Training batch loss 67515 0.1984732449054718\n",
      "Training batch loss 67535 0.20124468207359314\n",
      "Training batch loss 67555 0.18851247429847717\n",
      "Training batch loss 67575 0.19849780201911926\n",
      "Training batch loss 67595 0.20473094284534454\n",
      "Training batch loss 67615 0.19965031743049622\n",
      "Training batch loss 67635 0.20437705516815186\n",
      "Training batch loss 67655 0.19127696752548218\n",
      "Training batch loss 67675 0.1965131014585495\n",
      "Training batch loss 67695 0.195701465010643\n",
      "Training batch loss 67715 0.19478952884674072\n",
      "Training batch loss 67735 0.201440691947937\n",
      "Training epoch loss 49 0.19857197999954224\n",
      "Test loss during training 49 0.1976657658815384\n",
      "Test accuracy during training 49 0.0\n",
      "Training batch loss 67750 0.19170010089874268\n",
      "Training batch loss 67770 0.19589971005916595\n",
      "Training batch loss 67790 0.20131264626979828\n",
      "Training batch loss 67810 0.1983058601617813\n",
      "Training batch loss 67830 0.1991705298423767\n",
      "Training batch loss 67850 0.2009793221950531\n",
      "Training batch loss 67870 0.1980592906475067\n",
      "Training batch loss 67890 0.19715023040771484\n",
      "Training batch loss 67910 0.19260242581367493\n",
      "Training batch loss 67930 0.19880345463752747\n",
      "Training batch loss 67950 0.19632495939731598\n",
      "Training batch loss 67970 0.19872212409973145\n",
      "Training batch loss 67990 0.2011108696460724\n",
      "Training batch loss 68010 0.19334366917610168\n",
      "Training batch loss 68030 0.2011612057685852\n",
      "Training batch loss 68050 0.18688777089118958\n",
      "Training batch loss 68070 0.19738690555095673\n",
      "Training batch loss 68090 0.20211857557296753\n",
      "Training batch loss 68110 0.19543080031871796\n",
      "Training batch loss 68130 0.20585793256759644\n",
      "Training batch loss 68150 0.19723135232925415\n",
      "Training batch loss 68170 0.19044041633605957\n",
      "Training batch loss 68190 0.20283818244934082\n",
      "Training batch loss 68210 0.19655489921569824\n",
      "Training batch loss 68230 0.1955663561820984\n",
      "Training batch loss 68250 0.20146483182907104\n",
      "Training batch loss 68270 0.1988999843597412\n",
      "Training batch loss 68290 0.19841943681240082\n",
      "Training batch loss 68310 0.20389148592948914\n",
      "Training batch loss 68330 0.19226862490177155\n",
      "Training batch loss 68350 0.2035389244556427\n",
      "Training batch loss 68370 0.19257719814777374\n",
      "Training batch loss 68390 0.1931302547454834\n",
      "Training batch loss 68410 0.19947470724582672\n",
      "Training batch loss 68430 0.19628626108169556\n",
      "Training batch loss 68450 0.20586371421813965\n",
      "Training batch loss 68470 0.1968308389186859\n",
      "Training batch loss 68490 0.20596987009048462\n",
      "Training batch loss 68510 0.1997758150100708\n",
      "Training batch loss 68530 0.20147401094436646\n",
      "Training batch loss 68550 0.19950464367866516\n",
      "Training batch loss 68570 0.1979617178440094\n",
      "Training batch loss 68590 0.20617517828941345\n",
      "Training batch loss 68610 0.2074558138847351\n",
      "Training batch loss 68630 0.19798772037029266\n",
      "Training batch loss 68650 0.20131948590278625\n",
      "Training batch loss 68670 0.21275150775909424\n",
      "Training batch loss 68690 0.19735084474086761\n",
      "Training batch loss 68710 0.2057875394821167\n",
      "Training batch loss 68730 0.2038630247116089\n",
      "Training batch loss 68750 0.1960001289844513\n",
      "Training batch loss 68770 0.19405975937843323\n",
      "Training batch loss 68790 0.20012935996055603\n",
      "Training batch loss 68810 0.20398616790771484\n",
      "Training batch loss 68830 0.19369086623191833\n",
      "Training batch loss 68850 0.20635294914245605\n",
      "Training batch loss 68870 0.1984732449054718\n",
      "Training batch loss 68890 0.20124468207359314\n",
      "Training batch loss 68910 0.18851247429847717\n",
      "Training batch loss 68930 0.19849780201911926\n",
      "Training batch loss 68950 0.20473094284534454\n",
      "Training batch loss 68970 0.19965031743049622\n",
      "Training batch loss 68990 0.20437705516815186\n",
      "Training batch loss 69010 0.19127696752548218\n",
      "Training batch loss 69030 0.1965131014585495\n",
      "Training batch loss 69050 0.195701465010643\n",
      "Training batch loss 69070 0.19478952884674072\n",
      "Training batch loss 69090 0.201440691947937\n",
      "Training epoch loss 50 0.19857197999954224\n",
      "Test loss during training 50 0.1976657658815384\n",
      "Test accuracy during training 50 0.0\n",
      "Training batch loss 69105 0.19170010089874268\n",
      "Training batch loss 69125 0.19589971005916595\n",
      "Training batch loss 69145 0.20131264626979828\n",
      "Training batch loss 69165 0.1983058601617813\n",
      "Training batch loss 69185 0.1991705298423767\n",
      "Training batch loss 69205 0.2009793221950531\n",
      "Training batch loss 69225 0.1980592906475067\n",
      "Training batch loss 69245 0.19715023040771484\n",
      "Training batch loss 69265 0.19260242581367493\n",
      "Training batch loss 69285 0.19880345463752747\n",
      "Training batch loss 69305 0.19632495939731598\n",
      "Training batch loss 69325 0.19872212409973145\n",
      "Training batch loss 69345 0.2011108696460724\n",
      "Training batch loss 69365 0.19334366917610168\n",
      "Training batch loss 69385 0.2011612057685852\n",
      "Training batch loss 69405 0.18688777089118958\n",
      "Training batch loss 69425 0.19738690555095673\n",
      "Training batch loss 69445 0.20211857557296753\n",
      "Training batch loss 69465 0.19543080031871796\n",
      "Training batch loss 69485 0.20585793256759644\n",
      "Training batch loss 69505 0.19723135232925415\n",
      "Training batch loss 69525 0.19044041633605957\n",
      "Training batch loss 69545 0.20283818244934082\n",
      "Training batch loss 69565 0.19655489921569824\n",
      "Training batch loss 69585 0.1955663561820984\n",
      "Training batch loss 69605 0.20146483182907104\n",
      "Training batch loss 69625 0.1988999843597412\n",
      "Training batch loss 69645 0.19841943681240082\n",
      "Training batch loss 69665 0.20389148592948914\n",
      "Training batch loss 69685 0.19226862490177155\n",
      "Training batch loss 69705 0.2035389244556427\n",
      "Training batch loss 69725 0.19257719814777374\n",
      "Training batch loss 69745 0.1931302547454834\n",
      "Training batch loss 69765 0.19947470724582672\n",
      "Training batch loss 69785 0.19628626108169556\n",
      "Training batch loss 69805 0.20586371421813965\n",
      "Training batch loss 69825 0.1968308389186859\n",
      "Training batch loss 69845 0.20596987009048462\n",
      "Training batch loss 69865 0.1997758150100708\n",
      "Training batch loss 69885 0.20147401094436646\n",
      "Training batch loss 69905 0.19950464367866516\n",
      "Training batch loss 69925 0.1979617178440094\n",
      "Training batch loss 69945 0.20617517828941345\n",
      "Training batch loss 69965 0.2074558138847351\n",
      "Training batch loss 69985 0.19798772037029266\n",
      "Training batch loss 70005 0.20131948590278625\n",
      "Training batch loss 70025 0.21275150775909424\n",
      "Training batch loss 70045 0.19735084474086761\n",
      "Training batch loss 70065 0.2057875394821167\n",
      "Training batch loss 70085 0.2038630247116089\n",
      "Training batch loss 70105 0.1960001289844513\n",
      "Training batch loss 70125 0.19405975937843323\n",
      "Training batch loss 70145 0.20012935996055603\n",
      "Training batch loss 70165 0.20398616790771484\n",
      "Training batch loss 70185 0.19369086623191833\n",
      "Training batch loss 70205 0.20635294914245605\n",
      "Training batch loss 70225 0.1984732449054718\n",
      "Training batch loss 70245 0.20124468207359314\n",
      "Training batch loss 70265 0.18851247429847717\n",
      "Training batch loss 70285 0.19849780201911926\n",
      "Training batch loss 70305 0.20473094284534454\n",
      "Training batch loss 70325 0.19965031743049622\n",
      "Training batch loss 70345 0.20437705516815186\n",
      "Training batch loss 70365 0.19127696752548218\n",
      "Training batch loss 70385 0.1965131014585495\n",
      "Training batch loss 70405 0.195701465010643\n",
      "Training batch loss 70425 0.19478952884674072\n",
      "Training batch loss 70445 0.201440691947937\n",
      "Training epoch loss 51 0.19857197999954224\n",
      "Test loss during training 51 0.1976657658815384\n",
      "Test accuracy during training 51 0.0\n",
      "Training batch loss 70460 0.19170010089874268\n",
      "Training batch loss 70480 0.19589971005916595\n",
      "Training batch loss 70500 0.20131264626979828\n",
      "Training batch loss 70520 0.1983058601617813\n",
      "Training batch loss 70540 0.1991705298423767\n",
      "Training batch loss 70560 0.2009793221950531\n",
      "Training batch loss 70580 0.1980592906475067\n",
      "Training batch loss 70600 0.19715023040771484\n",
      "Training batch loss 70620 0.19260242581367493\n",
      "Training batch loss 70640 0.19880345463752747\n",
      "Training batch loss 70660 0.19632495939731598\n",
      "Training batch loss 70680 0.19872212409973145\n",
      "Training batch loss 70700 0.2011108696460724\n",
      "Training batch loss 70720 0.19334366917610168\n",
      "Training batch loss 70740 0.2011612057685852\n",
      "Training batch loss 70760 0.18688777089118958\n",
      "Training batch loss 70780 0.19738690555095673\n",
      "Training batch loss 70800 0.20211857557296753\n",
      "Training batch loss 70820 0.19543080031871796\n",
      "Training batch loss 70840 0.20585793256759644\n",
      "Training batch loss 70860 0.19723135232925415\n",
      "Training batch loss 70880 0.19044041633605957\n",
      "Training batch loss 70900 0.20283818244934082\n",
      "Training batch loss 70920 0.19655489921569824\n",
      "Training batch loss 70940 0.1955663561820984\n",
      "Training batch loss 70960 0.20146483182907104\n",
      "Training batch loss 70980 0.1988999843597412\n",
      "Training batch loss 71000 0.19841943681240082\n",
      "Training batch loss 71020 0.20389148592948914\n",
      "Training batch loss 71040 0.19226862490177155\n",
      "Training batch loss 71060 0.2035389244556427\n",
      "Training batch loss 71080 0.19257719814777374\n",
      "Training batch loss 71100 0.1931302547454834\n",
      "Training batch loss 71120 0.19947470724582672\n",
      "Training batch loss 71140 0.19628626108169556\n",
      "Training batch loss 71160 0.20586371421813965\n",
      "Training batch loss 71180 0.1968308389186859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss 71200 0.20596987009048462\n",
      "Training batch loss 71220 0.1997758150100708\n",
      "Training batch loss 71240 0.20147401094436646\n",
      "Training batch loss 71260 0.19950464367866516\n",
      "Training batch loss 71280 0.1979617178440094\n",
      "Training batch loss 71300 0.20617517828941345\n",
      "Training batch loss 71320 0.2074558138847351\n",
      "Training batch loss 71340 0.19798772037029266\n",
      "Training batch loss 71360 0.20131948590278625\n",
      "Training batch loss 71380 0.21275150775909424\n",
      "Training batch loss 71400 0.19735084474086761\n",
      "Training batch loss 71420 0.2057875394821167\n",
      "Training batch loss 71440 0.2038630247116089\n",
      "Training batch loss 71460 0.1960001289844513\n",
      "Training batch loss 71480 0.19405975937843323\n",
      "Training batch loss 71500 0.20012935996055603\n",
      "Training batch loss 71520 0.20398616790771484\n",
      "Training batch loss 71540 0.19369086623191833\n",
      "Training batch loss 71560 0.20635294914245605\n",
      "Training batch loss 71580 0.1984732449054718\n",
      "Training batch loss 71600 0.20124468207359314\n",
      "Training batch loss 71620 0.18851247429847717\n",
      "Training batch loss 71640 0.19849780201911926\n",
      "Training batch loss 71660 0.20473094284534454\n",
      "Training batch loss 71680 0.19965031743049622\n",
      "Training batch loss 71700 0.20437705516815186\n",
      "Training batch loss 71720 0.19127696752548218\n",
      "Training batch loss 71740 0.1965131014585495\n",
      "Training batch loss 71760 0.195701465010643\n",
      "Training batch loss 71780 0.19478952884674072\n",
      "Training batch loss 71800 0.201440691947937\n",
      "Training epoch loss 52 0.19857197999954224\n",
      "Test loss during training 52 0.1976657658815384\n",
      "Test accuracy during training 52 0.0\n",
      "Training batch loss 71815 0.19170010089874268\n",
      "Training batch loss 71835 0.19589971005916595\n",
      "Training batch loss 71855 0.20131264626979828\n",
      "Training batch loss 71875 0.1983058601617813\n",
      "Training batch loss 71895 0.1991705298423767\n",
      "Training batch loss 71915 0.2009793221950531\n",
      "Training batch loss 71935 0.1980592906475067\n",
      "Training batch loss 71955 0.19715023040771484\n",
      "Training batch loss 71975 0.19260242581367493\n",
      "Training batch loss 71995 0.19880345463752747\n",
      "Training batch loss 72015 0.19632495939731598\n",
      "Training batch loss 72035 0.19872212409973145\n",
      "Training batch loss 72055 0.2011108696460724\n",
      "Training batch loss 72075 0.19334366917610168\n",
      "Training batch loss 72095 0.2011612057685852\n",
      "Training batch loss 72115 0.18688777089118958\n",
      "Training batch loss 72135 0.19738690555095673\n",
      "Training batch loss 72155 0.20211857557296753\n",
      "Training batch loss 72175 0.19543080031871796\n",
      "Training batch loss 72195 0.20585793256759644\n",
      "Training batch loss 72215 0.19723135232925415\n",
      "Training batch loss 72235 0.19044041633605957\n",
      "Training batch loss 72255 0.20283818244934082\n",
      "Training batch loss 72275 0.19655489921569824\n",
      "Training batch loss 72295 0.1955663561820984\n",
      "Training batch loss 72315 0.20146483182907104\n",
      "Training batch loss 72335 0.1988999843597412\n",
      "Training batch loss 72355 0.19841943681240082\n",
      "Training batch loss 72375 0.20389148592948914\n",
      "Training batch loss 72395 0.19226862490177155\n",
      "Training batch loss 72415 0.2035389244556427\n",
      "Training batch loss 72435 0.19257719814777374\n",
      "Training batch loss 72455 0.1931302547454834\n",
      "Training batch loss 72475 0.19947470724582672\n",
      "Training batch loss 72495 0.19628626108169556\n",
      "Training batch loss 72515 0.20586371421813965\n",
      "Training batch loss 72535 0.1968308389186859\n",
      "Training batch loss 72555 0.20596987009048462\n",
      "Training batch loss 72575 0.1997758150100708\n",
      "Training batch loss 72595 0.20147401094436646\n",
      "Training batch loss 72615 0.19950464367866516\n",
      "Training batch loss 72635 0.1979617178440094\n",
      "Training batch loss 72655 0.20617517828941345\n",
      "Training batch loss 72675 0.2074558138847351\n",
      "Training batch loss 72695 0.19798772037029266\n",
      "Training batch loss 72715 0.20131948590278625\n",
      "Training batch loss 72735 0.21275150775909424\n",
      "Training batch loss 72755 0.19735084474086761\n",
      "Training batch loss 72775 0.2057875394821167\n",
      "Training batch loss 72795 0.2038630247116089\n",
      "Training batch loss 72815 0.1960001289844513\n",
      "Training batch loss 72835 0.19405975937843323\n",
      "Training batch loss 72855 0.20012935996055603\n",
      "Training batch loss 72875 0.20398616790771484\n",
      "Training batch loss 72895 0.19369086623191833\n",
      "Training batch loss 72915 0.20635294914245605\n",
      "Training batch loss 72935 0.1984732449054718\n",
      "Training batch loss 72955 0.20124468207359314\n",
      "Training batch loss 72975 0.18851247429847717\n",
      "Training batch loss 72995 0.19849780201911926\n",
      "Training batch loss 73015 0.20473094284534454\n",
      "Training batch loss 73035 0.19965031743049622\n",
      "Training batch loss 73055 0.20437705516815186\n",
      "Training batch loss 73075 0.19127696752548218\n",
      "Training batch loss 73095 0.1965131014585495\n",
      "Training batch loss 73115 0.195701465010643\n",
      "Training batch loss 73135 0.19478952884674072\n",
      "Training batch loss 73155 0.201440691947937\n",
      "Training epoch loss 53 0.19857197999954224\n",
      "Test loss during training 53 0.1976657509803772\n",
      "Test accuracy during training 53 0.0\n",
      "Training batch loss 73170 0.19170010089874268\n",
      "Training batch loss 73190 0.19589971005916595\n",
      "Training batch loss 73210 0.20131264626979828\n",
      "Training batch loss 73230 0.1983058601617813\n",
      "Training batch loss 73250 0.1991705298423767\n",
      "Training batch loss 73270 0.2009793221950531\n",
      "Training batch loss 73290 0.1980592906475067\n",
      "Training batch loss 73310 0.19715023040771484\n",
      "Training batch loss 73330 0.19260242581367493\n",
      "Training batch loss 73350 0.19880345463752747\n",
      "Training batch loss 73370 0.19632495939731598\n",
      "Training batch loss 73390 0.19872212409973145\n",
      "Training batch loss 73410 0.2011108696460724\n",
      "Training batch loss 73430 0.19334366917610168\n",
      "Training batch loss 73450 0.2011612057685852\n",
      "Training batch loss 73470 0.18688777089118958\n",
      "Training batch loss 73490 0.19738690555095673\n",
      "Training batch loss 73510 0.20211857557296753\n",
      "Training batch loss 73530 0.19543080031871796\n",
      "Training batch loss 73550 0.20585793256759644\n",
      "Training batch loss 73570 0.19723135232925415\n",
      "Training batch loss 73590 0.19044041633605957\n",
      "Training batch loss 73610 0.20283818244934082\n",
      "Training batch loss 73630 0.19655489921569824\n",
      "Training batch loss 73650 0.1955663561820984\n",
      "Training batch loss 73670 0.20146483182907104\n",
      "Training batch loss 73690 0.1988999843597412\n",
      "Training batch loss 73710 0.19841943681240082\n",
      "Training batch loss 73730 0.20389148592948914\n",
      "Training batch loss 73750 0.19226862490177155\n",
      "Training batch loss 73770 0.2035389244556427\n",
      "Training batch loss 73790 0.19257719814777374\n",
      "Training batch loss 73810 0.1931302547454834\n",
      "Training batch loss 73830 0.19947470724582672\n",
      "Training batch loss 73850 0.19628626108169556\n",
      "Training batch loss 73870 0.20586371421813965\n",
      "Training batch loss 73890 0.1968308389186859\n",
      "Training batch loss 73910 0.20596987009048462\n",
      "Training batch loss 73930 0.1997758150100708\n",
      "Training batch loss 73950 0.20147401094436646\n",
      "Training batch loss 73970 0.19950464367866516\n",
      "Training batch loss 73990 0.1979617178440094\n",
      "Training batch loss 74010 0.20617517828941345\n",
      "Training batch loss 74030 0.2074558138847351\n",
      "Training batch loss 74050 0.19798772037029266\n",
      "Training batch loss 74070 0.20131948590278625\n",
      "Training batch loss 74090 0.21275150775909424\n",
      "Training batch loss 74110 0.19735084474086761\n",
      "Training batch loss 74130 0.2057875394821167\n",
      "Training batch loss 74150 0.2038630247116089\n",
      "Training batch loss 74170 0.1960001289844513\n",
      "Training batch loss 74190 0.19405975937843323\n",
      "Training batch loss 74210 0.20012935996055603\n",
      "Training batch loss 74230 0.20398616790771484\n",
      "Training batch loss 74250 0.19369086623191833\n",
      "Training batch loss 74270 0.20635294914245605\n",
      "Training batch loss 74290 0.1984732449054718\n",
      "Training batch loss 74310 0.20124468207359314\n",
      "Training batch loss 74330 0.18851247429847717\n",
      "Training batch loss 74350 0.19849780201911926\n",
      "Training batch loss 74370 0.20473094284534454\n",
      "Training batch loss 74390 0.19965031743049622\n",
      "Training batch loss 74410 0.20437705516815186\n",
      "Training batch loss 74430 0.19127696752548218\n",
      "Training batch loss 74450 0.1965131014585495\n",
      "Training batch loss 74470 0.195701465010643\n",
      "Training batch loss 74490 0.19478952884674072\n",
      "Training batch loss 74510 0.201440691947937\n",
      "Training epoch loss 54 0.19857197999954224\n",
      "Test loss during training 54 0.1976657658815384\n",
      "Test accuracy during training 54 0.0\n",
      "Training batch loss 74525 0.19170010089874268\n",
      "Training batch loss 74545 0.19589971005916595\n",
      "Training batch loss 74565 0.20131264626979828\n",
      "Training batch loss 74585 0.1983058601617813\n",
      "Training batch loss 74605 0.1991705298423767\n",
      "Training batch loss 74625 0.2009793221950531\n",
      "Training batch loss 74645 0.1980592906475067\n",
      "Training batch loss 74665 0.19715023040771484\n",
      "Training batch loss 74685 0.19260242581367493\n",
      "Training batch loss 74705 0.19880345463752747\n",
      "Training batch loss 74725 0.19632495939731598\n",
      "Training batch loss 74745 0.19872212409973145\n",
      "Training batch loss 74765 0.2011108696460724\n",
      "Training batch loss 74785 0.19334366917610168\n",
      "Training batch loss 74805 0.2011612057685852\n",
      "Training batch loss 74825 0.18688777089118958\n",
      "Training batch loss 74845 0.19738690555095673\n",
      "Training batch loss 74865 0.20211857557296753\n",
      "Training batch loss 74885 0.19543080031871796\n",
      "Training batch loss 74905 0.20585793256759644\n",
      "Training batch loss 74925 0.19723135232925415\n",
      "Training batch loss 74945 0.19044041633605957\n",
      "Training batch loss 74965 0.20283818244934082\n",
      "Training batch loss 74985 0.19655489921569824\n",
      "Training batch loss 75005 0.1955663561820984\n",
      "Training batch loss 75025 0.20146483182907104\n",
      "Training batch loss 75045 0.1988999843597412\n",
      "Training batch loss 75065 0.19841943681240082\n",
      "Training batch loss 75085 0.20389148592948914\n",
      "Training batch loss 75105 0.19226862490177155\n",
      "Training batch loss 75125 0.2035389244556427\n",
      "Training batch loss 75145 0.19257719814777374\n",
      "Training batch loss 75165 0.1931302547454834\n",
      "Training batch loss 75185 0.19947470724582672\n",
      "Training batch loss 75205 0.19628626108169556\n",
      "Training batch loss 75225 0.20586371421813965\n",
      "Training batch loss 75245 0.1968308389186859\n",
      "Training batch loss 75265 0.20596987009048462\n",
      "Training batch loss 75285 0.1997758150100708\n",
      "Training batch loss 75305 0.20147401094436646\n",
      "Training batch loss 75325 0.19950464367866516\n",
      "Training batch loss 75345 0.1979617178440094\n",
      "Training batch loss 75365 0.20617517828941345\n",
      "Training batch loss 75385 0.2074558138847351\n",
      "Training batch loss 75405 0.19798772037029266\n",
      "Training batch loss 75425 0.20131948590278625\n",
      "Training batch loss 75445 0.21275150775909424\n",
      "Training batch loss 75465 0.19735084474086761\n",
      "Training batch loss 75485 0.2057875394821167\n",
      "Training batch loss 75505 0.2038630247116089\n",
      "Training batch loss 75525 0.1960001289844513\n",
      "Training batch loss 75545 0.19405975937843323\n",
      "Training batch loss 75565 0.20012935996055603\n",
      "Training batch loss 75585 0.20398616790771484\n",
      "Training batch loss 75605 0.19369086623191833\n",
      "Training batch loss 75625 0.20635294914245605\n",
      "Training batch loss 75645 0.1984732449054718\n",
      "Training batch loss 75665 0.20124468207359314\n",
      "Training batch loss 75685 0.18851247429847717\n",
      "Training batch loss 75705 0.19849780201911926\n",
      "Training batch loss 75725 0.20473094284534454\n",
      "Training batch loss 75745 0.19965031743049622\n",
      "Training batch loss 75765 0.20437705516815186\n",
      "Training batch loss 75785 0.19127696752548218\n",
      "Training batch loss 75805 0.1965131014585495\n",
      "Training batch loss 75825 0.195701465010643\n",
      "Training batch loss 75845 0.19478952884674072\n",
      "Training batch loss 75865 0.201440691947937\n",
      "Training epoch loss 55 0.19857197999954224\n",
      "Test loss during training 55 0.19766579568386078\n",
      "Test accuracy during training 55 0.0\n",
      "Training batch loss 75880 0.19170010089874268\n",
      "Training batch loss 75900 0.19589971005916595\n",
      "Training batch loss 75920 0.20131264626979828\n",
      "Training batch loss 75940 0.1983058601617813\n",
      "Training batch loss 75960 0.1991705298423767\n",
      "Training batch loss 75980 0.2009793221950531\n",
      "Training batch loss 76000 0.1980592906475067\n",
      "Training batch loss 76020 0.19715023040771484\n",
      "Training batch loss 76040 0.19260242581367493\n",
      "Training batch loss 76060 0.19880345463752747\n",
      "Training batch loss 76080 0.19632495939731598\n",
      "Training batch loss 76100 0.19872212409973145\n",
      "Training batch loss 76120 0.2011108696460724\n",
      "Training batch loss 76140 0.19334366917610168\n",
      "Training batch loss 76160 0.2011612057685852\n",
      "Training batch loss 76180 0.18688777089118958\n",
      "Training batch loss 76200 0.19738690555095673\n",
      "Training batch loss 76220 0.20211857557296753\n",
      "Training batch loss 76240 0.19543080031871796\n",
      "Training batch loss 76260 0.20585793256759644\n",
      "Training batch loss 76280 0.19723135232925415\n",
      "Training batch loss 76300 0.19044041633605957\n",
      "Training batch loss 76320 0.20283818244934082\n",
      "Training batch loss 76340 0.19655489921569824\n",
      "Training batch loss 76360 0.1955663561820984\n",
      "Training batch loss 76380 0.20146483182907104\n",
      "Training batch loss 76400 0.1988999843597412\n",
      "Training batch loss 76420 0.19841943681240082\n",
      "Training batch loss 76440 0.20389148592948914\n",
      "Training batch loss 76460 0.19226862490177155\n",
      "Training batch loss 76480 0.2035389244556427\n",
      "Training batch loss 76500 0.19257719814777374\n",
      "Training batch loss 76520 0.1931302547454834\n",
      "Training batch loss 76540 0.19947470724582672\n",
      "Training batch loss 76560 0.19628626108169556\n",
      "Training batch loss 76580 0.20586371421813965\n",
      "Training batch loss 76600 0.1968308389186859\n",
      "Training batch loss 76620 0.20596987009048462\n",
      "Training batch loss 76640 0.1997758150100708\n",
      "Training batch loss 76660 0.20147401094436646\n",
      "Training batch loss 76680 0.19950464367866516\n",
      "Training batch loss 76700 0.1979617178440094\n",
      "Training batch loss 76720 0.20617517828941345\n",
      "Training batch loss 76740 0.2074558138847351\n",
      "Training batch loss 76760 0.19798772037029266\n",
      "Training batch loss 76780 0.20131948590278625\n",
      "Training batch loss 76800 0.21275150775909424\n",
      "Training batch loss 76820 0.19735084474086761\n",
      "Training batch loss 76840 0.2057875394821167\n",
      "Training batch loss 76860 0.2038630247116089\n",
      "Training batch loss 76880 0.1960001289844513\n",
      "Training batch loss 76900 0.19405975937843323\n",
      "Training batch loss 76920 0.20012935996055603\n",
      "Training batch loss 76940 0.20398616790771484\n",
      "Training batch loss 76960 0.19369086623191833\n",
      "Training batch loss 76980 0.20635294914245605\n",
      "Training batch loss 77000 0.1984732449054718\n",
      "Training batch loss 77020 0.20124468207359314\n",
      "Training batch loss 77040 0.18851247429847717\n",
      "Training batch loss 77060 0.19849780201911926\n",
      "Training batch loss 77080 0.20473094284534454\n",
      "Training batch loss 77100 0.19965031743049622\n",
      "Training batch loss 77120 0.20437705516815186\n",
      "Training batch loss 77140 0.19127696752548218\n",
      "Training batch loss 77160 0.1965131014585495\n",
      "Training batch loss 77180 0.195701465010643\n",
      "Training batch loss 77200 0.19478952884674072\n",
      "Training batch loss 77220 0.201440691947937\n",
      "Training epoch loss 56 0.19857197999954224\n",
      "Test loss during training 56 0.1976657658815384\n",
      "Test accuracy during training 56 0.0\n",
      "Training batch loss 77235 0.19170010089874268\n",
      "Training batch loss 77255 0.19589971005916595\n",
      "Training batch loss 77275 0.20131264626979828\n",
      "Training batch loss 77295 0.1983058601617813\n",
      "Training batch loss 77315 0.1991705298423767\n",
      "Training batch loss 77335 0.2009793221950531\n",
      "Training batch loss 77355 0.1980592906475067\n",
      "Training batch loss 77375 0.19715023040771484\n",
      "Training batch loss 77395 0.19260242581367493\n",
      "Training batch loss 77415 0.19880345463752747\n",
      "Training batch loss 77435 0.19632495939731598\n",
      "Training batch loss 77455 0.19872212409973145\n",
      "Training batch loss 77475 0.2011108696460724\n",
      "Training batch loss 77495 0.19334366917610168\n",
      "Training batch loss 77515 0.2011612057685852\n",
      "Training batch loss 77535 0.18688777089118958\n",
      "Training batch loss 77555 0.19738690555095673\n",
      "Training batch loss 77575 0.20211857557296753\n",
      "Training batch loss 77595 0.19543080031871796\n",
      "Training batch loss 77615 0.20585793256759644\n",
      "Training batch loss 77635 0.19723135232925415\n",
      "Training batch loss 77655 0.19044041633605957\n",
      "Training batch loss 77675 0.20283818244934082\n",
      "Training batch loss 77695 0.19655489921569824\n",
      "Training batch loss 77715 0.1955663561820984\n",
      "Training batch loss 77735 0.20146483182907104\n",
      "Training batch loss 77755 0.1988999843597412\n",
      "Training batch loss 77775 0.19841943681240082\n",
      "Training batch loss 77795 0.20389148592948914\n",
      "Training batch loss 77815 0.19226862490177155\n",
      "Training batch loss 77835 0.2035389244556427\n",
      "Training batch loss 77855 0.19257719814777374\n",
      "Training batch loss 77875 0.1931302547454834\n",
      "Training batch loss 77895 0.19947470724582672\n",
      "Training batch loss 77915 0.19628626108169556\n",
      "Training batch loss 77935 0.20586371421813965\n",
      "Training batch loss 77955 0.1968308389186859\n",
      "Training batch loss 77975 0.20596987009048462\n",
      "Training batch loss 77995 0.1997758150100708\n",
      "Training batch loss 78015 0.20147401094436646\n",
      "Training batch loss 78035 0.19950464367866516\n",
      "Training batch loss 78055 0.1979617178440094\n",
      "Training batch loss 78075 0.20617517828941345\n",
      "Training batch loss 78095 0.2074558138847351\n",
      "Training batch loss 78115 0.19798772037029266\n",
      "Training batch loss 78135 0.20131948590278625\n",
      "Training batch loss 78155 0.21275150775909424\n",
      "Training batch loss 78175 0.19735084474086761\n",
      "Training batch loss 78195 0.2057875394821167\n",
      "Training batch loss 78215 0.2038630247116089\n",
      "Training batch loss 78235 0.1960001289844513\n",
      "Training batch loss 78255 0.19405975937843323\n",
      "Training batch loss 78275 0.20012935996055603\n",
      "Training batch loss 78295 0.20398616790771484\n",
      "Training batch loss 78315 0.19369086623191833\n",
      "Training batch loss 78335 0.20635294914245605\n",
      "Training batch loss 78355 0.1984732449054718\n",
      "Training batch loss 78375 0.20124468207359314\n",
      "Training batch loss 78395 0.18851247429847717\n",
      "Training batch loss 78415 0.19849780201911926\n",
      "Training batch loss 78435 0.20473094284534454\n",
      "Training batch loss 78455 0.19965031743049622\n",
      "Training batch loss 78475 0.20437705516815186\n",
      "Training batch loss 78495 0.19127696752548218\n",
      "Training batch loss 78515 0.1965131014585495\n",
      "Training batch loss 78535 0.195701465010643\n",
      "Training batch loss 78555 0.19478952884674072\n",
      "Training batch loss 78575 0.201440691947937\n",
      "Training epoch loss 57 0.19857197999954224\n",
      "Test loss during training 57 0.1976657658815384\n",
      "Test accuracy during training 57 0.0\n",
      "Training batch loss 78590 0.19170010089874268\n",
      "Training batch loss 78610 0.19589971005916595\n",
      "Training batch loss 78630 0.20131264626979828\n",
      "Training batch loss 78650 0.1983058601617813\n",
      "Training batch loss 78670 0.1991705298423767\n",
      "Training batch loss 78690 0.2009793221950531\n",
      "Training batch loss 78710 0.1980592906475067\n",
      "Training batch loss 78730 0.19715023040771484\n",
      "Training batch loss 78750 0.19260242581367493\n",
      "Training batch loss 78770 0.19880345463752747\n",
      "Training batch loss 78790 0.19632495939731598\n",
      "Training batch loss 78810 0.19872212409973145\n",
      "Training batch loss 78830 0.2011108696460724\n",
      "Training batch loss 78850 0.19334366917610168\n",
      "Training batch loss 78870 0.2011612057685852\n",
      "Training batch loss 78890 0.18688777089118958\n",
      "Training batch loss 78910 0.19738690555095673\n",
      "Training batch loss 78930 0.20211857557296753\n",
      "Training batch loss 78950 0.19543080031871796\n",
      "Training batch loss 78970 0.20585793256759644\n",
      "Training batch loss 78990 0.19723135232925415\n",
      "Training batch loss 79010 0.19044041633605957\n",
      "Training batch loss 79030 0.20283818244934082\n",
      "Training batch loss 79050 0.19655489921569824\n",
      "Training batch loss 79070 0.1955663561820984\n",
      "Training batch loss 79090 0.20146483182907104\n",
      "Training batch loss 79110 0.1988999843597412\n",
      "Training batch loss 79130 0.19841943681240082\n",
      "Training batch loss 79150 0.20389148592948914\n",
      "Training batch loss 79170 0.19226862490177155\n",
      "Training batch loss 79190 0.2035389244556427\n",
      "Training batch loss 79210 0.19257719814777374\n",
      "Training batch loss 79230 0.1931302547454834\n",
      "Training batch loss 79250 0.19947470724582672\n",
      "Training batch loss 79270 0.19628626108169556\n",
      "Training batch loss 79290 0.20586371421813965\n",
      "Training batch loss 79310 0.1968308389186859\n",
      "Training batch loss 79330 0.20596987009048462\n",
      "Training batch loss 79350 0.1997758150100708\n",
      "Training batch loss 79370 0.20147401094436646\n",
      "Training batch loss 79390 0.19950464367866516\n",
      "Training batch loss 79410 0.1979617178440094\n",
      "Training batch loss 79430 0.20617517828941345\n",
      "Training batch loss 79450 0.2074558138847351\n",
      "Training batch loss 79470 0.19798772037029266\n",
      "Training batch loss 79490 0.20131948590278625\n",
      "Training batch loss 79510 0.21275150775909424\n",
      "Training batch loss 79530 0.19735084474086761\n",
      "Training batch loss 79550 0.2057875394821167\n",
      "Training batch loss 79570 0.2038630247116089\n",
      "Training batch loss 79590 0.1960001289844513\n",
      "Training batch loss 79610 0.19405975937843323\n",
      "Training batch loss 79630 0.20012935996055603\n",
      "Training batch loss 79650 0.20398616790771484\n",
      "Training batch loss 79670 0.19369086623191833\n",
      "Training batch loss 79690 0.20635294914245605\n",
      "Training batch loss 79710 0.1984732449054718\n",
      "Training batch loss 79730 0.20124468207359314\n",
      "Training batch loss 79750 0.18851247429847717\n",
      "Training batch loss 79770 0.19849780201911926\n",
      "Training batch loss 79790 0.20473094284534454\n",
      "Training batch loss 79810 0.19965031743049622\n",
      "Training batch loss 79830 0.20437705516815186\n",
      "Training batch loss 79850 0.19127696752548218\n",
      "Training batch loss 79870 0.1965131014585495\n",
      "Training batch loss 79890 0.195701465010643\n",
      "Training batch loss 79910 0.19478952884674072\n",
      "Training batch loss 79930 0.201440691947937\n",
      "Training epoch loss 58 0.19857197999954224\n",
      "Test loss during training 58 0.1976657658815384\n",
      "Test accuracy during training 58 0.0\n",
      "Training batch loss 79945 0.19170010089874268\n",
      "Training batch loss 79965 0.19589971005916595\n",
      "Training batch loss 79985 0.20131264626979828\n",
      "Training batch loss 80005 0.1983058601617813\n",
      "Training batch loss 80025 0.1991705298423767\n",
      "Training batch loss 80045 0.2009793221950531\n",
      "Training batch loss 80065 0.1980592906475067\n",
      "Training batch loss 80085 0.19715023040771484\n",
      "Training batch loss 80105 0.19260242581367493\n",
      "Training batch loss 80125 0.19880345463752747\n",
      "Training batch loss 80145 0.19632495939731598\n",
      "Training batch loss 80165 0.19872212409973145\n",
      "Training batch loss 80185 0.2011108696460724\n",
      "Training batch loss 80205 0.19334366917610168\n",
      "Training batch loss 80225 0.2011612057685852\n",
      "Training batch loss 80245 0.18688777089118958\n",
      "Training batch loss 80265 0.19738690555095673\n",
      "Training batch loss 80285 0.20211857557296753\n",
      "Training batch loss 80305 0.19543080031871796\n",
      "Training batch loss 80325 0.20585793256759644\n",
      "Training batch loss 80345 0.19723135232925415\n",
      "Training batch loss 80365 0.19044041633605957\n",
      "Training batch loss 80385 0.20283818244934082\n",
      "Training batch loss 80405 0.19655489921569824\n",
      "Training batch loss 80425 0.1955663561820984\n",
      "Training batch loss 80445 0.20146483182907104\n",
      "Training batch loss 80465 0.1988999843597412\n",
      "Training batch loss 80485 0.19841943681240082\n",
      "Training batch loss 80505 0.20389148592948914\n",
      "Training batch loss 80525 0.19226862490177155\n",
      "Training batch loss 80545 0.2035389244556427\n",
      "Training batch loss 80565 0.19257719814777374\n",
      "Training batch loss 80585 0.1931302547454834\n",
      "Training batch loss 80605 0.19947470724582672\n",
      "Training batch loss 80625 0.19628626108169556\n",
      "Training batch loss 80645 0.20586371421813965\n",
      "Training batch loss 80665 0.1968308389186859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss 80685 0.20596987009048462\n",
      "Training batch loss 80705 0.1997758150100708\n",
      "Training batch loss 80725 0.20147401094436646\n",
      "Training batch loss 80745 0.19950464367866516\n",
      "Training batch loss 80765 0.1979617178440094\n",
      "Training batch loss 80785 0.20617517828941345\n",
      "Training batch loss 80805 0.2074558138847351\n",
      "Training batch loss 80825 0.19798772037029266\n",
      "Training batch loss 80845 0.20131948590278625\n",
      "Training batch loss 80865 0.21275150775909424\n",
      "Training batch loss 80885 0.19735084474086761\n",
      "Training batch loss 80905 0.2057875394821167\n",
      "Training batch loss 80925 0.2038630247116089\n",
      "Training batch loss 80945 0.1960001289844513\n",
      "Training batch loss 80965 0.19405975937843323\n",
      "Training batch loss 80985 0.20012935996055603\n",
      "Training batch loss 81005 0.20398616790771484\n",
      "Training batch loss 81025 0.19369086623191833\n",
      "Training batch loss 81045 0.20635294914245605\n",
      "Training batch loss 81065 0.1984732449054718\n",
      "Training batch loss 81085 0.20124468207359314\n",
      "Training batch loss 81105 0.18851247429847717\n",
      "Training batch loss 81125 0.19849780201911926\n",
      "Training batch loss 81145 0.20473094284534454\n",
      "Training batch loss 81165 0.19965031743049622\n",
      "Training batch loss 81185 0.20437705516815186\n",
      "Training batch loss 81205 0.19127696752548218\n",
      "Training batch loss 81225 0.1965131014585495\n",
      "Training batch loss 81245 0.195701465010643\n",
      "Training batch loss 81265 0.19478952884674072\n",
      "Training batch loss 81285 0.201440691947937\n",
      "Training epoch loss 59 0.19857197999954224\n",
      "Test loss during training 59 0.1976657658815384\n",
      "Test accuracy during training 59 0.0\n",
      "Training batch loss 81300 0.19170010089874268\n",
      "Training batch loss 81320 0.19589971005916595\n",
      "Training batch loss 81340 0.20131264626979828\n",
      "Training batch loss 81360 0.1983058601617813\n",
      "Training batch loss 81380 0.1991705298423767\n",
      "Training batch loss 81400 0.2009793221950531\n",
      "Training batch loss 81420 0.1980592906475067\n",
      "Training batch loss 81440 0.19715023040771484\n",
      "Training batch loss 81460 0.19260242581367493\n",
      "Training batch loss 81480 0.19880345463752747\n",
      "Training batch loss 81500 0.19632495939731598\n",
      "Training batch loss 81520 0.19872212409973145\n",
      "Training batch loss 81540 0.2011108696460724\n",
      "Training batch loss 81560 0.19334366917610168\n",
      "Training batch loss 81580 0.2011612057685852\n",
      "Training batch loss 81600 0.18688777089118958\n",
      "Training batch loss 81620 0.19738690555095673\n",
      "Training batch loss 81640 0.20211857557296753\n",
      "Training batch loss 81660 0.19543080031871796\n",
      "Training batch loss 81680 0.20585793256759644\n",
      "Training batch loss 81700 0.19723135232925415\n",
      "Training batch loss 81720 0.19044041633605957\n",
      "Training batch loss 81740 0.20283818244934082\n",
      "Training batch loss 81760 0.19655489921569824\n",
      "Training batch loss 81780 0.1955663561820984\n",
      "Training batch loss 81800 0.20146483182907104\n",
      "Training batch loss 81820 0.1988999843597412\n",
      "Training batch loss 81840 0.19841943681240082\n",
      "Training batch loss 81860 0.20389148592948914\n",
      "Training batch loss 81880 0.19226862490177155\n",
      "Training batch loss 81900 0.2035389244556427\n",
      "Training batch loss 81920 0.19257719814777374\n",
      "Training batch loss 81940 0.1931302547454834\n",
      "Training batch loss 81960 0.19947470724582672\n",
      "Training batch loss 81980 0.19628626108169556\n",
      "Training batch loss 82000 0.20586371421813965\n",
      "Training batch loss 82020 0.1968308389186859\n",
      "Training batch loss 82040 0.20596987009048462\n",
      "Training batch loss 82060 0.1997758150100708\n",
      "Training batch loss 82080 0.20147401094436646\n",
      "Training batch loss 82100 0.19950464367866516\n",
      "Training batch loss 82120 0.1979617178440094\n",
      "Training batch loss 82140 0.20617517828941345\n",
      "Training batch loss 82160 0.2074558138847351\n",
      "Training batch loss 82180 0.19798772037029266\n",
      "Training batch loss 82200 0.20131948590278625\n",
      "Training batch loss 82220 0.21275150775909424\n",
      "Training batch loss 82240 0.19735084474086761\n",
      "Training batch loss 82260 0.2057875394821167\n",
      "Training batch loss 82280 0.2038630247116089\n",
      "Training batch loss 82300 0.1960001289844513\n",
      "Training batch loss 82320 0.19405975937843323\n",
      "Training batch loss 82340 0.20012935996055603\n",
      "Training batch loss 82360 0.20398616790771484\n",
      "Training batch loss 82380 0.19369086623191833\n",
      "Training batch loss 82400 0.20635294914245605\n",
      "Training batch loss 82420 0.1984732449054718\n",
      "Training batch loss 82440 0.20124468207359314\n",
      "Training batch loss 82460 0.18851247429847717\n",
      "Training batch loss 82480 0.19849780201911926\n",
      "Training batch loss 82500 0.20473094284534454\n",
      "Training batch loss 82520 0.19965031743049622\n",
      "Training batch loss 82540 0.20437705516815186\n",
      "Training batch loss 82560 0.19127696752548218\n",
      "Training batch loss 82580 0.1965131014585495\n",
      "Training batch loss 82600 0.195701465010643\n",
      "Training batch loss 82620 0.19478952884674072\n",
      "Training batch loss 82640 0.201440691947937\n",
      "Training epoch loss 60 0.19857197999954224\n",
      "Test loss during training 60 0.1976657658815384\n",
      "Test accuracy during training 60 0.0\n",
      "Training batch loss 82655 0.19170010089874268\n",
      "Training batch loss 82675 0.19589971005916595\n",
      "Training batch loss 82695 0.20131264626979828\n",
      "Training batch loss 82715 0.1983058601617813\n",
      "Training batch loss 82735 0.1991705298423767\n",
      "Training batch loss 82755 0.2009793221950531\n",
      "Training batch loss 82775 0.1980592906475067\n",
      "Training batch loss 82795 0.19715023040771484\n",
      "Training batch loss 82815 0.19260242581367493\n",
      "Training batch loss 82835 0.19880345463752747\n",
      "Training batch loss 82855 0.19632495939731598\n",
      "Training batch loss 82875 0.19872212409973145\n",
      "Training batch loss 82895 0.2011108696460724\n",
      "Training batch loss 82915 0.19334366917610168\n",
      "Training batch loss 82935 0.2011612057685852\n",
      "Training batch loss 82955 0.18688777089118958\n",
      "Training batch loss 82975 0.19738690555095673\n",
      "Training batch loss 82995 0.20211857557296753\n",
      "Training batch loss 83015 0.19543080031871796\n",
      "Training batch loss 83035 0.20585793256759644\n",
      "Training batch loss 83055 0.19723135232925415\n",
      "Training batch loss 83075 0.19044041633605957\n",
      "Training batch loss 83095 0.20283818244934082\n",
      "Training batch loss 83115 0.19655489921569824\n",
      "Training batch loss 83135 0.1955663561820984\n",
      "Training batch loss 83155 0.20146483182907104\n",
      "Training batch loss 83175 0.1988999843597412\n",
      "Training batch loss 83195 0.19841943681240082\n",
      "Training batch loss 83215 0.20389148592948914\n",
      "Training batch loss 83235 0.19226862490177155\n",
      "Training batch loss 83255 0.2035389244556427\n",
      "Training batch loss 83275 0.19257719814777374\n",
      "Training batch loss 83295 0.1931302547454834\n",
      "Training batch loss 83315 0.19947470724582672\n",
      "Training batch loss 83335 0.19628626108169556\n",
      "Training batch loss 83355 0.20586371421813965\n",
      "Training batch loss 83375 0.1968308389186859\n",
      "Training batch loss 83395 0.20596987009048462\n",
      "Training batch loss 83415 0.1997758150100708\n",
      "Training batch loss 83435 0.20147401094436646\n",
      "Training batch loss 83455 0.19950464367866516\n",
      "Training batch loss 83475 0.1979617178440094\n",
      "Training batch loss 83495 0.20617517828941345\n",
      "Training batch loss 83515 0.2074558138847351\n",
      "Training batch loss 83535 0.19798772037029266\n",
      "Training batch loss 83555 0.20131948590278625\n",
      "Training batch loss 83575 0.21275150775909424\n",
      "Training batch loss 83595 0.19735084474086761\n",
      "Training batch loss 83615 0.2057875394821167\n",
      "Training batch loss 83635 0.2038630247116089\n",
      "Training batch loss 83655 0.1960001289844513\n",
      "Training batch loss 83675 0.19405975937843323\n",
      "Training batch loss 83695 0.20012935996055603\n",
      "Training batch loss 83715 0.20398616790771484\n",
      "Training batch loss 83735 0.19369086623191833\n",
      "Training batch loss 83755 0.20635294914245605\n",
      "Training batch loss 83775 0.1984732449054718\n",
      "Training batch loss 83795 0.20124468207359314\n",
      "Training batch loss 83815 0.18851247429847717\n",
      "Training batch loss 83835 0.19849780201911926\n",
      "Training batch loss 83855 0.20473094284534454\n",
      "Training batch loss 83875 0.19965031743049622\n",
      "Training batch loss 83895 0.20437705516815186\n",
      "Training batch loss 83915 0.19127696752548218\n",
      "Training batch loss 83935 0.1965131014585495\n",
      "Training batch loss 83955 0.195701465010643\n",
      "Training batch loss 83975 0.19478952884674072\n",
      "Training batch loss 83995 0.201440691947937\n",
      "Training epoch loss 61 0.19857197999954224\n",
      "Test loss during training 61 0.1976657658815384\n",
      "Test accuracy during training 61 0.0\n",
      "Training batch loss 84010 0.19170010089874268\n",
      "Training batch loss 84030 0.19589971005916595\n",
      "Training batch loss 84050 0.20131264626979828\n",
      "Training batch loss 84070 0.1983058601617813\n",
      "Training batch loss 84090 0.1991705298423767\n",
      "Training batch loss 84110 0.2009793221950531\n",
      "Training batch loss 84130 0.1980592906475067\n",
      "Training batch loss 84150 0.19715023040771484\n",
      "Training batch loss 84170 0.19260242581367493\n",
      "Training batch loss 84190 0.19880345463752747\n",
      "Training batch loss 84210 0.19632495939731598\n",
      "Training batch loss 84230 0.19872212409973145\n",
      "Training batch loss 84250 0.2011108696460724\n",
      "Training batch loss 84270 0.19334366917610168\n",
      "Training batch loss 84290 0.2011612057685852\n",
      "Training batch loss 84310 0.18688777089118958\n",
      "Training batch loss 84330 0.19738690555095673\n",
      "Training batch loss 84350 0.20211857557296753\n",
      "Training batch loss 84370 0.19543080031871796\n",
      "Training batch loss 84390 0.20585793256759644\n",
      "Training batch loss 84410 0.19723135232925415\n",
      "Training batch loss 84430 0.19044041633605957\n",
      "Training batch loss 84450 0.20283818244934082\n",
      "Training batch loss 84470 0.19655489921569824\n",
      "Training batch loss 84490 0.1955663561820984\n",
      "Training batch loss 84510 0.20146483182907104\n",
      "Training batch loss 84530 0.1988999843597412\n",
      "Training batch loss 84550 0.19841943681240082\n",
      "Training batch loss 84570 0.20389148592948914\n",
      "Training batch loss 84590 0.19226862490177155\n",
      "Training batch loss 84610 0.2035389244556427\n",
      "Training batch loss 84630 0.19257719814777374\n",
      "Training batch loss 84650 0.1931302547454834\n",
      "Training batch loss 84670 0.19947470724582672\n",
      "Training batch loss 84690 0.19628626108169556\n",
      "Training batch loss 84710 0.20586371421813965\n",
      "Training batch loss 84730 0.1968308389186859\n",
      "Training batch loss 84750 0.20596987009048462\n",
      "Training batch loss 84770 0.1997758150100708\n",
      "Training batch loss 84790 0.20147401094436646\n",
      "Training batch loss 84810 0.19950464367866516\n",
      "Training batch loss 84830 0.1979617178440094\n",
      "Training batch loss 84850 0.20617517828941345\n",
      "Training batch loss 84870 0.2074558138847351\n",
      "Training batch loss 84890 0.19798772037029266\n",
      "Training batch loss 84910 0.20131948590278625\n",
      "Training batch loss 84930 0.21275150775909424\n",
      "Training batch loss 84950 0.19735084474086761\n",
      "Training batch loss 84970 0.2057875394821167\n",
      "Training batch loss 84990 0.2038630247116089\n",
      "Training batch loss 85010 0.1960001289844513\n",
      "Training batch loss 85030 0.19405975937843323\n",
      "Training batch loss 85050 0.20012935996055603\n",
      "Training batch loss 85070 0.20398616790771484\n",
      "Training batch loss 85090 0.19369086623191833\n",
      "Training batch loss 85110 0.20635294914245605\n",
      "Training batch loss 85130 0.1984732449054718\n",
      "Training batch loss 85150 0.20124468207359314\n",
      "Training batch loss 85170 0.18851247429847717\n",
      "Training batch loss 85190 0.19849780201911926\n",
      "Training batch loss 85210 0.20473094284534454\n",
      "Training batch loss 85230 0.19965031743049622\n",
      "Training batch loss 85250 0.20437705516815186\n",
      "Training batch loss 85270 0.19127696752548218\n",
      "Training batch loss 85290 0.1965131014585495\n",
      "Training batch loss 85310 0.195701465010643\n",
      "Training batch loss 85330 0.19478952884674072\n",
      "Training batch loss 85350 0.201440691947937\n",
      "Training epoch loss 62 0.19857197999954224\n",
      "Test loss during training 62 0.1976657658815384\n",
      "Test accuracy during training 62 0.0\n",
      "Training batch loss 85365 0.19170010089874268\n",
      "Training batch loss 85385 0.19589971005916595\n",
      "Training batch loss 85405 0.20131264626979828\n",
      "Training batch loss 85425 0.1983058601617813\n",
      "Training batch loss 85445 0.1991705298423767\n",
      "Training batch loss 85465 0.2009793221950531\n",
      "Training batch loss 85485 0.1980592906475067\n",
      "Training batch loss 85505 0.19715023040771484\n",
      "Training batch loss 85525 0.19260242581367493\n",
      "Training batch loss 85545 0.19880345463752747\n",
      "Training batch loss 85565 0.19632495939731598\n",
      "Training batch loss 85585 0.19872212409973145\n",
      "Training batch loss 85605 0.2011108696460724\n",
      "Training batch loss 85625 0.19334366917610168\n",
      "Training batch loss 85645 0.2011612057685852\n",
      "Training batch loss 85665 0.18688777089118958\n",
      "Training batch loss 85685 0.19738690555095673\n",
      "Training batch loss 85705 0.20211857557296753\n",
      "Training batch loss 85725 0.19543080031871796\n",
      "Training batch loss 85745 0.20585793256759644\n",
      "Training batch loss 85765 0.19723135232925415\n",
      "Training batch loss 85785 0.19044041633605957\n",
      "Training batch loss 85805 0.20283818244934082\n",
      "Training batch loss 85825 0.19655489921569824\n",
      "Training batch loss 85845 0.1955663561820984\n",
      "Training batch loss 85865 0.20146483182907104\n",
      "Training batch loss 85885 0.1988999843597412\n",
      "Training batch loss 85905 0.19841943681240082\n",
      "Training batch loss 85925 0.20389148592948914\n",
      "Training batch loss 85945 0.19226862490177155\n",
      "Training batch loss 85965 0.2035389244556427\n",
      "Training batch loss 85985 0.19257719814777374\n",
      "Training batch loss 86005 0.1931302547454834\n",
      "Training batch loss 86025 0.19947470724582672\n",
      "Training batch loss 86045 0.19628626108169556\n",
      "Training batch loss 86065 0.20586371421813965\n",
      "Training batch loss 86085 0.1968308389186859\n",
      "Training batch loss 86105 0.20596987009048462\n",
      "Training batch loss 86125 0.1997758150100708\n",
      "Training batch loss 86145 0.20147401094436646\n",
      "Training batch loss 86165 0.19950464367866516\n",
      "Training batch loss 86185 0.1979617178440094\n",
      "Training batch loss 86205 0.20617517828941345\n",
      "Training batch loss 86225 0.2074558138847351\n",
      "Training batch loss 86245 0.19798772037029266\n",
      "Training batch loss 86265 0.20131948590278625\n",
      "Training batch loss 86285 0.21275150775909424\n",
      "Training batch loss 86305 0.19735084474086761\n",
      "Training batch loss 86325 0.2057875394821167\n",
      "Training batch loss 86345 0.2038630247116089\n",
      "Training batch loss 86365 0.1960001289844513\n",
      "Training batch loss 86385 0.19405975937843323\n",
      "Training batch loss 86405 0.20012935996055603\n",
      "Training batch loss 86425 0.20398616790771484\n",
      "Training batch loss 86445 0.19369086623191833\n",
      "Training batch loss 86465 0.20635294914245605\n",
      "Training batch loss 86485 0.1984732449054718\n",
      "Training batch loss 86505 0.20124468207359314\n",
      "Training batch loss 86525 0.18851247429847717\n",
      "Training batch loss 86545 0.19849780201911926\n",
      "Training batch loss 86565 0.20473094284534454\n",
      "Training batch loss 86585 0.19965031743049622\n",
      "Training batch loss 86605 0.20437705516815186\n",
      "Training batch loss 86625 0.19127696752548218\n",
      "Training batch loss 86645 0.1965131014585495\n",
      "Training batch loss 86665 0.195701465010643\n",
      "Training batch loss 86685 0.19478952884674072\n",
      "Training batch loss 86705 0.201440691947937\n",
      "Training epoch loss 63 0.19857197999954224\n",
      "Test loss during training 63 0.1976657658815384\n",
      "Test accuracy during training 63 0.0\n",
      "Training batch loss 86720 0.19170010089874268\n",
      "Training batch loss 86740 0.19589971005916595\n",
      "Training batch loss 86760 0.20131264626979828\n",
      "Training batch loss 86780 0.1983058601617813\n",
      "Training batch loss 86800 0.1991705298423767\n",
      "Training batch loss 86820 0.2009793221950531\n",
      "Training batch loss 86840 0.1980592906475067\n",
      "Training batch loss 86860 0.19715023040771484\n",
      "Training batch loss 86880 0.19260242581367493\n",
      "Training batch loss 86900 0.19880345463752747\n",
      "Training batch loss 86920 0.19632495939731598\n",
      "Training batch loss 86940 0.19872212409973145\n",
      "Training batch loss 86960 0.2011108696460724\n",
      "Training batch loss 86980 0.19334366917610168\n",
      "Training batch loss 87000 0.2011612057685852\n",
      "Training batch loss 87020 0.18688777089118958\n",
      "Training batch loss 87040 0.19738690555095673\n",
      "Training batch loss 87060 0.20211857557296753\n",
      "Training batch loss 87080 0.19543080031871796\n",
      "Training batch loss 87100 0.20585793256759644\n",
      "Training batch loss 87120 0.19723135232925415\n",
      "Training batch loss 87140 0.19044041633605957\n",
      "Training batch loss 87160 0.20283818244934082\n",
      "Training batch loss 87180 0.19655489921569824\n",
      "Training batch loss 87200 0.1955663561820984\n",
      "Training batch loss 87220 0.20146483182907104\n",
      "Training batch loss 87240 0.1988999843597412\n",
      "Training batch loss 87260 0.19841943681240082\n",
      "Training batch loss 87280 0.20389148592948914\n",
      "Training batch loss 87300 0.19226862490177155\n",
      "Training batch loss 87320 0.2035389244556427\n",
      "Training batch loss 87340 0.19257719814777374\n",
      "Training batch loss 87360 0.1931302547454834\n",
      "Training batch loss 87380 0.19947470724582672\n",
      "Training batch loss 87400 0.19628626108169556\n",
      "Training batch loss 87420 0.20586371421813965\n",
      "Training batch loss 87440 0.1968308389186859\n",
      "Training batch loss 87460 0.20596987009048462\n",
      "Training batch loss 87480 0.1997758150100708\n",
      "Training batch loss 87500 0.20147401094436646\n",
      "Training batch loss 87520 0.19950464367866516\n",
      "Training batch loss 87540 0.1979617178440094\n",
      "Training batch loss 87560 0.20617517828941345\n",
      "Training batch loss 87580 0.2074558138847351\n",
      "Training batch loss 87600 0.19798772037029266\n",
      "Training batch loss 87620 0.20131948590278625\n",
      "Training batch loss 87640 0.21275150775909424\n",
      "Training batch loss 87660 0.19735084474086761\n",
      "Training batch loss 87680 0.2057875394821167\n",
      "Training batch loss 87700 0.2038630247116089\n",
      "Training batch loss 87720 0.1960001289844513\n",
      "Training batch loss 87740 0.19405975937843323\n",
      "Training batch loss 87760 0.20012935996055603\n",
      "Training batch loss 87780 0.20398616790771484\n",
      "Training batch loss 87800 0.19369086623191833\n",
      "Training batch loss 87820 0.20635294914245605\n",
      "Training batch loss 87840 0.1984732449054718\n",
      "Training batch loss 87860 0.20124468207359314\n",
      "Training batch loss 87880 0.18851247429847717\n",
      "Training batch loss 87900 0.19849780201911926\n",
      "Training batch loss 87920 0.20473094284534454\n",
      "Training batch loss 87940 0.19965031743049622\n",
      "Training batch loss 87960 0.20437705516815186\n",
      "Training batch loss 87980 0.19127696752548218\n",
      "Training batch loss 88000 0.1965131014585495\n",
      "Training batch loss 88020 0.195701465010643\n",
      "Training batch loss 88040 0.19478952884674072\n",
      "Training batch loss 88060 0.201440691947937\n",
      "Training epoch loss 64 0.19857197999954224\n",
      "Test loss during training 64 0.1976657658815384\n",
      "Test accuracy during training 64 0.0\n",
      "Training batch loss 88075 0.19170010089874268\n",
      "Training batch loss 88095 0.19589971005916595\n",
      "Training batch loss 88115 0.20131264626979828\n",
      "Training batch loss 88135 0.1983058601617813\n",
      "Training batch loss 88155 0.1991705298423767\n",
      "Training batch loss 88175 0.2009793221950531\n",
      "Training batch loss 88195 0.1980592906475067\n",
      "Training batch loss 88215 0.19715023040771484\n",
      "Training batch loss 88235 0.19260242581367493\n",
      "Training batch loss 88255 0.19880345463752747\n",
      "Training batch loss 88275 0.19632495939731598\n",
      "Training batch loss 88295 0.19872212409973145\n",
      "Training batch loss 88315 0.2011108696460724\n",
      "Training batch loss 88335 0.19334366917610168\n",
      "Training batch loss 88355 0.2011612057685852\n",
      "Training batch loss 88375 0.18688777089118958\n",
      "Training batch loss 88395 0.19738690555095673\n",
      "Training batch loss 88415 0.20211857557296753\n",
      "Training batch loss 88435 0.19543080031871796\n",
      "Training batch loss 88455 0.20585793256759644\n",
      "Training batch loss 88475 0.19723135232925415\n",
      "Training batch loss 88495 0.19044041633605957\n",
      "Training batch loss 88515 0.20283818244934082\n",
      "Training batch loss 88535 0.19655489921569824\n",
      "Training batch loss 88555 0.1955663561820984\n",
      "Training batch loss 88575 0.20146483182907104\n",
      "Training batch loss 88595 0.1988999843597412\n",
      "Training batch loss 88615 0.19841943681240082\n",
      "Training batch loss 88635 0.20389148592948914\n",
      "Training batch loss 88655 0.19226862490177155\n",
      "Training batch loss 88675 0.2035389244556427\n",
      "Training batch loss 88695 0.19257719814777374\n",
      "Training batch loss 88715 0.1931302547454834\n",
      "Training batch loss 88735 0.19947470724582672\n",
      "Training batch loss 88755 0.19628626108169556\n",
      "Training batch loss 88775 0.20586371421813965\n",
      "Training batch loss 88795 0.1968308389186859\n",
      "Training batch loss 88815 0.20596987009048462\n",
      "Training batch loss 88835 0.1997758150100708\n",
      "Training batch loss 88855 0.20147401094436646\n",
      "Training batch loss 88875 0.19950464367866516\n",
      "Training batch loss 88895 0.1979617178440094\n",
      "Training batch loss 88915 0.20617517828941345\n",
      "Training batch loss 88935 0.2074558138847351\n",
      "Training batch loss 88955 0.19798772037029266\n",
      "Training batch loss 88975 0.20131948590278625\n",
      "Training batch loss 88995 0.21275150775909424\n",
      "Training batch loss 89015 0.19735084474086761\n",
      "Training batch loss 89035 0.2057875394821167\n",
      "Training batch loss 89055 0.2038630247116089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss 89075 0.1960001289844513\n",
      "Training batch loss 89095 0.19405975937843323\n",
      "Training batch loss 89115 0.20012935996055603\n",
      "Training batch loss 89135 0.20398616790771484\n",
      "Training batch loss 89155 0.19369086623191833\n",
      "Training batch loss 89175 0.20635294914245605\n",
      "Training batch loss 89195 0.1984732449054718\n",
      "Training batch loss 89215 0.20124468207359314\n",
      "Training batch loss 89235 0.18851247429847717\n",
      "Training batch loss 89255 0.19849780201911926\n",
      "Training batch loss 89275 0.20473094284534454\n",
      "Training batch loss 89295 0.19965031743049622\n",
      "Training batch loss 89315 0.20437705516815186\n",
      "Training batch loss 89335 0.19127696752548218\n",
      "Training batch loss 89355 0.1965131014585495\n",
      "Training batch loss 89375 0.195701465010643\n",
      "Training batch loss 89395 0.19478952884674072\n",
      "Training batch loss 89415 0.201440691947937\n",
      "Training epoch loss 65 0.19857197999954224\n",
      "Test loss during training 65 0.1976657658815384\n",
      "Test accuracy during training 65 0.0\n",
      "Training batch loss 89430 0.19170010089874268\n",
      "Training batch loss 89450 0.19589971005916595\n",
      "Training batch loss 89470 0.20131264626979828\n",
      "Training batch loss 89490 0.1983058601617813\n",
      "Training batch loss 89510 0.1991705298423767\n",
      "Training batch loss 89530 0.2009793221950531\n",
      "Training batch loss 89550 0.1980592906475067\n",
      "Training batch loss 89570 0.19715023040771484\n",
      "Training batch loss 89590 0.19260242581367493\n",
      "Training batch loss 89610 0.19880345463752747\n",
      "Training batch loss 89630 0.19632495939731598\n",
      "Training batch loss 89650 0.19872212409973145\n",
      "Training batch loss 89670 0.2011108696460724\n",
      "Training batch loss 89690 0.19334366917610168\n",
      "Training batch loss 89710 0.2011612057685852\n",
      "Training batch loss 89730 0.18688777089118958\n",
      "Training batch loss 89750 0.19738690555095673\n",
      "Training batch loss 89770 0.20211857557296753\n",
      "Training batch loss 89790 0.19543080031871796\n",
      "Training batch loss 89810 0.20585793256759644\n",
      "Training batch loss 89830 0.19723135232925415\n",
      "Training batch loss 89850 0.19044041633605957\n",
      "Training batch loss 89870 0.20283818244934082\n",
      "Training batch loss 89890 0.19655489921569824\n",
      "Training batch loss 89910 0.1955663561820984\n",
      "Training batch loss 89930 0.20146483182907104\n",
      "Training batch loss 89950 0.1988999843597412\n",
      "Training batch loss 89970 0.19841943681240082\n",
      "Training batch loss 89990 0.20389148592948914\n",
      "Training batch loss 90010 0.19226862490177155\n",
      "Training batch loss 90030 0.2035389244556427\n",
      "Training batch loss 90050 0.19257719814777374\n",
      "Training batch loss 90070 0.1931302547454834\n",
      "Training batch loss 90090 0.19947470724582672\n",
      "Training batch loss 90110 0.19628626108169556\n",
      "Training batch loss 90130 0.20586371421813965\n",
      "Training batch loss 90150 0.1968308389186859\n",
      "Training batch loss 90170 0.20596987009048462\n",
      "Training batch loss 90190 0.1997758150100708\n",
      "Training batch loss 90210 0.20147401094436646\n",
      "Training batch loss 90230 0.19950464367866516\n",
      "Training batch loss 90250 0.1979617178440094\n",
      "Training batch loss 90270 0.20617517828941345\n",
      "Training batch loss 90290 0.2074558138847351\n",
      "Training batch loss 90310 0.19798772037029266\n",
      "Training batch loss 90330 0.20131948590278625\n",
      "Training batch loss 90350 0.21275150775909424\n",
      "Training batch loss 90370 0.19735084474086761\n",
      "Training batch loss 90390 0.2057875394821167\n",
      "Training batch loss 90410 0.2038630247116089\n",
      "Training batch loss 90430 0.1960001289844513\n",
      "Training batch loss 90450 0.19405975937843323\n",
      "Training batch loss 90470 0.20012935996055603\n",
      "Training batch loss 90490 0.20398616790771484\n",
      "Training batch loss 90510 0.19369086623191833\n",
      "Training batch loss 90530 0.20635294914245605\n",
      "Training batch loss 90550 0.1984732449054718\n",
      "Training batch loss 90570 0.20124468207359314\n",
      "Training batch loss 90590 0.18851247429847717\n",
      "Training batch loss 90610 0.19849780201911926\n",
      "Training batch loss 90630 0.20473094284534454\n",
      "Training batch loss 90650 0.19965031743049622\n",
      "Training batch loss 90670 0.20437705516815186\n",
      "Training batch loss 90690 0.19127696752548218\n",
      "Training batch loss 90710 0.1965131014585495\n",
      "Training batch loss 90730 0.195701465010643\n",
      "Training batch loss 90750 0.19478952884674072\n",
      "Training batch loss 90770 0.201440691947937\n",
      "Training epoch loss 66 0.19857197999954224\n",
      "Test loss during training 66 0.1976657658815384\n",
      "Test accuracy during training 66 0.0\n",
      "Training batch loss 90785 0.19170010089874268\n",
      "Training batch loss 90805 0.19589971005916595\n",
      "Training batch loss 90825 0.20131264626979828\n",
      "Training batch loss 90845 0.1983058601617813\n",
      "Training batch loss 90865 0.1991705298423767\n",
      "Training batch loss 90885 0.2009793221950531\n",
      "Training batch loss 90905 0.1980592906475067\n",
      "Training batch loss 90925 0.19715023040771484\n",
      "Training batch loss 90945 0.19260242581367493\n",
      "Training batch loss 90965 0.19880345463752747\n",
      "Training batch loss 90985 0.19632495939731598\n",
      "Training batch loss 91005 0.19872212409973145\n",
      "Training batch loss 91025 0.2011108696460724\n",
      "Training batch loss 91045 0.19334366917610168\n",
      "Training batch loss 91065 0.2011612057685852\n",
      "Training batch loss 91085 0.18688777089118958\n",
      "Training batch loss 91105 0.19738690555095673\n",
      "Training batch loss 91125 0.20211857557296753\n",
      "Training batch loss 91145 0.19543080031871796\n",
      "Training batch loss 91165 0.20585793256759644\n",
      "Training batch loss 91185 0.19723135232925415\n",
      "Training batch loss 91205 0.19044041633605957\n",
      "Training batch loss 91225 0.20283818244934082\n",
      "Training batch loss 91245 0.19655489921569824\n",
      "Training batch loss 91265 0.1955663561820984\n",
      "Training batch loss 91285 0.20146483182907104\n",
      "Training batch loss 91305 0.1988999843597412\n",
      "Training batch loss 91325 0.19841943681240082\n",
      "Training batch loss 91345 0.20389148592948914\n",
      "Training batch loss 91365 0.19226862490177155\n",
      "Training batch loss 91385 0.2035389244556427\n",
      "Training batch loss 91405 0.19257719814777374\n",
      "Training batch loss 91425 0.1931302547454834\n",
      "Training batch loss 91445 0.19947470724582672\n",
      "Training batch loss 91465 0.19628626108169556\n",
      "Training batch loss 91485 0.20586371421813965\n",
      "Training batch loss 91505 0.1968308389186859\n",
      "Training batch loss 91525 0.20596987009048462\n",
      "Training batch loss 91545 0.1997758150100708\n",
      "Training batch loss 91565 0.20147401094436646\n",
      "Training batch loss 91585 0.19950464367866516\n",
      "Training batch loss 91605 0.1979617178440094\n",
      "Training batch loss 91625 0.20617517828941345\n",
      "Training batch loss 91645 0.2074558138847351\n",
      "Training batch loss 91665 0.19798772037029266\n",
      "Training batch loss 91685 0.20131948590278625\n",
      "Training batch loss 91705 0.21275150775909424\n",
      "Training batch loss 91725 0.19735084474086761\n",
      "Training batch loss 91745 0.2057875394821167\n",
      "Training batch loss 91765 0.2038630247116089\n",
      "Training batch loss 91785 0.1960001289844513\n",
      "Training batch loss 91805 0.19405975937843323\n",
      "Training batch loss 91825 0.20012935996055603\n",
      "Training batch loss 91845 0.20398616790771484\n",
      "Training batch loss 91865 0.19369086623191833\n",
      "Training batch loss 91885 0.20635294914245605\n",
      "Training batch loss 91905 0.1984732449054718\n",
      "Training batch loss 91925 0.20124468207359314\n",
      "Training batch loss 91945 0.18851247429847717\n",
      "Training batch loss 91965 0.19849780201911926\n",
      "Training batch loss 91985 0.20473094284534454\n",
      "Training batch loss 92005 0.19965031743049622\n",
      "Training batch loss 92025 0.20437705516815186\n",
      "Training batch loss 92045 0.19127696752548218\n",
      "Training batch loss 92065 0.1965131014585495\n",
      "Training batch loss 92085 0.195701465010643\n",
      "Training batch loss 92105 0.19478952884674072\n",
      "Training batch loss 92125 0.201440691947937\n",
      "Training epoch loss 67 0.19857197999954224\n",
      "Test loss during training 67 0.1976657658815384\n",
      "Test accuracy during training 67 0.0\n",
      "Training batch loss 92140 0.19170010089874268\n",
      "Training batch loss 92160 0.19589971005916595\n",
      "Training batch loss 92180 0.20131264626979828\n",
      "Training batch loss 92200 0.1983058601617813\n",
      "Training batch loss 92220 0.1991705298423767\n",
      "Training batch loss 92240 0.2009793221950531\n",
      "Training batch loss 92260 0.1980592906475067\n",
      "Training batch loss 92280 0.19715023040771484\n",
      "Training batch loss 92300 0.19260242581367493\n",
      "Training batch loss 92320 0.19880345463752747\n",
      "Training batch loss 92340 0.19632495939731598\n",
      "Training batch loss 92360 0.19872212409973145\n",
      "Training batch loss 92380 0.2011108696460724\n",
      "Training batch loss 92400 0.19334366917610168\n",
      "Training batch loss 92420 0.2011612057685852\n",
      "Training batch loss 92440 0.18688777089118958\n",
      "Training batch loss 92460 0.19738690555095673\n",
      "Training batch loss 92480 0.20211857557296753\n",
      "Training batch loss 92500 0.19543080031871796\n",
      "Training batch loss 92520 0.20585793256759644\n",
      "Training batch loss 92540 0.19723135232925415\n",
      "Training batch loss 92560 0.19044041633605957\n",
      "Training batch loss 92580 0.20283818244934082\n",
      "Training batch loss 92600 0.19655489921569824\n",
      "Training batch loss 92620 0.1955663561820984\n",
      "Training batch loss 92640 0.20146483182907104\n",
      "Training batch loss 92660 0.1988999843597412\n",
      "Training batch loss 92680 0.19841943681240082\n",
      "Training batch loss 92700 0.20389148592948914\n",
      "Training batch loss 92720 0.19226862490177155\n",
      "Training batch loss 92740 0.2035389244556427\n",
      "Training batch loss 92760 0.19257719814777374\n",
      "Training batch loss 92780 0.1931302547454834\n",
      "Training batch loss 92800 0.19947470724582672\n",
      "Training batch loss 92820 0.19628626108169556\n",
      "Training batch loss 92840 0.20586371421813965\n",
      "Training batch loss 92860 0.1968308389186859\n",
      "Training batch loss 92880 0.20596987009048462\n",
      "Training batch loss 92900 0.1997758150100708\n",
      "Training batch loss 92920 0.20147401094436646\n",
      "Training batch loss 92940 0.19950464367866516\n",
      "Training batch loss 92960 0.1979617178440094\n",
      "Training batch loss 92980 0.20617517828941345\n",
      "Training batch loss 93000 0.2074558138847351\n",
      "Training batch loss 93020 0.19798772037029266\n",
      "Training batch loss 93040 0.20131948590278625\n",
      "Training batch loss 93060 0.21275150775909424\n",
      "Training batch loss 93080 0.19735084474086761\n",
      "Training batch loss 93100 0.2057875394821167\n",
      "Training batch loss 93120 0.2038630247116089\n",
      "Training batch loss 93140 0.1960001289844513\n",
      "Training batch loss 93160 0.19405975937843323\n",
      "Training batch loss 93180 0.20012935996055603\n",
      "Training batch loss 93200 0.20398616790771484\n",
      "Training batch loss 93220 0.19369086623191833\n",
      "Training batch loss 93240 0.20635294914245605\n",
      "Training batch loss 93260 0.1984732449054718\n",
      "Training batch loss 93280 0.20124468207359314\n",
      "Training batch loss 93300 0.18851247429847717\n",
      "Training batch loss 93320 0.19849780201911926\n",
      "Training batch loss 93340 0.20473094284534454\n",
      "Training batch loss 93360 0.19965031743049622\n",
      "Training batch loss 93380 0.20437705516815186\n",
      "Training batch loss 93400 0.19127696752548218\n",
      "Training batch loss 93420 0.1965131014585495\n",
      "Training batch loss 93440 0.195701465010643\n",
      "Training batch loss 93460 0.19478952884674072\n",
      "Training batch loss 93480 0.201440691947937\n",
      "Training epoch loss 68 0.19857197999954224\n",
      "Test loss during training 68 0.1976657658815384\n",
      "Test accuracy during training 68 0.0\n",
      "Training batch loss 93495 0.19170010089874268\n",
      "Training batch loss 93515 0.19589971005916595\n",
      "Training batch loss 93535 0.20131264626979828\n",
      "Training batch loss 93555 0.1983058601617813\n",
      "Training batch loss 93575 0.1991705298423767\n",
      "Training batch loss 93595 0.2009793221950531\n",
      "Training batch loss 93615 0.1980592906475067\n",
      "Training batch loss 93635 0.19715023040771484\n",
      "Training batch loss 93655 0.19260242581367493\n",
      "Training batch loss 93675 0.19880345463752747\n",
      "Training batch loss 93695 0.19632495939731598\n",
      "Training batch loss 93715 0.19872212409973145\n",
      "Training batch loss 93735 0.2011108696460724\n",
      "Training batch loss 93755 0.19334366917610168\n",
      "Training batch loss 93775 0.2011612057685852\n",
      "Training batch loss 93795 0.18688777089118958\n",
      "Training batch loss 93815 0.19738690555095673\n",
      "Training batch loss 93835 0.20211857557296753\n",
      "Training batch loss 93855 0.19543080031871796\n",
      "Training batch loss 93875 0.20585793256759644\n",
      "Training batch loss 93895 0.19723135232925415\n",
      "Training batch loss 93915 0.19044041633605957\n",
      "Training batch loss 93935 0.20283818244934082\n",
      "Training batch loss 93955 0.19655489921569824\n",
      "Training batch loss 93975 0.1955663561820984\n",
      "Training batch loss 93995 0.20146483182907104\n",
      "Training batch loss 94015 0.1988999843597412\n",
      "Training batch loss 94035 0.19841943681240082\n",
      "Training batch loss 94055 0.20389148592948914\n",
      "Training batch loss 94075 0.19226862490177155\n",
      "Training batch loss 94095 0.2035389244556427\n",
      "Training batch loss 94115 0.19257719814777374\n",
      "Training batch loss 94135 0.1931302547454834\n",
      "Training batch loss 94155 0.19947470724582672\n",
      "Training batch loss 94175 0.19628626108169556\n",
      "Training batch loss 94195 0.20586371421813965\n",
      "Training batch loss 94215 0.1968308389186859\n",
      "Training batch loss 94235 0.20596987009048462\n",
      "Training batch loss 94255 0.1997758150100708\n",
      "Training batch loss 94275 0.20147401094436646\n",
      "Training batch loss 94295 0.19950464367866516\n",
      "Training batch loss 94315 0.1979617178440094\n",
      "Training batch loss 94335 0.20617517828941345\n",
      "Training batch loss 94355 0.2074558138847351\n",
      "Training batch loss 94375 0.19798772037029266\n",
      "Training batch loss 94395 0.20131948590278625\n",
      "Training batch loss 94415 0.21275150775909424\n",
      "Training batch loss 94435 0.19735084474086761\n",
      "Training batch loss 94455 0.2057875394821167\n",
      "Training batch loss 94475 0.2038630247116089\n",
      "Training batch loss 94495 0.1960001289844513\n",
      "Training batch loss 94515 0.19405975937843323\n",
      "Training batch loss 94535 0.20012935996055603\n",
      "Training batch loss 94555 0.20398616790771484\n",
      "Training batch loss 94575 0.19369086623191833\n",
      "Training batch loss 94595 0.20635294914245605\n",
      "Training batch loss 94615 0.1984732449054718\n",
      "Training batch loss 94635 0.20124468207359314\n",
      "Training batch loss 94655 0.18851247429847717\n",
      "Training batch loss 94675 0.19849780201911926\n",
      "Training batch loss 94695 0.20473094284534454\n",
      "Training batch loss 94715 0.19965031743049622\n",
      "Training batch loss 94735 0.20437705516815186\n",
      "Training batch loss 94755 0.19127696752548218\n",
      "Training batch loss 94775 0.1965131014585495\n",
      "Training batch loss 94795 0.195701465010643\n",
      "Training batch loss 94815 0.19478952884674072\n",
      "Training batch loss 94835 0.201440691947937\n",
      "Training epoch loss 69 0.19857197999954224\n",
      "Test loss during training 69 0.1976657658815384\n",
      "Test accuracy during training 69 0.0\n",
      "Training batch loss 94850 0.19170010089874268\n",
      "Training batch loss 94870 0.19589971005916595\n",
      "Training batch loss 94890 0.20131264626979828\n",
      "Training batch loss 94910 0.1983058601617813\n",
      "Training batch loss 94930 0.1991705298423767\n",
      "Training batch loss 94950 0.2009793221950531\n",
      "Training batch loss 94970 0.1980592906475067\n",
      "Training batch loss 94990 0.19715023040771484\n",
      "Training batch loss 95010 0.19260242581367493\n",
      "Training batch loss 95030 0.19880345463752747\n",
      "Training batch loss 95050 0.19632495939731598\n",
      "Training batch loss 95070 0.19872212409973145\n",
      "Training batch loss 95090 0.2011108696460724\n",
      "Training batch loss 95110 0.19334366917610168\n",
      "Training batch loss 95130 0.2011612057685852\n",
      "Training batch loss 95150 0.18688777089118958\n",
      "Training batch loss 95170 0.19738690555095673\n",
      "Training batch loss 95190 0.20211857557296753\n",
      "Training batch loss 95210 0.19543080031871796\n",
      "Training batch loss 95230 0.20585793256759644\n",
      "Training batch loss 95250 0.19723135232925415\n",
      "Training batch loss 95270 0.19044041633605957\n",
      "Training batch loss 95290 0.20283818244934082\n",
      "Training batch loss 95310 0.19655489921569824\n",
      "Training batch loss 95330 0.1955663561820984\n",
      "Training batch loss 95350 0.20146483182907104\n",
      "Training batch loss 95370 0.1988999843597412\n",
      "Training batch loss 95390 0.19841943681240082\n",
      "Training batch loss 95410 0.20389148592948914\n",
      "Training batch loss 95430 0.19226862490177155\n",
      "Training batch loss 95450 0.2035389244556427\n",
      "Training batch loss 95470 0.19257719814777374\n",
      "Training batch loss 95490 0.1931302547454834\n",
      "Training batch loss 95510 0.19947470724582672\n",
      "Training batch loss 95530 0.19628626108169556\n",
      "Training batch loss 95550 0.20586371421813965\n",
      "Training batch loss 95570 0.1968308389186859\n",
      "Training batch loss 95590 0.20596987009048462\n",
      "Training batch loss 95610 0.1997758150100708\n",
      "Training batch loss 95630 0.20147401094436646\n",
      "Training batch loss 95650 0.19950464367866516\n",
      "Training batch loss 95670 0.1979617178440094\n",
      "Training batch loss 95690 0.20617517828941345\n",
      "Training batch loss 95710 0.2074558138847351\n",
      "Training batch loss 95730 0.19798772037029266\n",
      "Training batch loss 95750 0.20131948590278625\n",
      "Training batch loss 95770 0.21275150775909424\n",
      "Training batch loss 95790 0.19735084474086761\n",
      "Training batch loss 95810 0.2057875394821167\n",
      "Training batch loss 95830 0.2038630247116089\n",
      "Training batch loss 95850 0.1960001289844513\n",
      "Training batch loss 95870 0.19405975937843323\n",
      "Training batch loss 95890 0.20012935996055603\n",
      "Training batch loss 95910 0.20398616790771484\n",
      "Training batch loss 95930 0.19369086623191833\n",
      "Training batch loss 95950 0.20635294914245605\n",
      "Training batch loss 95970 0.1984732449054718\n",
      "Training batch loss 95990 0.20124468207359314\n",
      "Training batch loss 96010 0.18851247429847717\n",
      "Training batch loss 96030 0.19849780201911926\n",
      "Training batch loss 96050 0.20473094284534454\n",
      "Training batch loss 96070 0.19965031743049622\n",
      "Training batch loss 96090 0.20437705516815186\n",
      "Training batch loss 96110 0.19127696752548218\n",
      "Training batch loss 96130 0.1965131014585495\n",
      "Training batch loss 96150 0.195701465010643\n",
      "Training batch loss 96170 0.19478952884674072\n",
      "Training batch loss 96190 0.201440691947937\n",
      "Training epoch loss 70 0.19857197999954224\n",
      "Test loss during training 70 0.1976657658815384\n",
      "Test accuracy during training 70 0.0\n",
      "Training batch loss 96205 0.19170010089874268\n",
      "Training batch loss 96225 0.19589971005916595\n",
      "Training batch loss 96245 0.20131264626979828\n",
      "Training batch loss 96265 0.1983058601617813\n",
      "Training batch loss 96285 0.1991705298423767\n",
      "Training batch loss 96305 0.2009793221950531\n",
      "Training batch loss 96325 0.1980592906475067\n",
      "Training batch loss 96345 0.19715023040771484\n",
      "Training batch loss 96365 0.19260242581367493\n",
      "Training batch loss 96385 0.19880345463752747\n",
      "Training batch loss 96405 0.19632495939731598\n",
      "Training batch loss 96425 0.19872212409973145\n",
      "Training batch loss 96445 0.2011108696460724\n",
      "Training batch loss 96465 0.19334366917610168\n",
      "Training batch loss 96485 0.2011612057685852\n",
      "Training batch loss 96505 0.18688777089118958\n",
      "Training batch loss 96525 0.19738690555095673\n",
      "Training batch loss 96545 0.20211857557296753\n",
      "Training batch loss 96565 0.19543080031871796\n",
      "Training batch loss 96585 0.20585793256759644\n",
      "Training batch loss 96605 0.19723135232925415\n",
      "Training batch loss 96625 0.19044041633605957\n",
      "Training batch loss 96645 0.20283818244934082\n",
      "Training batch loss 96665 0.19655489921569824\n",
      "Training batch loss 96685 0.1955663561820984\n",
      "Training batch loss 96705 0.20146483182907104\n",
      "Training batch loss 96725 0.1988999843597412\n",
      "Training batch loss 96745 0.19841943681240082\n",
      "Training batch loss 96765 0.20389148592948914\n",
      "Training batch loss 96785 0.19226862490177155\n",
      "Training batch loss 96805 0.2035389244556427\n",
      "Training batch loss 96825 0.19257719814777374\n",
      "Training batch loss 96845 0.1931302547454834\n",
      "Training batch loss 96865 0.19947470724582672\n",
      "Training batch loss 96885 0.19628626108169556\n",
      "Training batch loss 96905 0.20586371421813965\n",
      "Training batch loss 96925 0.1968308389186859\n",
      "Training batch loss 96945 0.20596987009048462\n",
      "Training batch loss 96965 0.1997758150100708\n",
      "Training batch loss 96985 0.20147401094436646\n",
      "Training batch loss 97005 0.19950464367866516\n",
      "Training batch loss 97025 0.1979617178440094\n",
      "Training batch loss 97045 0.20617517828941345\n",
      "Training batch loss 97065 0.2074558138847351\n",
      "Training batch loss 97085 0.19798772037029266\n",
      "Training batch loss 97105 0.20131948590278625\n",
      "Training batch loss 97125 0.21275150775909424\n",
      "Training batch loss 97145 0.19735084474086761\n",
      "Training batch loss 97165 0.2057875394821167\n",
      "Training batch loss 97185 0.2038630247116089\n",
      "Training batch loss 97205 0.1960001289844513\n",
      "Training batch loss 97225 0.19405975937843323\n",
      "Training batch loss 97245 0.20012935996055603\n",
      "Training batch loss 97265 0.20398616790771484\n",
      "Training batch loss 97285 0.19369086623191833\n",
      "Training batch loss 97305 0.20635294914245605\n",
      "Training batch loss 97325 0.1984732449054718\n",
      "Training batch loss 97345 0.20124468207359314\n",
      "Training batch loss 97365 0.18851247429847717\n",
      "Training batch loss 97385 0.19849780201911926\n",
      "Training batch loss 97405 0.20473094284534454\n",
      "Training batch loss 97425 0.19965031743049622\n",
      "Training batch loss 97445 0.20437705516815186\n",
      "Training batch loss 97465 0.19127696752548218\n",
      "Training batch loss 97485 0.1965131014585495\n",
      "Training batch loss 97505 0.195701465010643\n",
      "Training batch loss 97525 0.19478952884674072\n",
      "Training batch loss 97545 0.201440691947937\n",
      "Training epoch loss 71 0.19857197999954224\n",
      "Test loss during training 71 0.1976657658815384\n",
      "Test accuracy during training 71 0.0\n",
      "Training batch loss 97560 0.19170010089874268\n",
      "Training batch loss 97580 0.19589971005916595\n",
      "Training batch loss 97600 0.20131264626979828\n",
      "Training batch loss 97620 0.1983058601617813\n",
      "Training batch loss 97640 0.1991705298423767\n",
      "Training batch loss 97660 0.2009793221950531\n",
      "Training batch loss 97680 0.1980592906475067\n",
      "Training batch loss 97700 0.19715023040771484\n",
      "Training batch loss 97720 0.19260242581367493\n",
      "Training batch loss 97740 0.19880345463752747\n",
      "Training batch loss 97760 0.19632495939731598\n",
      "Training batch loss 97780 0.19872212409973145\n",
      "Training batch loss 97800 0.2011108696460724\n",
      "Training batch loss 97820 0.19334366917610168\n",
      "Training batch loss 97840 0.2011612057685852\n",
      "Training batch loss 97860 0.18688777089118958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss 97880 0.19738690555095673\n",
      "Training batch loss 97900 0.20211857557296753\n",
      "Training batch loss 97920 0.19543080031871796\n",
      "Training batch loss 97940 0.20585793256759644\n",
      "Training batch loss 97960 0.19723135232925415\n",
      "Training batch loss 97980 0.19044041633605957\n",
      "Training batch loss 98000 0.20283818244934082\n",
      "Training batch loss 98020 0.19655489921569824\n",
      "Training batch loss 98040 0.1955663561820984\n",
      "Training batch loss 98060 0.20146483182907104\n",
      "Training batch loss 98080 0.1988999843597412\n",
      "Training batch loss 98100 0.19841943681240082\n",
      "Training batch loss 98120 0.20389148592948914\n",
      "Training batch loss 98140 0.19226862490177155\n",
      "Training batch loss 98160 0.2035389244556427\n",
      "Training batch loss 98180 0.19257719814777374\n",
      "Training batch loss 98200 0.1931302547454834\n",
      "Training batch loss 98220 0.19947470724582672\n",
      "Training batch loss 98240 0.19628626108169556\n",
      "Training batch loss 98260 0.20586371421813965\n",
      "Training batch loss 98280 0.1968308389186859\n",
      "Training batch loss 98300 0.20596987009048462\n",
      "Training batch loss 98320 0.1997758150100708\n",
      "Training batch loss 98340 0.20147401094436646\n",
      "Training batch loss 98360 0.19950464367866516\n",
      "Training batch loss 98380 0.1979617178440094\n",
      "Training batch loss 98400 0.20617517828941345\n",
      "Training batch loss 98420 0.2074558138847351\n",
      "Training batch loss 98440 0.19798772037029266\n",
      "Training batch loss 98460 0.20131948590278625\n",
      "Training batch loss 98480 0.21275150775909424\n",
      "Training batch loss 98500 0.19735084474086761\n",
      "Training batch loss 98520 0.2057875394821167\n",
      "Training batch loss 98540 0.2038630247116089\n",
      "Training batch loss 98560 0.1960001289844513\n",
      "Training batch loss 98580 0.19405975937843323\n",
      "Training batch loss 98600 0.20012935996055603\n",
      "Training batch loss 98620 0.20398616790771484\n",
      "Training batch loss 98640 0.19369086623191833\n",
      "Training batch loss 98660 0.20635294914245605\n",
      "Training batch loss 98680 0.1984732449054718\n",
      "Training batch loss 98700 0.20124468207359314\n",
      "Training batch loss 98720 0.18851247429847717\n",
      "Training batch loss 98740 0.19849780201911926\n",
      "Training batch loss 98760 0.20473094284534454\n",
      "Training batch loss 98780 0.19965031743049622\n",
      "Training batch loss 98800 0.20437705516815186\n",
      "Training batch loss 98820 0.19127696752548218\n",
      "Training batch loss 98840 0.1965131014585495\n",
      "Training batch loss 98860 0.195701465010643\n",
      "Training batch loss 98880 0.19478952884674072\n",
      "Training batch loss 98900 0.201440691947937\n",
      "Training epoch loss 72 0.19857197999954224\n",
      "Test loss during training 72 0.1976657658815384\n",
      "Test accuracy during training 72 0.0\n",
      "Training batch loss 98915 0.19170010089874268\n",
      "Training batch loss 98935 0.19589971005916595\n",
      "Training batch loss 98955 0.20131264626979828\n",
      "Training batch loss 98975 0.1983058601617813\n",
      "Training batch loss 98995 0.1991705298423767\n",
      "Training batch loss 99015 0.2009793221950531\n",
      "Training batch loss 99035 0.1980592906475067\n",
      "Training batch loss 99055 0.19715023040771484\n",
      "Training batch loss 99075 0.19260242581367493\n",
      "Training batch loss 99095 0.19880345463752747\n",
      "Training batch loss 99115 0.19632495939731598\n",
      "Training batch loss 99135 0.19872212409973145\n",
      "Training batch loss 99155 0.2011108696460724\n",
      "Training batch loss 99175 0.19334366917610168\n",
      "Training batch loss 99195 0.2011612057685852\n",
      "Training batch loss 99215 0.18688777089118958\n",
      "Training batch loss 99235 0.19738690555095673\n",
      "Training batch loss 99255 0.20211857557296753\n",
      "Training batch loss 99275 0.19543080031871796\n",
      "Training batch loss 99295 0.20585793256759644\n",
      "Training batch loss 99315 0.19723135232925415\n",
      "Training batch loss 99335 0.19044041633605957\n",
      "Training batch loss 99355 0.20283818244934082\n",
      "Training batch loss 99375 0.19655489921569824\n",
      "Training batch loss 99395 0.1955663561820984\n",
      "Training batch loss 99415 0.20146483182907104\n",
      "Training batch loss 99435 0.1988999843597412\n",
      "Training batch loss 99455 0.19841943681240082\n",
      "Training batch loss 99475 0.20389148592948914\n",
      "Training batch loss 99495 0.19226862490177155\n",
      "Training batch loss 99515 0.2035389244556427\n",
      "Training batch loss 99535 0.19257719814777374\n",
      "Training batch loss 99555 0.1931302547454834\n",
      "Training batch loss 99575 0.19947470724582672\n",
      "Training batch loss 99595 0.19628626108169556\n",
      "Training batch loss 99615 0.20586371421813965\n",
      "Training batch loss 99635 0.1968308389186859\n",
      "Training batch loss 99655 0.20596987009048462\n",
      "Training batch loss 99675 0.1997758150100708\n",
      "Training batch loss 99695 0.20147401094436646\n",
      "Training batch loss 99715 0.19950464367866516\n",
      "Training batch loss 99735 0.1979617178440094\n",
      "Training batch loss 99755 0.20617517828941345\n",
      "Training batch loss 99775 0.2074558138847351\n",
      "Training batch loss 99795 0.19798772037029266\n",
      "Training batch loss 99815 0.20131948590278625\n",
      "Training batch loss 99835 0.21275150775909424\n",
      "Training batch loss 99855 0.19735084474086761\n",
      "Training batch loss 99875 0.2057875394821167\n",
      "Training batch loss 99895 0.2038630247116089\n",
      "Training batch loss 99915 0.1960001289844513\n",
      "Training batch loss 99935 0.19405975937843323\n",
      "Training batch loss 99955 0.20012935996055603\n",
      "Training batch loss 99975 0.20398616790771484\n",
      "Training batch loss 99995 0.19369086623191833\n",
      "Training batch loss 100015 0.20635294914245605\n",
      "Training batch loss 100035 0.1984732449054718\n",
      "Training batch loss 100055 0.20124468207359314\n",
      "Training batch loss 100075 0.18851247429847717\n",
      "Training batch loss 100095 0.19849780201911926\n",
      "Training batch loss 100115 0.20473094284534454\n",
      "Training batch loss 100135 0.19965031743049622\n",
      "Training batch loss 100155 0.20437705516815186\n",
      "Training batch loss 100175 0.19127696752548218\n",
      "Training batch loss 100195 0.1965131014585495\n",
      "Training batch loss 100215 0.195701465010643\n",
      "Training batch loss 100235 0.19478952884674072\n",
      "Training batch loss 100255 0.201440691947937\n",
      "Training epoch loss 73 0.19857197999954224\n",
      "Test loss during training 73 0.1976657658815384\n",
      "Test accuracy during training 73 0.0\n",
      "Training batch loss 100270 0.19170010089874268\n",
      "Training batch loss 100290 0.19589971005916595\n",
      "Training batch loss 100310 0.20131264626979828\n",
      "Training batch loss 100330 0.1983058601617813\n",
      "Training batch loss 100350 0.1991705298423767\n",
      "Training batch loss 100370 0.2009793221950531\n",
      "Training batch loss 100390 0.1980592906475067\n",
      "Training batch loss 100410 0.19715023040771484\n",
      "Training batch loss 100430 0.19260242581367493\n",
      "Training batch loss 100450 0.19880345463752747\n",
      "Training batch loss 100470 0.19632495939731598\n",
      "Training batch loss 100490 0.19872212409973145\n",
      "Training batch loss 100510 0.2011108696460724\n",
      "Training batch loss 100530 0.19334366917610168\n",
      "Training batch loss 100550 0.2011612057685852\n",
      "Training batch loss 100570 0.18688777089118958\n",
      "Training batch loss 100590 0.19738690555095673\n",
      "Training batch loss 100610 0.20211857557296753\n",
      "Training batch loss 100630 0.19543080031871796\n",
      "Training batch loss 100650 0.20585793256759644\n",
      "Training batch loss 100670 0.19723135232925415\n",
      "Training batch loss 100690 0.19044041633605957\n",
      "Training batch loss 100710 0.20283818244934082\n",
      "Training batch loss 100730 0.19655489921569824\n",
      "Training batch loss 100750 0.1955663561820984\n",
      "Training batch loss 100770 0.20146483182907104\n",
      "Training batch loss 100790 0.1988999843597412\n",
      "Training batch loss 100810 0.19841943681240082\n",
      "Training batch loss 100830 0.20389148592948914\n",
      "Training batch loss 100850 0.19226862490177155\n",
      "Training batch loss 100870 0.2035389244556427\n",
      "Training batch loss 100890 0.19257719814777374\n",
      "Training batch loss 100910 0.1931302547454834\n",
      "Training batch loss 100930 0.19947470724582672\n",
      "Training batch loss 100950 0.19628626108169556\n",
      "Training batch loss 100970 0.20586371421813965\n",
      "Training batch loss 100990 0.1968308389186859\n",
      "Training batch loss 101010 0.20596987009048462\n",
      "Training batch loss 101030 0.1997758150100708\n",
      "Training batch loss 101050 0.20147401094436646\n",
      "Training batch loss 101070 0.19950464367866516\n",
      "Training batch loss 101090 0.1979617178440094\n",
      "Training batch loss 101110 0.20617517828941345\n",
      "Training batch loss 101130 0.2074558138847351\n",
      "Training batch loss 101150 0.19798772037029266\n",
      "Training batch loss 101170 0.20131948590278625\n",
      "Training batch loss 101190 0.21275150775909424\n",
      "Training batch loss 101210 0.19735084474086761\n",
      "Training batch loss 101230 0.2057875394821167\n",
      "Training batch loss 101250 0.2038630247116089\n",
      "Training batch loss 101270 0.1960001289844513\n",
      "Training batch loss 101290 0.19405975937843323\n",
      "Training batch loss 101310 0.20012935996055603\n",
      "Training batch loss 101330 0.20398616790771484\n",
      "Training batch loss 101350 0.19369086623191833\n",
      "Training batch loss 101370 0.20635294914245605\n",
      "Training batch loss 101390 0.1984732449054718\n",
      "Training batch loss 101410 0.20124468207359314\n",
      "Training batch loss 101430 0.18851247429847717\n",
      "Training batch loss 101450 0.19849780201911926\n",
      "Training batch loss 101470 0.20473094284534454\n",
      "Training batch loss 101490 0.19965031743049622\n",
      "Training batch loss 101510 0.20437705516815186\n",
      "Training batch loss 101530 0.19127696752548218\n",
      "Training batch loss 101550 0.1965131014585495\n",
      "Training batch loss 101570 0.195701465010643\n",
      "Training batch loss 101590 0.19478952884674072\n",
      "Training batch loss 101610 0.201440691947937\n",
      "Training epoch loss 74 0.19857197999954224\n",
      "Test loss during training 74 0.1976657658815384\n",
      "Test accuracy during training 74 0.0\n",
      "Training batch loss 101625 0.19170010089874268\n",
      "Training batch loss 101645 0.19589971005916595\n",
      "Training batch loss 101665 0.20131264626979828\n",
      "Training batch loss 101685 0.1983058601617813\n",
      "Training batch loss 101705 0.1991705298423767\n",
      "Training batch loss 101725 0.2009793221950531\n",
      "Training batch loss 101745 0.1980592906475067\n",
      "Training batch loss 101765 0.19715023040771484\n",
      "Training batch loss 101785 0.19260242581367493\n",
      "Training batch loss 101805 0.19880345463752747\n",
      "Training batch loss 101825 0.19632495939731598\n",
      "Training batch loss 101845 0.19872212409973145\n",
      "Training batch loss 101865 0.2011108696460724\n",
      "Training batch loss 101885 0.19334366917610168\n",
      "Training batch loss 101905 0.2011612057685852\n",
      "Training batch loss 101925 0.18688777089118958\n",
      "Training batch loss 101945 0.19738690555095673\n",
      "Training batch loss 101965 0.20211857557296753\n",
      "Training batch loss 101985 0.19543080031871796\n",
      "Training batch loss 102005 0.20585793256759644\n",
      "Training batch loss 102025 0.19723135232925415\n",
      "Training batch loss 102045 0.19044041633605957\n",
      "Training batch loss 102065 0.20283818244934082\n",
      "Training batch loss 102085 0.19655489921569824\n",
      "Training batch loss 102105 0.1955663561820984\n",
      "Training batch loss 102125 0.20146483182907104\n",
      "Training batch loss 102145 0.1988999843597412\n",
      "Training batch loss 102165 0.19841943681240082\n",
      "Training batch loss 102185 0.20389148592948914\n",
      "Training batch loss 102205 0.19226862490177155\n",
      "Training batch loss 102225 0.2035389244556427\n",
      "Training batch loss 102245 0.19257719814777374\n",
      "Training batch loss 102265 0.1931302547454834\n",
      "Training batch loss 102285 0.19947470724582672\n",
      "Training batch loss 102305 0.19628626108169556\n",
      "Training batch loss 102325 0.20586371421813965\n",
      "Training batch loss 102345 0.1968308389186859\n",
      "Training batch loss 102365 0.20596987009048462\n",
      "Training batch loss 102385 0.1997758150100708\n",
      "Training batch loss 102405 0.20147401094436646\n",
      "Training batch loss 102425 0.19950464367866516\n",
      "Training batch loss 102445 0.1979617178440094\n",
      "Training batch loss 102465 0.20617517828941345\n",
      "Training batch loss 102485 0.2074558138847351\n",
      "Training batch loss 102505 0.19798772037029266\n",
      "Training batch loss 102525 0.20131948590278625\n",
      "Training batch loss 102545 0.21275150775909424\n",
      "Training batch loss 102565 0.19735084474086761\n",
      "Training batch loss 102585 0.2057875394821167\n",
      "Training batch loss 102605 0.2038630247116089\n",
      "Training batch loss 102625 0.1960001289844513\n",
      "Training batch loss 102645 0.19405975937843323\n",
      "Training batch loss 102665 0.20012935996055603\n",
      "Training batch loss 102685 0.20398616790771484\n",
      "Training batch loss 102705 0.19369086623191833\n",
      "Training batch loss 102725 0.20635294914245605\n",
      "Training batch loss 102745 0.1984732449054718\n",
      "Training batch loss 102765 0.20124468207359314\n",
      "Training batch loss 102785 0.18851247429847717\n",
      "Training batch loss 102805 0.19849780201911926\n",
      "Training batch loss 102825 0.20473094284534454\n",
      "Training batch loss 102845 0.19965031743049622\n",
      "Training batch loss 102865 0.20437705516815186\n",
      "Training batch loss 102885 0.19127696752548218\n",
      "Training batch loss 102905 0.1965131014585495\n",
      "Training batch loss 102925 0.195701465010643\n",
      "Training batch loss 102945 0.19478952884674072\n",
      "Training batch loss 102965 0.201440691947937\n",
      "Training epoch loss 75 0.19857197999954224\n",
      "Test loss during training 75 0.1976657658815384\n",
      "Test accuracy during training 75 0.0\n",
      "Training batch loss 102980 0.19170010089874268\n",
      "Training batch loss 103000 0.19589971005916595\n",
      "Training batch loss 103020 0.20131264626979828\n",
      "Training batch loss 103040 0.1983058601617813\n",
      "Training batch loss 103060 0.1991705298423767\n",
      "Training batch loss 103080 0.2009793221950531\n",
      "Training batch loss 103100 0.1980592906475067\n",
      "Training batch loss 103120 0.19715023040771484\n",
      "Training batch loss 103140 0.19260242581367493\n",
      "Training batch loss 103160 0.19880345463752747\n",
      "Training batch loss 103180 0.19632495939731598\n",
      "Training batch loss 103200 0.19872212409973145\n",
      "Training batch loss 103220 0.2011108696460724\n",
      "Training batch loss 103240 0.19334366917610168\n",
      "Training batch loss 103260 0.2011612057685852\n",
      "Training batch loss 103280 0.18688777089118958\n",
      "Training batch loss 103300 0.19738690555095673\n",
      "Training batch loss 103320 0.20211857557296753\n",
      "Training batch loss 103340 0.19543080031871796\n",
      "Training batch loss 103360 0.20585793256759644\n",
      "Training batch loss 103380 0.19723135232925415\n",
      "Training batch loss 103400 0.19044041633605957\n",
      "Training batch loss 103420 0.20283818244934082\n",
      "Training batch loss 103440 0.19655489921569824\n",
      "Training batch loss 103460 0.1955663561820984\n",
      "Training batch loss 103480 0.20146483182907104\n",
      "Training batch loss 103500 0.1988999843597412\n",
      "Training batch loss 103520 0.19841943681240082\n",
      "Training batch loss 103540 0.20389148592948914\n",
      "Training batch loss 103560 0.19226862490177155\n",
      "Training batch loss 103580 0.2035389244556427\n",
      "Training batch loss 103600 0.19257719814777374\n",
      "Training batch loss 103620 0.1931302547454834\n",
      "Training batch loss 103640 0.19947470724582672\n",
      "Training batch loss 103660 0.19628626108169556\n",
      "Training batch loss 103680 0.20586371421813965\n",
      "Training batch loss 103700 0.1968308389186859\n",
      "Training batch loss 103720 0.20596987009048462\n",
      "Training batch loss 103740 0.1997758150100708\n",
      "Training batch loss 103760 0.20147401094436646\n",
      "Training batch loss 103780 0.19950464367866516\n",
      "Training batch loss 103800 0.1979617178440094\n",
      "Training batch loss 103820 0.20617517828941345\n",
      "Training batch loss 103840 0.2074558138847351\n",
      "Training batch loss 103860 0.19798772037029266\n",
      "Training batch loss 103880 0.20131948590278625\n",
      "Training batch loss 103900 0.21275150775909424\n",
      "Training batch loss 103920 0.19735084474086761\n",
      "Training batch loss 103940 0.2057875394821167\n",
      "Training batch loss 103960 0.2038630247116089\n",
      "Training batch loss 103980 0.1960001289844513\n",
      "Training batch loss 104000 0.19405975937843323\n",
      "Training batch loss 104020 0.20012935996055603\n",
      "Training batch loss 104040 0.20398616790771484\n",
      "Training batch loss 104060 0.19369086623191833\n",
      "Training batch loss 104080 0.20635294914245605\n",
      "Training batch loss 104100 0.1984732449054718\n",
      "Training batch loss 104120 0.20124468207359314\n",
      "Training batch loss 104140 0.18851247429847717\n",
      "Training batch loss 104160 0.19849780201911926\n",
      "Training batch loss 104180 0.20473094284534454\n",
      "Training batch loss 104200 0.19965031743049622\n",
      "Training batch loss 104220 0.20437705516815186\n",
      "Training batch loss 104240 0.19127696752548218\n",
      "Training batch loss 104260 0.1965131014585495\n",
      "Training batch loss 104280 0.195701465010643\n",
      "Training batch loss 104300 0.19478952884674072\n",
      "Training batch loss 104320 0.201440691947937\n",
      "Training epoch loss 76 0.19857197999954224\n",
      "Test loss during training 76 0.1976657658815384\n",
      "Test accuracy during training 76 0.0\n",
      "Training batch loss 104335 0.19170010089874268\n",
      "Training batch loss 104355 0.19589971005916595\n",
      "Training batch loss 104375 0.20131264626979828\n",
      "Training batch loss 104395 0.1983058601617813\n",
      "Training batch loss 104415 0.1991705298423767\n",
      "Training batch loss 104435 0.2009793221950531\n",
      "Training batch loss 104455 0.1980592906475067\n",
      "Training batch loss 104475 0.19715023040771484\n",
      "Training batch loss 104495 0.19260242581367493\n",
      "Training batch loss 104515 0.19880345463752747\n",
      "Training batch loss 104535 0.19632495939731598\n",
      "Training batch loss 104555 0.19872212409973145\n",
      "Training batch loss 104575 0.2011108696460724\n",
      "Training batch loss 104595 0.19334366917610168\n",
      "Training batch loss 104615 0.2011612057685852\n",
      "Training batch loss 104635 0.18688777089118958\n",
      "Training batch loss 104655 0.19738690555095673\n",
      "Training batch loss 104675 0.20211857557296753\n",
      "Training batch loss 104695 0.19543080031871796\n",
      "Training batch loss 104715 0.20585793256759644\n",
      "Training batch loss 104735 0.19723135232925415\n",
      "Training batch loss 104755 0.19044041633605957\n",
      "Training batch loss 104775 0.20283818244934082\n",
      "Training batch loss 104795 0.19655489921569824\n",
      "Training batch loss 104815 0.1955663561820984\n",
      "Training batch loss 104835 0.20146483182907104\n",
      "Training batch loss 104855 0.1988999843597412\n",
      "Training batch loss 104875 0.19841943681240082\n",
      "Training batch loss 104895 0.20389148592948914\n",
      "Training batch loss 104915 0.19226862490177155\n",
      "Training batch loss 104935 0.2035389244556427\n",
      "Training batch loss 104955 0.19257719814777374\n",
      "Training batch loss 104975 0.1931302547454834\n",
      "Training batch loss 104995 0.19947470724582672\n",
      "Training batch loss 105015 0.19628626108169556\n",
      "Training batch loss 105035 0.20586371421813965\n",
      "Training batch loss 105055 0.1968308389186859\n",
      "Training batch loss 105075 0.20596987009048462\n",
      "Training batch loss 105095 0.1997758150100708\n",
      "Training batch loss 105115 0.20147401094436646\n",
      "Training batch loss 105135 0.19950464367866516\n",
      "Training batch loss 105155 0.1979617178440094\n",
      "Training batch loss 105175 0.20617517828941345\n",
      "Training batch loss 105195 0.2074558138847351\n",
      "Training batch loss 105215 0.19798772037029266\n",
      "Training batch loss 105235 0.20131948590278625\n",
      "Training batch loss 105255 0.21275150775909424\n",
      "Training batch loss 105275 0.19735084474086761\n",
      "Training batch loss 105295 0.2057875394821167\n",
      "Training batch loss 105315 0.2038630247116089\n",
      "Training batch loss 105335 0.1960001289844513\n",
      "Training batch loss 105355 0.19405975937843323\n",
      "Training batch loss 105375 0.20012935996055603\n",
      "Training batch loss 105395 0.20398616790771484\n",
      "Training batch loss 105415 0.19369086623191833\n",
      "Training batch loss 105435 0.20635294914245605\n",
      "Training batch loss 105455 0.1984732449054718\n",
      "Training batch loss 105475 0.20124468207359314\n",
      "Training batch loss 105495 0.18851247429847717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss 105515 0.19849780201911926\n",
      "Training batch loss 105535 0.20473094284534454\n",
      "Training batch loss 105555 0.19965031743049622\n",
      "Training batch loss 105575 0.20437705516815186\n",
      "Training batch loss 105595 0.19127696752548218\n",
      "Training batch loss 105615 0.1965131014585495\n",
      "Training batch loss 105635 0.195701465010643\n",
      "Training batch loss 105655 0.19478952884674072\n",
      "Training batch loss 105675 0.201440691947937\n",
      "Training epoch loss 77 0.19857197999954224\n",
      "Test loss during training 77 0.1976657658815384\n",
      "Test accuracy during training 77 0.0\n",
      "Training batch loss 105690 0.19170010089874268\n",
      "Training batch loss 105710 0.19589971005916595\n",
      "Training batch loss 105730 0.20131264626979828\n",
      "Training batch loss 105750 0.1983058601617813\n",
      "Training batch loss 105770 0.1991705298423767\n",
      "Training batch loss 105790 0.2009793221950531\n",
      "Training batch loss 105810 0.1980592906475067\n",
      "Training batch loss 105830 0.19715023040771484\n",
      "Training batch loss 105850 0.19260242581367493\n",
      "Training batch loss 105870 0.19880345463752747\n",
      "Training batch loss 105890 0.19632495939731598\n",
      "Training batch loss 105910 0.19872212409973145\n",
      "Training batch loss 105930 0.2011108696460724\n",
      "Training batch loss 105950 0.19334366917610168\n",
      "Training batch loss 105970 0.2011612057685852\n",
      "Training batch loss 105990 0.18688777089118958\n",
      "Training batch loss 106010 0.19738690555095673\n",
      "Training batch loss 106030 0.20211857557296753\n",
      "Training batch loss 106050 0.19543080031871796\n",
      "Training batch loss 106070 0.20585793256759644\n",
      "Training batch loss 106090 0.19723135232925415\n",
      "Training batch loss 106110 0.19044041633605957\n",
      "Training batch loss 106130 0.20283818244934082\n",
      "Training batch loss 106150 0.19655489921569824\n",
      "Training batch loss 106170 0.1955663561820984\n",
      "Training batch loss 106190 0.20146483182907104\n",
      "Training batch loss 106210 0.1988999843597412\n",
      "Training batch loss 106230 0.19841943681240082\n",
      "Training batch loss 106250 0.20389148592948914\n",
      "Training batch loss 106270 0.19226862490177155\n",
      "Training batch loss 106290 0.2035389244556427\n",
      "Training batch loss 106310 0.19257719814777374\n",
      "Training batch loss 106330 0.1931302547454834\n",
      "Training batch loss 106350 0.19947470724582672\n",
      "Training batch loss 106370 0.19628626108169556\n",
      "Training batch loss 106390 0.20586371421813965\n",
      "Training batch loss 106410 0.1968308389186859\n",
      "Training batch loss 106430 0.20596987009048462\n",
      "Training batch loss 106450 0.1997758150100708\n",
      "Training batch loss 106470 0.20147401094436646\n",
      "Training batch loss 106490 0.19950464367866516\n",
      "Training batch loss 106510 0.1979617178440094\n",
      "Training batch loss 106530 0.20617517828941345\n",
      "Training batch loss 106550 0.2074558138847351\n",
      "Training batch loss 106570 0.19798772037029266\n",
      "Training batch loss 106590 0.20131948590278625\n",
      "Training batch loss 106610 0.21275150775909424\n",
      "Training batch loss 106630 0.19735084474086761\n",
      "Training batch loss 106650 0.2057875394821167\n",
      "Training batch loss 106670 0.2038630247116089\n",
      "Training batch loss 106690 0.1960001289844513\n",
      "Training batch loss 106710 0.19405975937843323\n",
      "Training batch loss 106730 0.20012935996055603\n",
      "Training batch loss 106750 0.20398616790771484\n",
      "Training batch loss 106770 0.19369086623191833\n",
      "Training batch loss 106790 0.20635294914245605\n",
      "Training batch loss 106810 0.1984732449054718\n",
      "Training batch loss 106830 0.20124468207359314\n",
      "Training batch loss 106850 0.18851247429847717\n",
      "Training batch loss 106870 0.19849780201911926\n",
      "Training batch loss 106890 0.20473094284534454\n",
      "Training batch loss 106910 0.19965031743049622\n",
      "Training batch loss 106930 0.20437705516815186\n",
      "Training batch loss 106950 0.19127696752548218\n",
      "Training batch loss 106970 0.1965131014585495\n",
      "Training batch loss 106990 0.195701465010643\n",
      "Training batch loss 107010 0.19478952884674072\n",
      "Training batch loss 107030 0.201440691947937\n",
      "Training epoch loss 78 0.19857197999954224\n",
      "Test loss during training 78 0.1976657658815384\n",
      "Test accuracy during training 78 0.0\n",
      "Training batch loss 107045 0.19170010089874268\n",
      "Training batch loss 107065 0.19589971005916595\n",
      "Training batch loss 107085 0.20131264626979828\n",
      "Training batch loss 107105 0.1983058601617813\n",
      "Training batch loss 107125 0.1991705298423767\n",
      "Training batch loss 107145 0.2009793221950531\n",
      "Training batch loss 107165 0.1980592906475067\n",
      "Training batch loss 107185 0.19715023040771484\n",
      "Training batch loss 107205 0.19260242581367493\n",
      "Training batch loss 107225 0.19880345463752747\n",
      "Training batch loss 107245 0.19632495939731598\n",
      "Training batch loss 107265 0.19872212409973145\n",
      "Training batch loss 107285 0.2011108696460724\n",
      "Training batch loss 107305 0.19334366917610168\n",
      "Training batch loss 107325 0.2011612057685852\n",
      "Training batch loss 107345 0.18688777089118958\n",
      "Training batch loss 107365 0.19738690555095673\n",
      "Training batch loss 107385 0.20211857557296753\n",
      "Training batch loss 107405 0.19543080031871796\n",
      "Training batch loss 107425 0.20585793256759644\n",
      "Training batch loss 107445 0.19723135232925415\n",
      "Training batch loss 107465 0.19044041633605957\n",
      "Training batch loss 107485 0.20283818244934082\n",
      "Training batch loss 107505 0.19655489921569824\n",
      "Training batch loss 107525 0.1955663561820984\n",
      "Training batch loss 107545 0.20146483182907104\n",
      "Training batch loss 107565 0.1988999843597412\n",
      "Training batch loss 107585 0.19841943681240082\n",
      "Training batch loss 107605 0.20389148592948914\n",
      "Training batch loss 107625 0.19226862490177155\n",
      "Training batch loss 107645 0.2035389244556427\n",
      "Training batch loss 107665 0.19257719814777374\n",
      "Training batch loss 107685 0.1931302547454834\n",
      "Training batch loss 107705 0.19947470724582672\n",
      "Training batch loss 107725 0.19628626108169556\n",
      "Training batch loss 107745 0.20586371421813965\n",
      "Training batch loss 107765 0.1968308389186859\n",
      "Training batch loss 107785 0.20596987009048462\n",
      "Training batch loss 107805 0.1997758150100708\n",
      "Training batch loss 107825 0.20147401094436646\n",
      "Training batch loss 107845 0.19950464367866516\n",
      "Training batch loss 107865 0.1979617178440094\n",
      "Training batch loss 107885 0.20617517828941345\n",
      "Training batch loss 107905 0.2074558138847351\n",
      "Training batch loss 107925 0.19798772037029266\n",
      "Training batch loss 107945 0.20131948590278625\n",
      "Training batch loss 107965 0.21275150775909424\n",
      "Training batch loss 107985 0.19735084474086761\n",
      "Training batch loss 108005 0.2057875394821167\n",
      "Training batch loss 108025 0.2038630247116089\n",
      "Training batch loss 108045 0.1960001289844513\n",
      "Training batch loss 108065 0.19405975937843323\n",
      "Training batch loss 108085 0.20012935996055603\n",
      "Training batch loss 108105 0.20398616790771484\n",
      "Training batch loss 108125 0.19369086623191833\n",
      "Training batch loss 108145 0.20635294914245605\n",
      "Training batch loss 108165 0.1984732449054718\n",
      "Training batch loss 108185 0.20124468207359314\n",
      "Training batch loss 108205 0.18851247429847717\n",
      "Training batch loss 108225 0.19849780201911926\n",
      "Training batch loss 108245 0.20473094284534454\n",
      "Training batch loss 108265 0.19965031743049622\n",
      "Training batch loss 108285 0.20437705516815186\n",
      "Training batch loss 108305 0.19127696752548218\n",
      "Training batch loss 108325 0.1965131014585495\n",
      "Training batch loss 108345 0.195701465010643\n",
      "Training batch loss 108365 0.19478952884674072\n",
      "Training batch loss 108385 0.201440691947937\n",
      "Training epoch loss 79 0.19857197999954224\n",
      "Test loss during training 79 0.1976657658815384\n",
      "Test accuracy during training 79 0.0\n",
      "Training batch loss 108400 0.19170010089874268\n",
      "Training batch loss 108420 0.19589971005916595\n",
      "Training batch loss 108440 0.20131264626979828\n",
      "Training batch loss 108460 0.1983058601617813\n",
      "Training batch loss 108480 0.1991705298423767\n",
      "Training batch loss 108500 0.2009793221950531\n",
      "Training batch loss 108520 0.1980592906475067\n",
      "Training batch loss 108540 0.19715023040771484\n",
      "Training batch loss 108560 0.19260242581367493\n",
      "Training batch loss 108580 0.19880345463752747\n",
      "Training batch loss 108600 0.19632495939731598\n",
      "Training batch loss 108620 0.19872212409973145\n",
      "Training batch loss 108640 0.2011108696460724\n",
      "Training batch loss 108660 0.19334366917610168\n",
      "Training batch loss 108680 0.2011612057685852\n",
      "Training batch loss 108700 0.18688777089118958\n",
      "Training batch loss 108720 0.19738690555095673\n",
      "Training batch loss 108740 0.20211857557296753\n",
      "Training batch loss 108760 0.19543080031871796\n",
      "Training batch loss 108780 0.20585793256759644\n",
      "Training batch loss 108800 0.19723135232925415\n",
      "Training batch loss 108820 0.19044041633605957\n",
      "Training batch loss 108840 0.20283818244934082\n",
      "Training batch loss 108860 0.19655489921569824\n",
      "Training batch loss 108880 0.1955663561820984\n",
      "Training batch loss 108900 0.20146483182907104\n",
      "Training batch loss 108920 0.1988999843597412\n",
      "Training batch loss 108940 0.19841943681240082\n",
      "Training batch loss 108960 0.20389148592948914\n",
      "Training batch loss 108980 0.19226862490177155\n",
      "Training batch loss 109000 0.2035389244556427\n",
      "Training batch loss 109020 0.19257719814777374\n",
      "Training batch loss 109040 0.1931302547454834\n",
      "Training batch loss 109060 0.19947470724582672\n",
      "Training batch loss 109080 0.19628626108169556\n",
      "Training batch loss 109100 0.20586371421813965\n",
      "Training batch loss 109120 0.1968308389186859\n",
      "Training batch loss 109140 0.20596987009048462\n",
      "Training batch loss 109160 0.1997758150100708\n",
      "Training batch loss 109180 0.20147401094436646\n",
      "Training batch loss 109200 0.19950464367866516\n",
      "Training batch loss 109220 0.1979617178440094\n",
      "Training batch loss 109240 0.20617517828941345\n",
      "Training batch loss 109260 0.2074558138847351\n",
      "Training batch loss 109280 0.19798772037029266\n",
      "Training batch loss 109300 0.20131948590278625\n",
      "Training batch loss 109320 0.21275150775909424\n",
      "Training batch loss 109340 0.19735084474086761\n",
      "Training batch loss 109360 0.2057875394821167\n",
      "Training batch loss 109380 0.2038630247116089\n",
      "Training batch loss 109400 0.1960001289844513\n",
      "Training batch loss 109420 0.19405975937843323\n",
      "Training batch loss 109440 0.20012935996055603\n",
      "Training batch loss 109460 0.20398616790771484\n",
      "Training batch loss 109480 0.19369086623191833\n",
      "Training batch loss 109500 0.20635294914245605\n",
      "Training batch loss 109520 0.1984732449054718\n",
      "Training batch loss 109540 0.20124468207359314\n",
      "Training batch loss 109560 0.18851247429847717\n",
      "Training batch loss 109580 0.19849780201911926\n",
      "Training batch loss 109600 0.20473094284534454\n",
      "Training batch loss 109620 0.19965031743049622\n",
      "Training batch loss 109640 0.20437705516815186\n",
      "Training batch loss 109660 0.19127696752548218\n",
      "Training batch loss 109680 0.1965131014585495\n",
      "Training batch loss 109700 0.195701465010643\n",
      "Training batch loss 109720 0.19478952884674072\n",
      "Training batch loss 109740 0.201440691947937\n",
      "Training epoch loss 80 0.19857197999954224\n",
      "Test loss during training 80 0.1976657658815384\n",
      "Test accuracy during training 80 0.0\n",
      "Training batch loss 109755 0.19170010089874268\n",
      "Training batch loss 109775 0.19589971005916595\n",
      "Training batch loss 109795 0.20131264626979828\n",
      "Training batch loss 109815 0.1983058601617813\n",
      "Training batch loss 109835 0.1991705298423767\n",
      "Training batch loss 109855 0.2009793221950531\n",
      "Training batch loss 109875 0.1980592906475067\n",
      "Training batch loss 109895 0.19715023040771484\n",
      "Training batch loss 109915 0.19260242581367493\n",
      "Training batch loss 109935 0.19880345463752747\n",
      "Training batch loss 109955 0.19632495939731598\n",
      "Training batch loss 109975 0.19872212409973145\n",
      "Training batch loss 109995 0.2011108696460724\n",
      "Training batch loss 110015 0.19334366917610168\n",
      "Training batch loss 110035 0.2011612057685852\n",
      "Training batch loss 110055 0.18688777089118958\n",
      "Training batch loss 110075 0.19738690555095673\n",
      "Training batch loss 110095 0.20211857557296753\n",
      "Training batch loss 110115 0.19543080031871796\n",
      "Training batch loss 110135 0.20585793256759644\n",
      "Training batch loss 110155 0.19723135232925415\n",
      "Training batch loss 110175 0.19044041633605957\n",
      "Training batch loss 110195 0.20283818244934082\n",
      "Training batch loss 110215 0.19655489921569824\n",
      "Training batch loss 110235 0.1955663561820984\n",
      "Training batch loss 110255 0.20146483182907104\n",
      "Training batch loss 110275 0.1988999843597412\n",
      "Training batch loss 110295 0.19841943681240082\n",
      "Training batch loss 110315 0.20389148592948914\n",
      "Training batch loss 110335 0.19226862490177155\n",
      "Training batch loss 110355 0.2035389244556427\n",
      "Training batch loss 110375 0.19257719814777374\n",
      "Training batch loss 110395 0.1931302547454834\n",
      "Training batch loss 110415 0.19947470724582672\n",
      "Training batch loss 110435 0.19628626108169556\n",
      "Training batch loss 110455 0.20586371421813965\n",
      "Training batch loss 110475 0.1968308389186859\n",
      "Training batch loss 110495 0.20596987009048462\n",
      "Training batch loss 110515 0.1997758150100708\n",
      "Training batch loss 110535 0.20147401094436646\n",
      "Training batch loss 110555 0.19950464367866516\n",
      "Training batch loss 110575 0.1979617178440094\n",
      "Training batch loss 110595 0.20617517828941345\n",
      "Training batch loss 110615 0.2074558138847351\n",
      "Training batch loss 110635 0.19798772037029266\n",
      "Training batch loss 110655 0.20131948590278625\n",
      "Training batch loss 110675 0.21275150775909424\n",
      "Training batch loss 110695 0.19735084474086761\n",
      "Training batch loss 110715 0.2057875394821167\n",
      "Training batch loss 110735 0.2038630247116089\n",
      "Training batch loss 110755 0.1960001289844513\n",
      "Training batch loss 110775 0.19405975937843323\n",
      "Training batch loss 110795 0.20012935996055603\n",
      "Training batch loss 110815 0.20398616790771484\n",
      "Training batch loss 110835 0.19369086623191833\n",
      "Training batch loss 110855 0.20635294914245605\n",
      "Training batch loss 110875 0.1984732449054718\n",
      "Training batch loss 110895 0.20124468207359314\n",
      "Training batch loss 110915 0.18851247429847717\n",
      "Training batch loss 110935 0.19849780201911926\n",
      "Training batch loss 110955 0.20473094284534454\n",
      "Training batch loss 110975 0.19965031743049622\n",
      "Training batch loss 110995 0.20437705516815186\n",
      "Training batch loss 111015 0.19127696752548218\n",
      "Training batch loss 111035 0.1965131014585495\n",
      "Training batch loss 111055 0.195701465010643\n",
      "Training batch loss 111075 0.19478952884674072\n",
      "Training batch loss 111095 0.201440691947937\n",
      "Training epoch loss 81 0.19857197999954224\n",
      "Test loss during training 81 0.1976657658815384\n",
      "Test accuracy during training 81 0.0\n",
      "Training batch loss 111110 0.19170010089874268\n",
      "Training batch loss 111130 0.19589971005916595\n",
      "Training batch loss 111150 0.20131264626979828\n",
      "Training batch loss 111170 0.1983058601617813\n",
      "Training batch loss 111190 0.1991705298423767\n",
      "Training batch loss 111210 0.2009793221950531\n",
      "Training batch loss 111230 0.1980592906475067\n",
      "Training batch loss 111250 0.19715023040771484\n",
      "Training batch loss 111270 0.19260242581367493\n",
      "Training batch loss 111290 0.19880345463752747\n",
      "Training batch loss 111310 0.19632495939731598\n",
      "Training batch loss 111330 0.19872212409973145\n",
      "Training batch loss 111350 0.2011108696460724\n",
      "Training batch loss 111370 0.19334366917610168\n",
      "Training batch loss 111390 0.2011612057685852\n",
      "Training batch loss 111410 0.18688777089118958\n",
      "Training batch loss 111430 0.19738690555095673\n",
      "Training batch loss 111450 0.20211857557296753\n",
      "Training batch loss 111470 0.19543080031871796\n",
      "Training batch loss 111490 0.20585793256759644\n",
      "Training batch loss 111510 0.19723135232925415\n",
      "Training batch loss 111530 0.19044041633605957\n",
      "Training batch loss 111550 0.20283818244934082\n",
      "Training batch loss 111570 0.19655489921569824\n",
      "Training batch loss 111590 0.1955663561820984\n",
      "Training batch loss 111610 0.20146483182907104\n",
      "Training batch loss 111630 0.1988999843597412\n",
      "Training batch loss 111650 0.19841943681240082\n",
      "Training batch loss 111670 0.20389148592948914\n",
      "Training batch loss 111690 0.19226862490177155\n",
      "Training batch loss 111710 0.2035389244556427\n",
      "Training batch loss 111730 0.19257719814777374\n",
      "Training batch loss 111750 0.1931302547454834\n",
      "Training batch loss 111770 0.19947470724582672\n",
      "Training batch loss 111790 0.19628626108169556\n",
      "Training batch loss 111810 0.20586371421813965\n",
      "Training batch loss 111830 0.1968308389186859\n",
      "Training batch loss 111850 0.20596987009048462\n",
      "Training batch loss 111870 0.1997758150100708\n",
      "Training batch loss 111890 0.20147401094436646\n",
      "Training batch loss 111910 0.19950464367866516\n",
      "Training batch loss 111930 0.1979617178440094\n",
      "Training batch loss 111950 0.20617517828941345\n",
      "Training batch loss 111970 0.2074558138847351\n",
      "Training batch loss 111990 0.19798772037029266\n",
      "Training batch loss 112010 0.20131948590278625\n",
      "Training batch loss 112030 0.21275150775909424\n",
      "Training batch loss 112050 0.19735084474086761\n",
      "Training batch loss 112070 0.2057875394821167\n",
      "Training batch loss 112090 0.2038630247116089\n",
      "Training batch loss 112110 0.1960001289844513\n",
      "Training batch loss 112130 0.19405975937843323\n",
      "Training batch loss 112150 0.20012935996055603\n",
      "Training batch loss 112170 0.20398616790771484\n",
      "Training batch loss 112190 0.19369086623191833\n",
      "Training batch loss 112210 0.20635294914245605\n",
      "Training batch loss 112230 0.1984732449054718\n",
      "Training batch loss 112250 0.20124468207359314\n",
      "Training batch loss 112270 0.18851247429847717\n",
      "Training batch loss 112290 0.19849780201911926\n",
      "Training batch loss 112310 0.20473094284534454\n",
      "Training batch loss 112330 0.19965031743049622\n",
      "Training batch loss 112350 0.20437705516815186\n",
      "Training batch loss 112370 0.19127696752548218\n",
      "Training batch loss 112390 0.1965131014585495\n",
      "Training batch loss 112410 0.195701465010643\n",
      "Training batch loss 112430 0.19478952884674072\n",
      "Training batch loss 112450 0.201440691947937\n",
      "Training epoch loss 82 0.19857197999954224\n",
      "Test loss during training 82 0.1976657658815384\n",
      "Test accuracy during training 82 0.0\n",
      "Training batch loss 112465 0.19170010089874268\n",
      "Training batch loss 112485 0.19589971005916595\n",
      "Training batch loss 112505 0.20131264626979828\n",
      "Training batch loss 112525 0.1983058601617813\n",
      "Training batch loss 112545 0.1991705298423767\n",
      "Training batch loss 112565 0.2009793221950531\n",
      "Training batch loss 112585 0.1980592906475067\n",
      "Training batch loss 112605 0.19715023040771484\n",
      "Training batch loss 112625 0.19260242581367493\n",
      "Training batch loss 112645 0.19880345463752747\n",
      "Training batch loss 112665 0.19632495939731598\n",
      "Training batch loss 112685 0.19872212409973145\n",
      "Training batch loss 112705 0.2011108696460724\n",
      "Training batch loss 112725 0.19334366917610168\n",
      "Training batch loss 112745 0.2011612057685852\n",
      "Training batch loss 112765 0.18688777089118958\n",
      "Training batch loss 112785 0.19738690555095673\n",
      "Training batch loss 112805 0.20211857557296753\n",
      "Training batch loss 112825 0.19543080031871796\n",
      "Training batch loss 112845 0.20585793256759644\n",
      "Training batch loss 112865 0.19723135232925415\n",
      "Training batch loss 112885 0.19044041633605957\n",
      "Training batch loss 112905 0.20283818244934082\n",
      "Training batch loss 112925 0.19655489921569824\n",
      "Training batch loss 112945 0.1955663561820984\n",
      "Training batch loss 112965 0.20146483182907104\n",
      "Training batch loss 112985 0.1988999843597412\n",
      "Training batch loss 113005 0.19841943681240082\n",
      "Training batch loss 113025 0.20389148592948914\n",
      "Training batch loss 113045 0.19226862490177155\n",
      "Training batch loss 113065 0.2035389244556427\n",
      "Training batch loss 113085 0.19257719814777374\n",
      "Training batch loss 113105 0.1931302547454834\n",
      "Training batch loss 113125 0.19947470724582672\n",
      "Training batch loss 113145 0.19628626108169556\n",
      "Training batch loss 113165 0.20586371421813965\n",
      "Training batch loss 113185 0.1968308389186859\n",
      "Training batch loss 113205 0.20596987009048462\n",
      "Training batch loss 113225 0.1997758150100708\n",
      "Training batch loss 113245 0.20147401094436646\n",
      "Training batch loss 113265 0.19950464367866516\n",
      "Training batch loss 113285 0.1979617178440094\n",
      "Training batch loss 113305 0.20617517828941345\n",
      "Training batch loss 113325 0.2074558138847351\n",
      "Training batch loss 113345 0.19798772037029266\n",
      "Training batch loss 113365 0.20131948590278625\n",
      "Training batch loss 113385 0.21275150775909424\n",
      "Training batch loss 113405 0.19735084474086761\n",
      "Training batch loss 113425 0.2057875394821167\n",
      "Training batch loss 113445 0.2038630247116089\n",
      "Training batch loss 113465 0.1960001289844513\n",
      "Training batch loss 113485 0.19405975937843323\n",
      "Training batch loss 113505 0.20012935996055603\n",
      "Training batch loss 113525 0.20398616790771484\n",
      "Training batch loss 113545 0.19369086623191833\n",
      "Training batch loss 113565 0.20635294914245605\n",
      "Training batch loss 113585 0.1984732449054718\n",
      "Training batch loss 113605 0.20124468207359314\n",
      "Training batch loss 113625 0.18851247429847717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss 113645 0.19849780201911926\n",
      "Training batch loss 113665 0.20473094284534454\n",
      "Training batch loss 113685 0.19965031743049622\n",
      "Training batch loss 113705 0.20437705516815186\n",
      "Training batch loss 113725 0.19127696752548218\n",
      "Training batch loss 113745 0.1965131014585495\n",
      "Training batch loss 113765 0.195701465010643\n",
      "Training batch loss 113785 0.19478952884674072\n",
      "Training batch loss 113805 0.201440691947937\n",
      "Training epoch loss 83 0.19857197999954224\n",
      "Test loss during training 83 0.1976657658815384\n",
      "Test accuracy during training 83 0.0\n",
      "Training batch loss 113820 0.19170010089874268\n",
      "Training batch loss 113840 0.19589971005916595\n",
      "Training batch loss 113860 0.20131264626979828\n",
      "Training batch loss 113880 0.1983058601617813\n",
      "Training batch loss 113900 0.1991705298423767\n",
      "Training batch loss 113920 0.2009793221950531\n",
      "Training batch loss 113940 0.1980592906475067\n",
      "Training batch loss 113960 0.19715023040771484\n",
      "Training batch loss 113980 0.19260242581367493\n",
      "Training batch loss 114000 0.19880345463752747\n",
      "Training batch loss 114020 0.19632495939731598\n",
      "Training batch loss 114040 0.19872212409973145\n",
      "Training batch loss 114060 0.2011108696460724\n",
      "Training batch loss 114080 0.19334366917610168\n",
      "Training batch loss 114100 0.2011612057685852\n",
      "Training batch loss 114120 0.18688777089118958\n",
      "Training batch loss 114140 0.19738690555095673\n",
      "Training batch loss 114160 0.20211857557296753\n",
      "Training batch loss 114180 0.19543080031871796\n",
      "Training batch loss 114200 0.20585793256759644\n",
      "Training batch loss 114220 0.19723135232925415\n",
      "Training batch loss 114240 0.19044041633605957\n",
      "Training batch loss 114260 0.20283818244934082\n",
      "Training batch loss 114280 0.19655489921569824\n",
      "Training batch loss 114300 0.1955663561820984\n",
      "Training batch loss 114320 0.20146483182907104\n",
      "Training batch loss 114340 0.1988999843597412\n",
      "Training batch loss 114360 0.19841943681240082\n",
      "Training batch loss 114380 0.20389148592948914\n",
      "Training batch loss 114400 0.19226862490177155\n",
      "Training batch loss 114420 0.2035389244556427\n",
      "Training batch loss 114440 0.19257719814777374\n",
      "Training batch loss 114460 0.1931302547454834\n",
      "Training batch loss 114480 0.19947470724582672\n",
      "Training batch loss 114500 0.19628626108169556\n",
      "Training batch loss 114520 0.20586371421813965\n",
      "Training batch loss 114540 0.1968308389186859\n",
      "Training batch loss 114560 0.20596987009048462\n",
      "Training batch loss 114580 0.1997758150100708\n",
      "Training batch loss 114600 0.20147401094436646\n",
      "Training batch loss 114620 0.19950464367866516\n",
      "Training batch loss 114640 0.1979617178440094\n",
      "Training batch loss 114660 0.20617517828941345\n",
      "Training batch loss 114680 0.2074558138847351\n",
      "Training batch loss 114700 0.19798772037029266\n",
      "Training batch loss 114720 0.20131948590278625\n",
      "Training batch loss 114740 0.21275150775909424\n",
      "Training batch loss 114760 0.19735084474086761\n",
      "Training batch loss 114780 0.2057875394821167\n",
      "Training batch loss 114800 0.2038630247116089\n",
      "Training batch loss 114820 0.1960001289844513\n",
      "Training batch loss 114840 0.19405975937843323\n",
      "Training batch loss 114860 0.20012935996055603\n",
      "Training batch loss 114880 0.20398616790771484\n",
      "Training batch loss 114900 0.19369086623191833\n",
      "Training batch loss 114920 0.20635294914245605\n",
      "Training batch loss 114940 0.1984732449054718\n",
      "Training batch loss 114960 0.20124468207359314\n",
      "Training batch loss 114980 0.18851247429847717\n",
      "Training batch loss 115000 0.19849780201911926\n",
      "Training batch loss 115020 0.20473094284534454\n",
      "Training batch loss 115040 0.19965031743049622\n",
      "Training batch loss 115060 0.20437705516815186\n",
      "Training batch loss 115080 0.19127696752548218\n",
      "Training batch loss 115100 0.1965131014585495\n",
      "Training batch loss 115120 0.195701465010643\n",
      "Training batch loss 115140 0.19478952884674072\n",
      "Training batch loss 115160 0.201440691947937\n",
      "Training epoch loss 84 0.19857197999954224\n",
      "Test loss during training 84 0.1976657658815384\n",
      "Test accuracy during training 84 0.0\n",
      "Training batch loss 115175 0.19170010089874268\n",
      "Training batch loss 115195 0.19589971005916595\n",
      "Training batch loss 115215 0.20131264626979828\n",
      "Training batch loss 115235 0.1983058601617813\n",
      "Training batch loss 115255 0.1991705298423767\n",
      "Training batch loss 115275 0.2009793221950531\n",
      "Training batch loss 115295 0.1980592906475067\n",
      "Training batch loss 115315 0.19715023040771484\n",
      "Training batch loss 115335 0.19260242581367493\n",
      "Training batch loss 115355 0.19880345463752747\n",
      "Training batch loss 115375 0.19632495939731598\n",
      "Training batch loss 115395 0.19872212409973145\n",
      "Training batch loss 115415 0.2011108696460724\n",
      "Training batch loss 115435 0.19334366917610168\n",
      "Training batch loss 115455 0.2011612057685852\n",
      "Training batch loss 115475 0.18688777089118958\n",
      "Training batch loss 115495 0.19738690555095673\n",
      "Training batch loss 115515 0.20211857557296753\n",
      "Training batch loss 115535 0.19543080031871796\n",
      "Training batch loss 115555 0.20585793256759644\n",
      "Training batch loss 115575 0.19723135232925415\n",
      "Training batch loss 115595 0.19044041633605957\n",
      "Training batch loss 115615 0.20283818244934082\n",
      "Training batch loss 115635 0.19655489921569824\n",
      "Training batch loss 115655 0.1955663561820984\n",
      "Training batch loss 115675 0.20146483182907104\n",
      "Training batch loss 115695 0.1988999843597412\n",
      "Training batch loss 115715 0.19841943681240082\n",
      "Training batch loss 115735 0.20389148592948914\n",
      "Training batch loss 115755 0.19226862490177155\n",
      "Training batch loss 115775 0.2035389244556427\n",
      "Training batch loss 115795 0.19257719814777374\n",
      "Training batch loss 115815 0.1931302547454834\n",
      "Training batch loss 115835 0.19947470724582672\n",
      "Training batch loss 115855 0.19628626108169556\n",
      "Training batch loss 115875 0.20586371421813965\n",
      "Training batch loss 115895 0.1968308389186859\n",
      "Training batch loss 115915 0.20596987009048462\n",
      "Training batch loss 115935 0.1997758150100708\n",
      "Training batch loss 115955 0.20147401094436646\n",
      "Training batch loss 115975 0.19950464367866516\n",
      "Training batch loss 115995 0.1979617178440094\n",
      "Training batch loss 116015 0.20617517828941345\n",
      "Training batch loss 116035 0.2074558138847351\n",
      "Training batch loss 116055 0.19798772037029266\n",
      "Training batch loss 116075 0.20131948590278625\n",
      "Training batch loss 116095 0.21275150775909424\n",
      "Training batch loss 116115 0.19735084474086761\n",
      "Training batch loss 116135 0.2057875394821167\n",
      "Training batch loss 116155 0.2038630247116089\n",
      "Training batch loss 116175 0.1960001289844513\n",
      "Training batch loss 116195 0.19405975937843323\n",
      "Training batch loss 116215 0.20012935996055603\n",
      "Training batch loss 116235 0.20398616790771484\n",
      "Training batch loss 116255 0.19369086623191833\n",
      "Training batch loss 116275 0.20635294914245605\n",
      "Training batch loss 116295 0.1984732449054718\n",
      "Training batch loss 116315 0.20124468207359314\n",
      "Training batch loss 116335 0.18851247429847717\n",
      "Training batch loss 116355 0.19849780201911926\n",
      "Training batch loss 116375 0.20473094284534454\n",
      "Training batch loss 116395 0.19965031743049622\n",
      "Training batch loss 116415 0.20437705516815186\n",
      "Training batch loss 116435 0.19127696752548218\n",
      "Training batch loss 116455 0.1965131014585495\n",
      "Training batch loss 116475 0.195701465010643\n",
      "Training batch loss 116495 0.19478952884674072\n",
      "Training batch loss 116515 0.201440691947937\n",
      "Training epoch loss 85 0.19857197999954224\n",
      "Test loss during training 85 0.1976657658815384\n",
      "Test accuracy during training 85 0.0\n",
      "Training batch loss 116530 0.19170010089874268\n",
      "Training batch loss 116550 0.19589971005916595\n",
      "Training batch loss 116570 0.20131264626979828\n",
      "Training batch loss 116590 0.1983058601617813\n",
      "Training batch loss 116610 0.1991705298423767\n",
      "Training batch loss 116630 0.2009793221950531\n",
      "Training batch loss 116650 0.1980592906475067\n",
      "Training batch loss 116670 0.19715023040771484\n",
      "Training batch loss 116690 0.19260242581367493\n",
      "Training batch loss 116710 0.19880345463752747\n",
      "Training batch loss 116730 0.19632495939731598\n",
      "Training batch loss 116750 0.19872212409973145\n",
      "Training batch loss 116770 0.2011108696460724\n",
      "Training batch loss 116790 0.19334366917610168\n",
      "Training batch loss 116810 0.2011612057685852\n",
      "Training batch loss 116830 0.18688777089118958\n",
      "Training batch loss 116850 0.19738690555095673\n",
      "Training batch loss 116870 0.20211857557296753\n",
      "Training batch loss 116890 0.19543080031871796\n",
      "Training batch loss 116910 0.20585793256759644\n",
      "Training batch loss 116930 0.19723135232925415\n",
      "Training batch loss 116950 0.19044041633605957\n",
      "Training batch loss 116970 0.20283818244934082\n",
      "Training batch loss 116990 0.19655489921569824\n",
      "Training batch loss 117010 0.1955663561820984\n",
      "Training batch loss 117030 0.20146483182907104\n",
      "Training batch loss 117050 0.1988999843597412\n",
      "Training batch loss 117070 0.19841943681240082\n",
      "Training batch loss 117090 0.20389148592948914\n",
      "Training batch loss 117110 0.19226862490177155\n",
      "Training batch loss 117130 0.2035389244556427\n",
      "Training batch loss 117150 0.19257719814777374\n",
      "Training batch loss 117170 0.1931302547454834\n",
      "Training batch loss 117190 0.19947470724582672\n",
      "Training batch loss 117210 0.19628626108169556\n",
      "Training batch loss 117230 0.20586371421813965\n",
      "Training batch loss 117250 0.1968308389186859\n",
      "Training batch loss 117270 0.20596987009048462\n",
      "Training batch loss 117290 0.1997758150100708\n",
      "Training batch loss 117310 0.20147401094436646\n",
      "Training batch loss 117330 0.19950464367866516\n",
      "Training batch loss 117350 0.1979617178440094\n",
      "Training batch loss 117370 0.20617517828941345\n",
      "Training batch loss 117390 0.2074558138847351\n",
      "Training batch loss 117410 0.19798772037029266\n",
      "Training batch loss 117430 0.20131948590278625\n",
      "Training batch loss 117450 0.21275150775909424\n",
      "Training batch loss 117470 0.19735084474086761\n",
      "Training batch loss 117490 0.2057875394821167\n",
      "Training batch loss 117510 0.2038630247116089\n",
      "Training batch loss 117530 0.1960001289844513\n",
      "Training batch loss 117550 0.19405975937843323\n",
      "Training batch loss 117570 0.20012935996055603\n",
      "Training batch loss 117590 0.20398616790771484\n",
      "Training batch loss 117610 0.19369086623191833\n",
      "Training batch loss 117630 0.20635294914245605\n",
      "Training batch loss 117650 0.1984732449054718\n",
      "Training batch loss 117670 0.20124468207359314\n",
      "Training batch loss 117690 0.18851247429847717\n",
      "Training batch loss 117710 0.19849780201911926\n",
      "Training batch loss 117730 0.20473094284534454\n",
      "Training batch loss 117750 0.19965031743049622\n",
      "Training batch loss 117770 0.20437705516815186\n",
      "Training batch loss 117790 0.19127696752548218\n",
      "Training batch loss 117810 0.1965131014585495\n",
      "Training batch loss 117830 0.195701465010643\n",
      "Training batch loss 117850 0.19478952884674072\n",
      "Training batch loss 117870 0.201440691947937\n",
      "Training epoch loss 86 0.19857197999954224\n",
      "Test loss during training 86 0.1976657658815384\n",
      "Test accuracy during training 86 0.0\n",
      "Training batch loss 117885 0.19170010089874268\n",
      "Training batch loss 117905 0.19589971005916595\n",
      "Training batch loss 117925 0.20131264626979828\n",
      "Training batch loss 117945 0.1983058601617813\n",
      "Training batch loss 117965 0.1991705298423767\n",
      "Training batch loss 117985 0.2009793221950531\n",
      "Training batch loss 118005 0.1980592906475067\n",
      "Training batch loss 118025 0.19715023040771484\n",
      "Training batch loss 118045 0.19260242581367493\n",
      "Training batch loss 118065 0.19880345463752747\n",
      "Training batch loss 118085 0.19632495939731598\n",
      "Training batch loss 118105 0.19872212409973145\n",
      "Training batch loss 118125 0.2011108696460724\n",
      "Training batch loss 118145 0.19334366917610168\n",
      "Training batch loss 118165 0.2011612057685852\n",
      "Training batch loss 118185 0.18688777089118958\n",
      "Training batch loss 118205 0.19738690555095673\n",
      "Training batch loss 118225 0.20211857557296753\n",
      "Training batch loss 118245 0.19543080031871796\n",
      "Training batch loss 118265 0.20585793256759644\n",
      "Training batch loss 118285 0.19723135232925415\n",
      "Training batch loss 118305 0.19044041633605957\n",
      "Training batch loss 118325 0.20283818244934082\n",
      "Training batch loss 118345 0.19655489921569824\n",
      "Training batch loss 118365 0.1955663561820984\n",
      "Training batch loss 118385 0.20146483182907104\n",
      "Training batch loss 118405 0.1988999843597412\n",
      "Training batch loss 118425 0.19841943681240082\n",
      "Training batch loss 118445 0.20389148592948914\n",
      "Training batch loss 118465 0.19226862490177155\n",
      "Training batch loss 118485 0.2035389244556427\n",
      "Training batch loss 118505 0.19257719814777374\n",
      "Training batch loss 118525 0.1931302547454834\n",
      "Training batch loss 118545 0.19947470724582672\n",
      "Training batch loss 118565 0.19628626108169556\n",
      "Training batch loss 118585 0.20586371421813965\n",
      "Training batch loss 118605 0.1968308389186859\n",
      "Training batch loss 118625 0.20596987009048462\n",
      "Training batch loss 118645 0.1997758150100708\n",
      "Training batch loss 118665 0.20147401094436646\n",
      "Training batch loss 118685 0.19950464367866516\n",
      "Training batch loss 118705 0.1979617178440094\n",
      "Training batch loss 118725 0.20617517828941345\n",
      "Training batch loss 118745 0.2074558138847351\n",
      "Training batch loss 118765 0.19798772037029266\n",
      "Training batch loss 118785 0.20131948590278625\n",
      "Training batch loss 118805 0.21275150775909424\n",
      "Training batch loss 118825 0.19735084474086761\n",
      "Training batch loss 118845 0.2057875394821167\n",
      "Training batch loss 118865 0.2038630247116089\n",
      "Training batch loss 118885 0.1960001289844513\n",
      "Training batch loss 118905 0.19405975937843323\n",
      "Training batch loss 118925 0.20012935996055603\n",
      "Training batch loss 118945 0.20398616790771484\n",
      "Training batch loss 118965 0.19369086623191833\n",
      "Training batch loss 118985 0.20635294914245605\n",
      "Training batch loss 119005 0.1984732449054718\n",
      "Training batch loss 119025 0.20124468207359314\n",
      "Training batch loss 119045 0.18851247429847717\n",
      "Training batch loss 119065 0.19849780201911926\n",
      "Training batch loss 119085 0.20473094284534454\n",
      "Training batch loss 119105 0.19965031743049622\n",
      "Training batch loss 119125 0.20437705516815186\n",
      "Training batch loss 119145 0.19127696752548218\n",
      "Training batch loss 119165 0.1965131014585495\n",
      "Training batch loss 119185 0.195701465010643\n",
      "Training batch loss 119205 0.19478952884674072\n",
      "Training batch loss 119225 0.201440691947937\n",
      "Training epoch loss 87 0.19857197999954224\n",
      "Test loss during training 87 0.1976657658815384\n",
      "Test accuracy during training 87 0.0\n",
      "Training batch loss 119240 0.19170010089874268\n",
      "Training batch loss 119260 0.19589971005916595\n",
      "Training batch loss 119280 0.20131264626979828\n",
      "Training batch loss 119300 0.1983058601617813\n",
      "Training batch loss 119320 0.1991705298423767\n",
      "Training batch loss 119340 0.2009793221950531\n",
      "Training batch loss 119360 0.1980592906475067\n",
      "Training batch loss 119380 0.19715023040771484\n",
      "Training batch loss 119400 0.19260242581367493\n",
      "Training batch loss 119420 0.19880345463752747\n",
      "Training batch loss 119440 0.19632495939731598\n",
      "Training batch loss 119460 0.19872212409973145\n",
      "Training batch loss 119480 0.2011108696460724\n",
      "Training batch loss 119500 0.19334366917610168\n",
      "Training batch loss 119520 0.2011612057685852\n",
      "Training batch loss 119540 0.18688777089118958\n",
      "Training batch loss 119560 0.19738690555095673\n",
      "Training batch loss 119580 0.20211857557296753\n",
      "Training batch loss 119600 0.19543080031871796\n",
      "Training batch loss 119620 0.20585793256759644\n",
      "Training batch loss 119640 0.19723135232925415\n",
      "Training batch loss 119660 0.19044041633605957\n",
      "Training batch loss 119680 0.20283818244934082\n",
      "Training batch loss 119700 0.19655489921569824\n",
      "Training batch loss 119720 0.1955663561820984\n",
      "Training batch loss 119740 0.20146483182907104\n",
      "Training batch loss 119760 0.1988999843597412\n",
      "Training batch loss 119780 0.19841943681240082\n",
      "Training batch loss 119800 0.20389148592948914\n",
      "Training batch loss 119820 0.19226862490177155\n",
      "Training batch loss 119840 0.2035389244556427\n",
      "Training batch loss 119860 0.19257719814777374\n",
      "Training batch loss 119880 0.1931302547454834\n",
      "Training batch loss 119900 0.19947470724582672\n",
      "Training batch loss 119920 0.19628626108169556\n",
      "Training batch loss 119940 0.20586371421813965\n",
      "Training batch loss 119960 0.1968308389186859\n",
      "Training batch loss 119980 0.20596987009048462\n",
      "Training batch loss 120000 0.1997758150100708\n",
      "Training batch loss 120020 0.20147401094436646\n",
      "Training batch loss 120040 0.19950464367866516\n",
      "Training batch loss 120060 0.1979617178440094\n",
      "Training batch loss 120080 0.20617517828941345\n",
      "Training batch loss 120100 0.2074558138847351\n",
      "Training batch loss 120120 0.19798772037029266\n",
      "Training batch loss 120140 0.20131948590278625\n",
      "Training batch loss 120160 0.21275150775909424\n",
      "Training batch loss 120180 0.19735084474086761\n",
      "Training batch loss 120200 0.2057875394821167\n",
      "Training batch loss 120220 0.2038630247116089\n",
      "Training batch loss 120240 0.1960001289844513\n",
      "Training batch loss 120260 0.19405975937843323\n",
      "Training batch loss 120280 0.20012935996055603\n",
      "Training batch loss 120300 0.20398616790771484\n",
      "Training batch loss 120320 0.19369086623191833\n",
      "Training batch loss 120340 0.20635294914245605\n",
      "Training batch loss 120360 0.1984732449054718\n",
      "Training batch loss 120380 0.20124468207359314\n",
      "Training batch loss 120400 0.18851247429847717\n",
      "Training batch loss 120420 0.19849780201911926\n",
      "Training batch loss 120440 0.20473094284534454\n",
      "Training batch loss 120460 0.19965031743049622\n",
      "Training batch loss 120480 0.20437705516815186\n",
      "Training batch loss 120500 0.19127696752548218\n",
      "Training batch loss 120520 0.1965131014585495\n",
      "Training batch loss 120540 0.195701465010643\n",
      "Training batch loss 120560 0.19478952884674072\n",
      "Training batch loss 120580 0.201440691947937\n",
      "Training epoch loss 88 0.19857197999954224\n",
      "Test loss during training 88 0.1976657658815384\n",
      "Test accuracy during training 88 0.0\n",
      "Training batch loss 120595 0.19170010089874268\n",
      "Training batch loss 120615 0.19589971005916595\n",
      "Training batch loss 120635 0.20131264626979828\n",
      "Training batch loss 120655 0.1983058601617813\n",
      "Training batch loss 120675 0.1991705298423767\n",
      "Training batch loss 120695 0.2009793221950531\n",
      "Training batch loss 120715 0.1980592906475067\n",
      "Training batch loss 120735 0.19715023040771484\n",
      "Training batch loss 120755 0.19260242581367493\n",
      "Training batch loss 120775 0.19880345463752747\n",
      "Training batch loss 120795 0.19632495939731598\n",
      "Training batch loss 120815 0.19872212409973145\n",
      "Training batch loss 120835 0.2011108696460724\n",
      "Training batch loss 120855 0.19334366917610168\n",
      "Training batch loss 120875 0.2011612057685852\n",
      "Training batch loss 120895 0.18688777089118958\n",
      "Training batch loss 120915 0.19738690555095673\n",
      "Training batch loss 120935 0.20211857557296753\n",
      "Training batch loss 120955 0.19543080031871796\n",
      "Training batch loss 120975 0.20585793256759644\n",
      "Training batch loss 120995 0.19723135232925415\n",
      "Training batch loss 121015 0.19044041633605957\n",
      "Training batch loss 121035 0.20283818244934082\n",
      "Training batch loss 121055 0.19655489921569824\n",
      "Training batch loss 121075 0.1955663561820984\n",
      "Training batch loss 121095 0.20146483182907104\n",
      "Training batch loss 121115 0.1988999843597412\n",
      "Training batch loss 121135 0.19841943681240082\n",
      "Training batch loss 121155 0.20389148592948914\n",
      "Training batch loss 121175 0.19226862490177155\n",
      "Training batch loss 121195 0.2035389244556427\n",
      "Training batch loss 121215 0.19257719814777374\n",
      "Training batch loss 121235 0.1931302547454834\n",
      "Training batch loss 121255 0.19947470724582672\n",
      "Training batch loss 121275 0.19628626108169556\n",
      "Training batch loss 121295 0.20586371421813965\n",
      "Training batch loss 121315 0.1968308389186859\n",
      "Training batch loss 121335 0.20596987009048462\n",
      "Training batch loss 121355 0.1997758150100708\n",
      "Training batch loss 121375 0.20147401094436646\n",
      "Training batch loss 121395 0.19950464367866516\n",
      "Training batch loss 121415 0.1979617178440094\n",
      "Training batch loss 121435 0.20617517828941345\n",
      "Training batch loss 121455 0.2074558138847351\n",
      "Training batch loss 121475 0.19798772037029266\n",
      "Training batch loss 121495 0.20131948590278625\n",
      "Training batch loss 121515 0.21275150775909424\n",
      "Training batch loss 121535 0.19735084474086761\n",
      "Training batch loss 121555 0.2057875394821167\n",
      "Training batch loss 121575 0.2038630247116089\n",
      "Training batch loss 121595 0.1960001289844513\n",
      "Training batch loss 121615 0.19405975937843323\n",
      "Training batch loss 121635 0.20012935996055603\n",
      "Training batch loss 121655 0.20398616790771484\n",
      "Training batch loss 121675 0.19369086623191833\n",
      "Training batch loss 121695 0.20635294914245605\n",
      "Training batch loss 121715 0.1984732449054718\n",
      "Training batch loss 121735 0.20124468207359314\n",
      "Training batch loss 121755 0.18851247429847717\n",
      "Training batch loss 121775 0.19849780201911926\n",
      "Training batch loss 121795 0.20473094284534454\n",
      "Training batch loss 121815 0.19965031743049622\n",
      "Training batch loss 121835 0.20437705516815186\n",
      "Training batch loss 121855 0.19127696752548218\n",
      "Training batch loss 121875 0.1965131014585495\n",
      "Training batch loss 121895 0.195701465010643\n",
      "Training batch loss 121915 0.19478952884674072\n",
      "Training batch loss 121935 0.201440691947937\n",
      "Training epoch loss 89 0.19857197999954224\n",
      "Test loss during training 89 0.1976657658815384\n",
      "Test accuracy during training 89 0.0\n",
      "Training batch loss 121950 0.19170010089874268\n",
      "Training batch loss 121970 0.19589971005916595\n",
      "Training batch loss 121990 0.20131264626979828\n",
      "Training batch loss 122010 0.1983058601617813\n",
      "Training batch loss 122030 0.1991705298423767\n",
      "Training batch loss 122050 0.2009793221950531\n",
      "Training batch loss 122070 0.1980592906475067\n",
      "Training batch loss 122090 0.19715023040771484\n",
      "Training batch loss 122110 0.19260242581367493\n",
      "Training batch loss 122130 0.19880345463752747\n",
      "Training batch loss 122150 0.19632495939731598\n",
      "Training batch loss 122170 0.19872212409973145\n",
      "Training batch loss 122190 0.2011108696460724\n",
      "Training batch loss 122210 0.19334366917610168\n",
      "Training batch loss 122230 0.2011612057685852\n",
      "Training batch loss 122250 0.18688777089118958\n",
      "Training batch loss 122270 0.19738690555095673\n",
      "Training batch loss 122290 0.20211857557296753\n",
      "Training batch loss 122310 0.19543080031871796\n",
      "Training batch loss 122330 0.20585793256759644\n",
      "Training batch loss 122350 0.19723135232925415\n",
      "Training batch loss 122370 0.19044041633605957\n",
      "Training batch loss 122390 0.20283818244934082\n",
      "Training batch loss 122410 0.19655489921569824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss 122430 0.1955663561820984\n",
      "Training batch loss 122450 0.20146483182907104\n",
      "Training batch loss 122470 0.1988999843597412\n",
      "Training batch loss 122490 0.19841943681240082\n",
      "Training batch loss 122510 0.20389148592948914\n",
      "Training batch loss 122530 0.19226862490177155\n",
      "Training batch loss 122550 0.2035389244556427\n",
      "Training batch loss 122570 0.19257719814777374\n",
      "Training batch loss 122590 0.1931302547454834\n",
      "Training batch loss 122610 0.19947470724582672\n",
      "Training batch loss 122630 0.19628626108169556\n",
      "Training batch loss 122650 0.20586371421813965\n",
      "Training batch loss 122670 0.1968308389186859\n",
      "Training batch loss 122690 0.20596987009048462\n",
      "Training batch loss 122710 0.1997758150100708\n",
      "Training batch loss 122730 0.20147401094436646\n",
      "Training batch loss 122750 0.19950464367866516\n",
      "Training batch loss 122770 0.1979617178440094\n",
      "Training batch loss 122790 0.20617517828941345\n",
      "Training batch loss 122810 0.2074558138847351\n",
      "Training batch loss 122830 0.19798772037029266\n",
      "Training batch loss 122850 0.20131948590278625\n",
      "Training batch loss 122870 0.21275150775909424\n",
      "Training batch loss 122890 0.19735084474086761\n",
      "Training batch loss 122910 0.2057875394821167\n",
      "Training batch loss 122930 0.2038630247116089\n",
      "Training batch loss 122950 0.1960001289844513\n",
      "Training batch loss 122970 0.19405975937843323\n",
      "Training batch loss 122990 0.20012935996055603\n",
      "Training batch loss 123010 0.20398616790771484\n",
      "Training batch loss 123030 0.19369086623191833\n",
      "Training batch loss 123050 0.20635294914245605\n",
      "Training batch loss 123070 0.1984732449054718\n",
      "Training batch loss 123090 0.20124468207359314\n",
      "Training batch loss 123110 0.18851247429847717\n",
      "Training batch loss 123130 0.19849780201911926\n",
      "Training batch loss 123150 0.20473094284534454\n",
      "Training batch loss 123170 0.19965031743049622\n",
      "Training batch loss 123190 0.20437705516815186\n",
      "Training batch loss 123210 0.19127696752548218\n",
      "Training batch loss 123230 0.1965131014585495\n",
      "Training batch loss 123250 0.195701465010643\n",
      "Training batch loss 123270 0.19478952884674072\n",
      "Training batch loss 123290 0.201440691947937\n",
      "Training epoch loss 90 0.19857197999954224\n",
      "Test loss during training 90 0.1976657658815384\n",
      "Test accuracy during training 90 0.0\n",
      "Training batch loss 123305 0.19170010089874268\n",
      "Training batch loss 123325 0.19589971005916595\n",
      "Training batch loss 123345 0.20131264626979828\n",
      "Training batch loss 123365 0.1983058601617813\n",
      "Training batch loss 123385 0.1991705298423767\n",
      "Training batch loss 123405 0.2009793221950531\n",
      "Training batch loss 123425 0.1980592906475067\n",
      "Training batch loss 123445 0.19715023040771484\n",
      "Training batch loss 123465 0.19260242581367493\n",
      "Training batch loss 123485 0.19880345463752747\n",
      "Training batch loss 123505 0.19632495939731598\n",
      "Training batch loss 123525 0.19872212409973145\n",
      "Training batch loss 123545 0.2011108696460724\n",
      "Training batch loss 123565 0.19334366917610168\n",
      "Training batch loss 123585 0.2011612057685852\n",
      "Training batch loss 123605 0.18688777089118958\n",
      "Training batch loss 123625 0.19738690555095673\n",
      "Training batch loss 123645 0.20211857557296753\n",
      "Training batch loss 123665 0.19543080031871796\n",
      "Training batch loss 123685 0.20585793256759644\n",
      "Training batch loss 123705 0.19723135232925415\n",
      "Training batch loss 123725 0.19044041633605957\n",
      "Training batch loss 123745 0.20283818244934082\n",
      "Training batch loss 123765 0.19655489921569824\n",
      "Training batch loss 123785 0.1955663561820984\n",
      "Training batch loss 123805 0.20146483182907104\n",
      "Training batch loss 123825 0.1988999843597412\n",
      "Training batch loss 123845 0.19841943681240082\n",
      "Training batch loss 123865 0.20389148592948914\n",
      "Training batch loss 123885 0.19226862490177155\n",
      "Training batch loss 123905 0.2035389244556427\n",
      "Training batch loss 123925 0.19257719814777374\n",
      "Training batch loss 123945 0.1931302547454834\n",
      "Training batch loss 123965 0.19947470724582672\n",
      "Training batch loss 123985 0.19628626108169556\n",
      "Training batch loss 124005 0.20586371421813965\n",
      "Training batch loss 124025 0.1968308389186859\n",
      "Training batch loss 124045 0.20596987009048462\n",
      "Training batch loss 124065 0.1997758150100708\n",
      "Training batch loss 124085 0.20147401094436646\n",
      "Training batch loss 124105 0.19950464367866516\n",
      "Training batch loss 124125 0.1979617178440094\n",
      "Training batch loss 124145 0.20617517828941345\n",
      "Training batch loss 124165 0.2074558138847351\n",
      "Training batch loss 124185 0.19798772037029266\n",
      "Training batch loss 124205 0.20131948590278625\n",
      "Training batch loss 124225 0.21275150775909424\n",
      "Training batch loss 124245 0.19735084474086761\n",
      "Training batch loss 124265 0.2057875394821167\n",
      "Training batch loss 124285 0.2038630247116089\n",
      "Training batch loss 124305 0.1960001289844513\n",
      "Training batch loss 124325 0.19405975937843323\n",
      "Training batch loss 124345 0.20012935996055603\n",
      "Training batch loss 124365 0.20398616790771484\n",
      "Training batch loss 124385 0.19369086623191833\n",
      "Training batch loss 124405 0.20635294914245605\n",
      "Training batch loss 124425 0.1984732449054718\n",
      "Training batch loss 124445 0.20124468207359314\n",
      "Training batch loss 124465 0.18851247429847717\n",
      "Training batch loss 124485 0.19849780201911926\n",
      "Training batch loss 124505 0.20473094284534454\n",
      "Training batch loss 124525 0.19965031743049622\n",
      "Training batch loss 124545 0.20437705516815186\n",
      "Training batch loss 124565 0.19127696752548218\n",
      "Training batch loss 124585 0.1965131014585495\n",
      "Training batch loss 124605 0.195701465010643\n",
      "Training batch loss 124625 0.19478952884674072\n",
      "Training batch loss 124645 0.201440691947937\n",
      "Training epoch loss 91 0.19857197999954224\n",
      "Test loss during training 91 0.1976657658815384\n",
      "Test accuracy during training 91 0.0\n",
      "Training batch loss 124660 0.19170010089874268\n",
      "Training batch loss 124680 0.19589971005916595\n",
      "Training batch loss 124700 0.20131264626979828\n",
      "Training batch loss 124720 0.1983058601617813\n",
      "Training batch loss 124740 0.1991705298423767\n",
      "Training batch loss 124760 0.2009793221950531\n",
      "Training batch loss 124780 0.1980592906475067\n",
      "Training batch loss 124800 0.19715023040771484\n",
      "Training batch loss 124820 0.19260242581367493\n",
      "Training batch loss 124840 0.19880345463752747\n",
      "Training batch loss 124860 0.19632495939731598\n",
      "Training batch loss 124880 0.19872212409973145\n",
      "Training batch loss 124900 0.2011108696460724\n",
      "Training batch loss 124920 0.19334366917610168\n",
      "Training batch loss 124940 0.2011612057685852\n",
      "Training batch loss 124960 0.18688777089118958\n",
      "Training batch loss 124980 0.19738690555095673\n",
      "Training batch loss 125000 0.20211857557296753\n",
      "Training batch loss 125020 0.19543080031871796\n",
      "Training batch loss 125040 0.20585793256759644\n",
      "Training batch loss 125060 0.19723135232925415\n",
      "Training batch loss 125080 0.19044041633605957\n",
      "Training batch loss 125100 0.20283818244934082\n",
      "Training batch loss 125120 0.19655489921569824\n",
      "Training batch loss 125140 0.1955663561820984\n",
      "Training batch loss 125160 0.20146483182907104\n",
      "Training batch loss 125180 0.1988999843597412\n",
      "Training batch loss 125200 0.19841943681240082\n",
      "Training batch loss 125220 0.20389148592948914\n",
      "Training batch loss 125240 0.19226862490177155\n",
      "Training batch loss 125260 0.2035389244556427\n",
      "Training batch loss 125280 0.19257719814777374\n",
      "Training batch loss 125300 0.1931302547454834\n",
      "Training batch loss 125320 0.19947470724582672\n",
      "Training batch loss 125340 0.19628626108169556\n",
      "Training batch loss 125360 0.20586371421813965\n",
      "Training batch loss 125380 0.1968308389186859\n",
      "Training batch loss 125400 0.20596987009048462\n",
      "Training batch loss 125420 0.1997758150100708\n",
      "Training batch loss 125440 0.20147401094436646\n",
      "Training batch loss 125460 0.19950464367866516\n",
      "Training batch loss 125480 0.1979617178440094\n",
      "Training batch loss 125500 0.20617517828941345\n",
      "Training batch loss 125520 0.2074558138847351\n",
      "Training batch loss 125540 0.19798772037029266\n",
      "Training batch loss 125560 0.20131948590278625\n",
      "Training batch loss 125580 0.21275150775909424\n",
      "Training batch loss 125600 0.19735084474086761\n",
      "Training batch loss 125620 0.2057875394821167\n",
      "Training batch loss 125640 0.2038630247116089\n",
      "Training batch loss 125660 0.1960001289844513\n",
      "Training batch loss 125680 0.19405975937843323\n",
      "Training batch loss 125700 0.20012935996055603\n",
      "Training batch loss 125720 0.20398616790771484\n",
      "Training batch loss 125740 0.19369086623191833\n",
      "Training batch loss 125760 0.20635294914245605\n",
      "Training batch loss 125780 0.1984732449054718\n",
      "Training batch loss 125800 0.20124468207359314\n",
      "Training batch loss 125820 0.18851247429847717\n",
      "Training batch loss 125840 0.19849780201911926\n",
      "Training batch loss 125860 0.20473094284534454\n",
      "Training batch loss 125880 0.19965031743049622\n",
      "Training batch loss 125900 0.20437705516815186\n",
      "Training batch loss 125920 0.19127696752548218\n",
      "Training batch loss 125940 0.1965131014585495\n",
      "Training batch loss 125960 0.195701465010643\n",
      "Training batch loss 125980 0.19478952884674072\n",
      "Training batch loss 126000 0.201440691947937\n",
      "Training epoch loss 92 0.19857197999954224\n",
      "Test loss during training 92 0.19766582548618317\n",
      "Test accuracy during training 92 0.0\n",
      "Training batch loss 126015 0.19170010089874268\n",
      "Training batch loss 126035 0.19589971005916595\n",
      "Training batch loss 126055 0.20131264626979828\n",
      "Training batch loss 126075 0.1983058601617813\n",
      "Training batch loss 126095 0.1991705298423767\n",
      "Training batch loss 126115 0.2009793221950531\n",
      "Training batch loss 126135 0.1980592906475067\n",
      "Training batch loss 126155 0.19715023040771484\n",
      "Training batch loss 126175 0.19260242581367493\n",
      "Training batch loss 126195 0.19880345463752747\n",
      "Training batch loss 126215 0.19632495939731598\n",
      "Training batch loss 126235 0.19872212409973145\n",
      "Training batch loss 126255 0.2011108696460724\n",
      "Training batch loss 126275 0.19334366917610168\n",
      "Training batch loss 126295 0.2011612057685852\n",
      "Training batch loss 126315 0.18688777089118958\n",
      "Training batch loss 126335 0.19738690555095673\n",
      "Training batch loss 126355 0.20211857557296753\n",
      "Training batch loss 126375 0.19543080031871796\n",
      "Training batch loss 126395 0.20585793256759644\n",
      "Training batch loss 126415 0.19723135232925415\n",
      "Training batch loss 126435 0.19044041633605957\n",
      "Training batch loss 126455 0.20283818244934082\n",
      "Training batch loss 126475 0.19655489921569824\n",
      "Training batch loss 126495 0.1955663561820984\n",
      "Training batch loss 126515 0.20146483182907104\n",
      "Training batch loss 126535 0.1988999843597412\n",
      "Training batch loss 126555 0.19841943681240082\n",
      "Training batch loss 126575 0.20389148592948914\n",
      "Training batch loss 126595 0.19226862490177155\n",
      "Training batch loss 126615 0.2035389244556427\n",
      "Training batch loss 126635 0.19257719814777374\n",
      "Training batch loss 126655 0.1931302547454834\n",
      "Training batch loss 126675 0.19947470724582672\n",
      "Training batch loss 126695 0.19628626108169556\n",
      "Training batch loss 126715 0.20586371421813965\n",
      "Training batch loss 126735 0.1968308389186859\n",
      "Training batch loss 126755 0.20596987009048462\n",
      "Training batch loss 126775 0.1997758150100708\n",
      "Training batch loss 126795 0.20147401094436646\n",
      "Training batch loss 126815 0.19950464367866516\n",
      "Training batch loss 126835 0.1979617178440094\n",
      "Training batch loss 126855 0.20617517828941345\n",
      "Training batch loss 126875 0.2074558138847351\n",
      "Training batch loss 126895 0.19798772037029266\n",
      "Training batch loss 126915 0.20131948590278625\n",
      "Training batch loss 126935 0.21275150775909424\n",
      "Training batch loss 126955 0.19735084474086761\n",
      "Training batch loss 126975 0.2057875394821167\n",
      "Training batch loss 126995 0.2038630247116089\n",
      "Training batch loss 127015 0.1960001289844513\n",
      "Training batch loss 127035 0.19405975937843323\n",
      "Training batch loss 127055 0.20012935996055603\n",
      "Training batch loss 127075 0.20398616790771484\n",
      "Training batch loss 127095 0.19369086623191833\n",
      "Training batch loss 127115 0.20635294914245605\n",
      "Training batch loss 127135 0.1984732449054718\n",
      "Training batch loss 127155 0.20124468207359314\n",
      "Training batch loss 127175 0.18851247429847717\n",
      "Training batch loss 127195 0.19849780201911926\n",
      "Training batch loss 127215 0.20473094284534454\n",
      "Training batch loss 127235 0.19965031743049622\n",
      "Training batch loss 127255 0.20437705516815186\n",
      "Training batch loss 127275 0.19127696752548218\n",
      "Training batch loss 127295 0.1965131014585495\n",
      "Training batch loss 127315 0.195701465010643\n",
      "Training batch loss 127335 0.19478952884674072\n",
      "Training batch loss 127355 0.201440691947937\n",
      "Training epoch loss 93 0.19857197999954224\n",
      "Test loss during training 93 0.1976657658815384\n",
      "Test accuracy during training 93 0.0\n",
      "Training batch loss 127370 0.19170010089874268\n",
      "Training batch loss 127390 0.19589971005916595\n",
      "Training batch loss 127410 0.20131264626979828\n",
      "Training batch loss 127430 0.1983058601617813\n",
      "Training batch loss 127450 0.1991705298423767\n",
      "Training batch loss 127470 0.2009793221950531\n",
      "Training batch loss 127490 0.1980592906475067\n",
      "Training batch loss 127510 0.19715023040771484\n",
      "Training batch loss 127530 0.19260242581367493\n",
      "Training batch loss 127550 0.19880345463752747\n",
      "Training batch loss 127570 0.19632495939731598\n",
      "Training batch loss 127590 0.19872212409973145\n",
      "Training batch loss 127610 0.2011108696460724\n",
      "Training batch loss 127630 0.19334366917610168\n",
      "Training batch loss 127650 0.2011612057685852\n",
      "Training batch loss 127670 0.18688777089118958\n",
      "Training batch loss 127690 0.19738690555095673\n",
      "Training batch loss 127710 0.20211857557296753\n",
      "Training batch loss 127730 0.19543080031871796\n",
      "Training batch loss 127750 0.20585793256759644\n",
      "Training batch loss 127770 0.19723135232925415\n",
      "Training batch loss 127790 0.19044041633605957\n",
      "Training batch loss 127810 0.20283818244934082\n",
      "Training batch loss 127830 0.19655489921569824\n",
      "Training batch loss 127850 0.1955663561820984\n",
      "Training batch loss 127870 0.20146483182907104\n",
      "Training batch loss 127890 0.1988999843597412\n",
      "Training batch loss 127910 0.19841943681240082\n",
      "Training batch loss 127930 0.20389148592948914\n",
      "Training batch loss 127950 0.19226862490177155\n",
      "Training batch loss 127970 0.2035389244556427\n",
      "Training batch loss 127990 0.19257719814777374\n",
      "Training batch loss 128010 0.1931302547454834\n",
      "Training batch loss 128030 0.19947470724582672\n",
      "Training batch loss 128050 0.19628626108169556\n",
      "Training batch loss 128070 0.20586371421813965\n",
      "Training batch loss 128090 0.1968308389186859\n",
      "Training batch loss 128110 0.20596987009048462\n",
      "Training batch loss 128130 0.1997758150100708\n",
      "Training batch loss 128150 0.20147401094436646\n",
      "Training batch loss 128170 0.19950464367866516\n",
      "Training batch loss 128190 0.1979617178440094\n",
      "Training batch loss 128210 0.20617517828941345\n",
      "Training batch loss 128230 0.2074558138847351\n",
      "Training batch loss 128250 0.19798772037029266\n",
      "Training batch loss 128270 0.20131948590278625\n",
      "Training batch loss 128290 0.21275150775909424\n",
      "Training batch loss 128310 0.19735084474086761\n",
      "Training batch loss 128330 0.2057875394821167\n",
      "Training batch loss 128350 0.2038630247116089\n",
      "Training batch loss 128370 0.1960001289844513\n",
      "Training batch loss 128390 0.19405975937843323\n",
      "Training batch loss 128410 0.20012935996055603\n",
      "Training batch loss 128430 0.20398616790771484\n",
      "Training batch loss 128450 0.19369086623191833\n",
      "Training batch loss 128470 0.20635294914245605\n",
      "Training batch loss 128490 0.1984732449054718\n",
      "Training batch loss 128510 0.20124468207359314\n",
      "Training batch loss 128530 0.18851247429847717\n",
      "Training batch loss 128550 0.19849780201911926\n",
      "Training batch loss 128570 0.20473094284534454\n",
      "Training batch loss 128590 0.19965031743049622\n",
      "Training batch loss 128610 0.20437705516815186\n",
      "Training batch loss 128630 0.19127696752548218\n",
      "Training batch loss 128650 0.1965131014585495\n",
      "Training batch loss 128670 0.195701465010643\n",
      "Training batch loss 128690 0.19478952884674072\n",
      "Training batch loss 128710 0.201440691947937\n",
      "Training epoch loss 94 0.19857197999954224\n",
      "Test loss during training 94 0.1976657658815384\n",
      "Test accuracy during training 94 0.0\n",
      "Training batch loss 128725 0.19170010089874268\n",
      "Training batch loss 128745 0.19589971005916595\n",
      "Training batch loss 128765 0.20131264626979828\n",
      "Training batch loss 128785 0.1983058601617813\n",
      "Training batch loss 128805 0.1991705298423767\n",
      "Training batch loss 128825 0.2009793221950531\n",
      "Training batch loss 128845 0.1980592906475067\n",
      "Training batch loss 128865 0.19715023040771484\n",
      "Training batch loss 128885 0.19260242581367493\n",
      "Training batch loss 128905 0.19880345463752747\n",
      "Training batch loss 128925 0.19632495939731598\n",
      "Training batch loss 128945 0.19872212409973145\n",
      "Training batch loss 128965 0.2011108696460724\n",
      "Training batch loss 128985 0.19334366917610168\n",
      "Training batch loss 129005 0.2011612057685852\n",
      "Training batch loss 129025 0.18688777089118958\n",
      "Training batch loss 129045 0.19738690555095673\n",
      "Training batch loss 129065 0.20211857557296753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss 129085 0.19543080031871796\n",
      "Training batch loss 129105 0.20585793256759644\n",
      "Training batch loss 129125 0.19723135232925415\n",
      "Training batch loss 129145 0.19044041633605957\n",
      "Training batch loss 129165 0.20283818244934082\n",
      "Training batch loss 129185 0.19655489921569824\n",
      "Training batch loss 129205 0.1955663561820984\n",
      "Training batch loss 129225 0.20146483182907104\n",
      "Training batch loss 129245 0.1988999843597412\n",
      "Training batch loss 129265 0.19841943681240082\n",
      "Training batch loss 129285 0.20389148592948914\n",
      "Training batch loss 129305 0.19226862490177155\n",
      "Training batch loss 129325 0.2035389244556427\n",
      "Training batch loss 129345 0.19257719814777374\n",
      "Training batch loss 129365 0.1931302547454834\n",
      "Training batch loss 129385 0.19947470724582672\n",
      "Training batch loss 129405 0.19628626108169556\n",
      "Training batch loss 129425 0.20586371421813965\n",
      "Training batch loss 129445 0.1968308389186859\n",
      "Training batch loss 129465 0.20596987009048462\n",
      "Training batch loss 129485 0.1997758150100708\n",
      "Training batch loss 129505 0.20147401094436646\n",
      "Training batch loss 129525 0.19950464367866516\n",
      "Training batch loss 129545 0.1979617178440094\n",
      "Training batch loss 129565 0.20617517828941345\n",
      "Training batch loss 129585 0.2074558138847351\n",
      "Training batch loss 129605 0.19798772037029266\n",
      "Training batch loss 129625 0.20131948590278625\n",
      "Training batch loss 129645 0.21275150775909424\n",
      "Training batch loss 129665 0.19735084474086761\n",
      "Training batch loss 129685 0.2057875394821167\n",
      "Training batch loss 129705 0.2038630247116089\n",
      "Training batch loss 129725 0.1960001289844513\n",
      "Training batch loss 129745 0.19405975937843323\n",
      "Training batch loss 129765 0.20012935996055603\n",
      "Training batch loss 129785 0.20398616790771484\n",
      "Training batch loss 129805 0.19369086623191833\n",
      "Training batch loss 129825 0.20635294914245605\n",
      "Training batch loss 129845 0.1984732449054718\n",
      "Training batch loss 129865 0.20124468207359314\n",
      "Training batch loss 129885 0.18851247429847717\n",
      "Training batch loss 129905 0.19849780201911926\n",
      "Training batch loss 129925 0.20473094284534454\n",
      "Training batch loss 129945 0.19965031743049622\n",
      "Training batch loss 129965 0.20437705516815186\n",
      "Training batch loss 129985 0.19127696752548218\n",
      "Training batch loss 130005 0.1965131014585495\n",
      "Training batch loss 130025 0.195701465010643\n",
      "Training batch loss 130045 0.19478952884674072\n",
      "Training batch loss 130065 0.201440691947937\n",
      "Training epoch loss 95 0.19857197999954224\n",
      "Test loss during training 95 0.1976657658815384\n",
      "Test accuracy during training 95 0.0\n",
      "Training batch loss 130080 0.19170010089874268\n",
      "Training batch loss 130100 0.19589971005916595\n",
      "Training batch loss 130120 0.20131264626979828\n",
      "Training batch loss 130140 0.1983058601617813\n",
      "Training batch loss 130160 0.1991705298423767\n",
      "Training batch loss 130180 0.2009793221950531\n",
      "Training batch loss 130200 0.1980592906475067\n",
      "Training batch loss 130220 0.19715023040771484\n",
      "Training batch loss 130240 0.19260242581367493\n",
      "Training batch loss 130260 0.19880345463752747\n",
      "Training batch loss 130280 0.19632495939731598\n",
      "Training batch loss 130300 0.19872212409973145\n",
      "Training batch loss 130320 0.2011108696460724\n",
      "Training batch loss 130340 0.19334366917610168\n",
      "Training batch loss 130360 0.2011612057685852\n",
      "Training batch loss 130380 0.18688777089118958\n",
      "Training batch loss 130400 0.19738690555095673\n",
      "Training batch loss 130420 0.20211857557296753\n",
      "Training batch loss 130440 0.19543080031871796\n",
      "Training batch loss 130460 0.20585793256759644\n",
      "Training batch loss 130480 0.19723135232925415\n",
      "Training batch loss 130500 0.19044041633605957\n",
      "Training batch loss 130520 0.20283818244934082\n",
      "Training batch loss 130540 0.19655489921569824\n",
      "Training batch loss 130560 0.1955663561820984\n",
      "Training batch loss 130580 0.20146483182907104\n",
      "Training batch loss 130600 0.1988999843597412\n",
      "Training batch loss 130620 0.19841943681240082\n",
      "Training batch loss 130640 0.20389148592948914\n",
      "Training batch loss 130660 0.19226862490177155\n",
      "Training batch loss 130680 0.2035389244556427\n",
      "Training batch loss 130700 0.19257719814777374\n",
      "Training batch loss 130720 0.1931302547454834\n",
      "Training batch loss 130740 0.19947470724582672\n",
      "Training batch loss 130760 0.19628626108169556\n",
      "Training batch loss 130780 0.20586371421813965\n",
      "Training batch loss 130800 0.1968308389186859\n",
      "Training batch loss 130820 0.20596987009048462\n",
      "Training batch loss 130840 0.1997758150100708\n",
      "Training batch loss 130860 0.20147401094436646\n",
      "Training batch loss 130880 0.19950464367866516\n",
      "Training batch loss 130900 0.1979617178440094\n",
      "Training batch loss 130920 0.20617517828941345\n",
      "Training batch loss 130940 0.2074558138847351\n",
      "Training batch loss 130960 0.19798772037029266\n",
      "Training batch loss 130980 0.20131948590278625\n",
      "Training batch loss 131000 0.21275150775909424\n",
      "Training batch loss 131020 0.19735084474086761\n",
      "Training batch loss 131040 0.2057875394821167\n",
      "Training batch loss 131060 0.2038630247116089\n",
      "Training batch loss 131080 0.1960001289844513\n",
      "Training batch loss 131100 0.19405975937843323\n",
      "Training batch loss 131120 0.20012935996055603\n",
      "Training batch loss 131140 0.20398616790771484\n",
      "Training batch loss 131160 0.19369086623191833\n",
      "Training batch loss 131180 0.20635294914245605\n",
      "Training batch loss 131200 0.1984732449054718\n",
      "Training batch loss 131220 0.20124468207359314\n",
      "Training batch loss 131240 0.18851247429847717\n",
      "Training batch loss 131260 0.19849780201911926\n",
      "Training batch loss 131280 0.20473094284534454\n",
      "Training batch loss 131300 0.19965031743049622\n",
      "Training batch loss 131320 0.20437705516815186\n",
      "Training batch loss 131340 0.19127696752548218\n",
      "Training batch loss 131360 0.1965131014585495\n",
      "Training batch loss 131380 0.195701465010643\n",
      "Training batch loss 131400 0.19478952884674072\n",
      "Training batch loss 131420 0.201440691947937\n",
      "Training epoch loss 96 0.19857197999954224\n",
      "Test loss during training 96 0.1976657658815384\n",
      "Test accuracy during training 96 0.0\n",
      "Training batch loss 131435 0.19170010089874268\n",
      "Training batch loss 131455 0.19589971005916595\n",
      "Training batch loss 131475 0.20131264626979828\n",
      "Training batch loss 131495 0.1983058601617813\n",
      "Training batch loss 131515 0.1991705298423767\n",
      "Training batch loss 131535 0.2009793221950531\n",
      "Training batch loss 131555 0.1980592906475067\n",
      "Training batch loss 131575 0.19715023040771484\n",
      "Training batch loss 131595 0.19260242581367493\n",
      "Training batch loss 131615 0.19880345463752747\n",
      "Training batch loss 131635 0.19632495939731598\n",
      "Training batch loss 131655 0.19872212409973145\n",
      "Training batch loss 131675 0.2011108696460724\n",
      "Training batch loss 131695 0.19334366917610168\n",
      "Training batch loss 131715 0.2011612057685852\n",
      "Training batch loss 131735 0.18688777089118958\n",
      "Training batch loss 131755 0.19738690555095673\n",
      "Training batch loss 131775 0.20211857557296753\n",
      "Training batch loss 131795 0.19543080031871796\n",
      "Training batch loss 131815 0.20585793256759644\n",
      "Training batch loss 131835 0.19723135232925415\n",
      "Training batch loss 131855 0.19044041633605957\n",
      "Training batch loss 131875 0.20283818244934082\n",
      "Training batch loss 131895 0.19655489921569824\n",
      "Training batch loss 131915 0.1955663561820984\n",
      "Training batch loss 131935 0.20146483182907104\n",
      "Training batch loss 131955 0.1988999843597412\n",
      "Training batch loss 131975 0.19841943681240082\n",
      "Training batch loss 131995 0.20389148592948914\n",
      "Training batch loss 132015 0.19226862490177155\n",
      "Training batch loss 132035 0.2035389244556427\n",
      "Training batch loss 132055 0.19257719814777374\n",
      "Training batch loss 132075 0.1931302547454834\n",
      "Training batch loss 132095 0.19947470724582672\n",
      "Training batch loss 132115 0.19628626108169556\n",
      "Training batch loss 132135 0.20586371421813965\n",
      "Training batch loss 132155 0.1968308389186859\n",
      "Training batch loss 132175 0.20596987009048462\n",
      "Training batch loss 132195 0.1997758150100708\n",
      "Training batch loss 132215 0.20147401094436646\n",
      "Training batch loss 132235 0.19950464367866516\n",
      "Training batch loss 132255 0.1979617178440094\n",
      "Training batch loss 132275 0.20617517828941345\n",
      "Training batch loss 132295 0.2074558138847351\n",
      "Training batch loss 132315 0.19798772037029266\n",
      "Training batch loss 132335 0.20131948590278625\n",
      "Training batch loss 132355 0.21275150775909424\n",
      "Training batch loss 132375 0.19735084474086761\n",
      "Training batch loss 132395 0.2057875394821167\n",
      "Training batch loss 132415 0.2038630247116089\n",
      "Training batch loss 132435 0.1960001289844513\n",
      "Training batch loss 132455 0.19405975937843323\n",
      "Training batch loss 132475 0.20012935996055603\n",
      "Training batch loss 132495 0.20398616790771484\n",
      "Training batch loss 132515 0.19369086623191833\n",
      "Training batch loss 132535 0.20635294914245605\n",
      "Training batch loss 132555 0.1984732449054718\n",
      "Training batch loss 132575 0.20124468207359314\n",
      "Training batch loss 132595 0.18851247429847717\n",
      "Training batch loss 132615 0.19849780201911926\n",
      "Training batch loss 132635 0.20473094284534454\n",
      "Training batch loss 132655 0.19965031743049622\n",
      "Training batch loss 132675 0.20437705516815186\n",
      "Training batch loss 132695 0.19127696752548218\n",
      "Training batch loss 132715 0.1965131014585495\n",
      "Training batch loss 132735 0.195701465010643\n",
      "Training batch loss 132755 0.19478952884674072\n",
      "Training batch loss 132775 0.201440691947937\n",
      "Training epoch loss 97 0.19857197999954224\n",
      "Test loss during training 97 0.1976657658815384\n",
      "Test accuracy during training 97 0.0\n",
      "Training batch loss 132790 0.19170010089874268\n",
      "Training batch loss 132810 0.19589971005916595\n",
      "Training batch loss 132830 0.20131264626979828\n",
      "Training batch loss 132850 0.1983058601617813\n",
      "Training batch loss 132870 0.1991705298423767\n",
      "Training batch loss 132890 0.2009793221950531\n",
      "Training batch loss 132910 0.1980592906475067\n",
      "Training batch loss 132930 0.19715023040771484\n",
      "Training batch loss 132950 0.19260242581367493\n",
      "Training batch loss 132970 0.19880345463752747\n",
      "Training batch loss 132990 0.19632495939731598\n",
      "Training batch loss 133010 0.19872212409973145\n",
      "Training batch loss 133030 0.2011108696460724\n",
      "Training batch loss 133050 0.19334366917610168\n",
      "Training batch loss 133070 0.2011612057685852\n",
      "Training batch loss 133090 0.18688777089118958\n",
      "Training batch loss 133110 0.19738690555095673\n",
      "Training batch loss 133130 0.20211857557296753\n",
      "Training batch loss 133150 0.19543080031871796\n",
      "Training batch loss 133170 0.20585793256759644\n",
      "Training batch loss 133190 0.19723135232925415\n",
      "Training batch loss 133210 0.19044041633605957\n",
      "Training batch loss 133230 0.20283818244934082\n",
      "Training batch loss 133250 0.19655489921569824\n",
      "Training batch loss 133270 0.1955663561820984\n",
      "Training batch loss 133290 0.20146483182907104\n",
      "Training batch loss 133310 0.1988999843597412\n",
      "Training batch loss 133330 0.19841943681240082\n",
      "Training batch loss 133350 0.20389148592948914\n",
      "Training batch loss 133370 0.19226862490177155\n",
      "Training batch loss 133390 0.2035389244556427\n",
      "Training batch loss 133410 0.19257719814777374\n",
      "Training batch loss 133430 0.1931302547454834\n",
      "Training batch loss 133450 0.19947470724582672\n",
      "Training batch loss 133470 0.19628626108169556\n",
      "Training batch loss 133490 0.20586371421813965\n",
      "Training batch loss 133510 0.1968308389186859\n",
      "Training batch loss 133530 0.20596987009048462\n",
      "Training batch loss 133550 0.1997758150100708\n",
      "Training batch loss 133570 0.20147401094436646\n",
      "Training batch loss 133590 0.19950464367866516\n",
      "Training batch loss 133610 0.1979617178440094\n",
      "Training batch loss 133630 0.20617517828941345\n",
      "Training batch loss 133650 0.2074558138847351\n",
      "Training batch loss 133670 0.19798772037029266\n",
      "Training batch loss 133690 0.20131948590278625\n",
      "Training batch loss 133710 0.21275150775909424\n",
      "Training batch loss 133730 0.19735084474086761\n",
      "Training batch loss 133750 0.2057875394821167\n",
      "Training batch loss 133770 0.2038630247116089\n",
      "Training batch loss 133790 0.1960001289844513\n",
      "Training batch loss 133810 0.19405975937843323\n",
      "Training batch loss 133830 0.20012935996055603\n",
      "Training batch loss 133850 0.20398616790771484\n",
      "Training batch loss 133870 0.19369086623191833\n",
      "Training batch loss 133890 0.20635294914245605\n",
      "Training batch loss 133910 0.1984732449054718\n",
      "Training batch loss 133930 0.20124468207359314\n",
      "Training batch loss 133950 0.18851247429847717\n",
      "Training batch loss 133970 0.19849780201911926\n",
      "Training batch loss 133990 0.20473094284534454\n",
      "Training batch loss 134010 0.19965031743049622\n",
      "Training batch loss 134030 0.20437705516815186\n",
      "Training batch loss 134050 0.19127696752548218\n",
      "Training batch loss 134070 0.1965131014585495\n",
      "Training batch loss 134090 0.195701465010643\n",
      "Training batch loss 134110 0.19478952884674072\n",
      "Training batch loss 134130 0.201440691947937\n",
      "Training epoch loss 98 0.19857197999954224\n",
      "Test loss during training 98 0.1976657658815384\n",
      "Test accuracy during training 98 0.0\n",
      "Training batch loss 134145 0.19170010089874268\n",
      "Training batch loss 134165 0.19589971005916595\n",
      "Training batch loss 134185 0.20131264626979828\n",
      "Training batch loss 134205 0.1983058601617813\n",
      "Training batch loss 134225 0.1991705298423767\n",
      "Training batch loss 134245 0.2009793221950531\n",
      "Training batch loss 134265 0.1980592906475067\n",
      "Training batch loss 134285 0.19715023040771484\n",
      "Training batch loss 134305 0.19260242581367493\n",
      "Training batch loss 134325 0.19880345463752747\n",
      "Training batch loss 134345 0.19632495939731598\n",
      "Training batch loss 134365 0.19872212409973145\n",
      "Training batch loss 134385 0.2011108696460724\n",
      "Training batch loss 134405 0.19334366917610168\n",
      "Training batch loss 134425 0.2011612057685852\n",
      "Training batch loss 134445 0.18688777089118958\n",
      "Training batch loss 134465 0.19738690555095673\n",
      "Training batch loss 134485 0.20211857557296753\n",
      "Training batch loss 134505 0.19543080031871796\n",
      "Training batch loss 134525 0.20585793256759644\n",
      "Training batch loss 134545 0.19723135232925415\n",
      "Training batch loss 134565 0.19044041633605957\n",
      "Training batch loss 134585 0.20283818244934082\n",
      "Training batch loss 134605 0.19655489921569824\n",
      "Training batch loss 134625 0.1955663561820984\n",
      "Training batch loss 134645 0.20146483182907104\n",
      "Training batch loss 134665 0.1988999843597412\n",
      "Training batch loss 134685 0.19841943681240082\n",
      "Training batch loss 134705 0.20389148592948914\n",
      "Training batch loss 134725 0.19226862490177155\n",
      "Training batch loss 134745 0.2035389244556427\n",
      "Training batch loss 134765 0.19257719814777374\n",
      "Training batch loss 134785 0.1931302547454834\n",
      "Training batch loss 134805 0.19947470724582672\n",
      "Training batch loss 134825 0.19628626108169556\n",
      "Training batch loss 134845 0.20586371421813965\n",
      "Training batch loss 134865 0.1968308389186859\n",
      "Training batch loss 134885 0.20596987009048462\n",
      "Training batch loss 134905 0.1997758150100708\n",
      "Training batch loss 134925 0.20147401094436646\n",
      "Training batch loss 134945 0.19950464367866516\n",
      "Training batch loss 134965 0.1979617178440094\n",
      "Training batch loss 134985 0.20617517828941345\n",
      "Training batch loss 135005 0.2074558138847351\n",
      "Training batch loss 135025 0.19798772037029266\n",
      "Training batch loss 135045 0.20131948590278625\n",
      "Training batch loss 135065 0.21275150775909424\n",
      "Training batch loss 135085 0.19735084474086761\n",
      "Training batch loss 135105 0.2057875394821167\n",
      "Training batch loss 135125 0.2038630247116089\n",
      "Training batch loss 135145 0.1960001289844513\n",
      "Training batch loss 135165 0.19405975937843323\n",
      "Training batch loss 135185 0.20012935996055603\n",
      "Training batch loss 135205 0.20398616790771484\n",
      "Training batch loss 135225 0.19369086623191833\n",
      "Training batch loss 135245 0.20635294914245605\n",
      "Training batch loss 135265 0.1984732449054718\n",
      "Training batch loss 135285 0.20124468207359314\n",
      "Training batch loss 135305 0.18851247429847717\n",
      "Training batch loss 135325 0.19849780201911926\n",
      "Training batch loss 135345 0.20473094284534454\n",
      "Training batch loss 135365 0.19965031743049622\n",
      "Training batch loss 135385 0.20437705516815186\n",
      "Training batch loss 135405 0.19127696752548218\n",
      "Training batch loss 135425 0.1965131014585495\n",
      "Training batch loss 135445 0.195701465010643\n",
      "Training batch loss 135465 0.19478952884674072\n",
      "Training batch loss 135485 0.201440691947937\n",
      "Training epoch loss 99 0.19857197999954224\n",
      "Test loss during training 99 0.1976657658815384\n",
      "Test accuracy during training 99 0.0\n"
     ]
    }
   ],
   "source": [
    "#load\n",
    "name = 'mb-gpu_longer_random'\n",
    "in_path = os.path.join('logs/',name)\n",
    "\n",
    "out_path = os.path.join('clean_logs/',name)\n",
    "\n",
    "writer = Tensorboard(out_path)\n",
    "\n",
    "for event_name in os.listdir(in_path):\n",
    "    event_path = os.path.join(in_path, event_name)\n",
    "    for event in tf.train.summary_iterator(event_path):\n",
    "        #print(event)\n",
    "        #print(event.summary.value)\n",
    "        #print(event.summary.value.get('tag'))\n",
    "        step = event.step\n",
    "        val = event.summary.value\n",
    "        if val:\n",
    "            if len(val)>1:\n",
    "                print(val)\n",
    "                raise ValueError\n",
    "            val = val[0]\n",
    "            \n",
    "            #print('VALEUR ')\n",
    "            tag =val.tag\n",
    "            \n",
    "            if tag in renaming:\n",
    "                tag = renaming[tag]\n",
    "            if tag == 'Test loss during training':\n",
    "                value = val.simple_value * 64\n",
    "            else : \n",
    "                value = val.simple_value\n",
    "            print(tag, step, value)\n",
    "            writer.log_scalar(tag, value, step)\n",
    "            #print('END_VALEUR')\n",
    "            \n",
    "        \n",
    "        #for value in event.summary.value:\n",
    "         #   print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/unaligned_hard/\n",
      "['events.out.tfevents.1555339982.mb-gpu1']\n",
      "logs/unaligned_hard/events.out.tfevents.1555339982.mb-gpu1\n",
      "wall_time: 1555339982.4510272\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.3296055495738983\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555339989.6706326\n",
      "step: 20\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1689498871564865\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340000.5548882\n",
      "step: 40\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.15553104877471924\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340010.787094\n",
      "step: 60\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1492927372455597\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340019.920911\n",
      "step: 80\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1396157592535019\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340027.9420524\n",
      "step: 100\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.13492825627326965\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340038.058603\n",
      "step: 120\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.14473870396614075\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340047.0315223\n",
      "step: 140\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.13603034615516663\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340054.8169603\n",
      "step: 160\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.13006414473056793\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340064.96241\n",
      "step: 180\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.12878704071044922\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340075.7356412\n",
      "step: 200\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1293000876903534\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340084.6328182\n",
      "step: 220\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.13124409317970276\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340093.6443083\n",
      "step: 240\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.13270002603530884\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340105.3370872\n",
      "step: 260\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.12001283466815948\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340113.8799942\n",
      "step: 280\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1220940500497818\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340123.5137918\n",
      "step: 300\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.13213077187538147\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340130.9760287\n",
      "step: 320\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.12367922067642212\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340140.155929\n",
      "step: 340\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.12901127338409424\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340149.2372787\n",
      "step: 360\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.13020825386047363\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340158.8883753\n",
      "step: 380\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.12543785572052002\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340169.9544847\n",
      "step: 400\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1322317123413086\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340179.1068745\n",
      "step: 420\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.12485592812299728\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340186.827446\n",
      "step: 440\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.12114793062210083\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340196.753676\n",
      "step: 460\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1250225305557251\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340206.9565368\n",
      "step: 480\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1093345433473587\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340215.9759374\n",
      "step: 500\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.12244753539562225\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340226.4135232\n",
      "step: 520\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.12302927672863007\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340235.6510074\n",
      "step: 540\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.12766170501708984\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340246.7771301\n",
      "step: 560\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11551272124052048\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340254.4540334\n",
      "step: 580\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1083197146654129\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340264.2652967\n",
      "step: 600\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.12443186342716217\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340273.0116572\n",
      "step: 620\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1279846578836441\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340281.7990818\n",
      "step: 640\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.12399375438690186\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340292.2245245\n",
      "step: 660\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.12551862001419067\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340302.016807\n",
      "step: 680\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11992645263671875\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340311.2262974\n",
      "step: 700\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.12311750650405884\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340320.9879076\n",
      "step: 720\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11815652996301651\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340328.4138553\n",
      "step: 740\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11459831893444061\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340339.7538009\n",
      "step: 760\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11594341695308685\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340349.7083528\n",
      "step: 780\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11293067038059235\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340360.8062894\n",
      "step: 800\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.12085972726345062\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340371.2080774\n",
      "step: 820\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.12106704711914062\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340380.514214\n",
      "step: 840\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11088760197162628\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340389.6474807\n",
      "step: 860\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11392396688461304\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340398.4902453\n",
      "step: 880\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10409025847911835\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340408.491987\n",
      "step: 900\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.12106326222419739\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340417.1015718\n",
      "step: 920\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10819061845541\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340427.107863\n",
      "step: 940\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11414140462875366\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340436.993202\n",
      "step: 960\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.12116748094558716\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340446.736038\n",
      "step: 980\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.12333483248949051\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340454.5997589\n",
      "step: 1000\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10318093001842499\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340464.556219\n",
      "step: 1020\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11253230273723602\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340473.013555\n",
      "step: 1040\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10017415881156921\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340481.671776\n",
      "step: 1060\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10963800549507141\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340489.4468586\n",
      "step: 1080\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11589539051055908\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340498.8232586\n",
      "step: 1100\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1110009178519249\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340508.0274577\n",
      "step: 1120\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11802001297473907\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340519.7170656\n",
      "step: 1140\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.123097263276577\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340528.2894676\n",
      "step: 1160\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11823609471321106\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340537.5427773\n",
      "step: 1180\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1027715876698494\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340550.0978994\n",
      "step: 1200\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10671178996562958\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340558.2368786\n",
      "step: 1220\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10724551975727081\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340569.7036462\n",
      "step: 1240\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11214760690927505\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340579.6554399\n",
      "step: 1260\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1208345890045166\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340588.349223\n",
      "step: 1280\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11352835595607758\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340598.8181334\n",
      "step: 1300\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10343284904956818\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340606.5052245\n",
      "step: 1320\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10782983899116516\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340615.9132395\n",
      "step: 1340\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09806771576404572\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555340619.785146\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.12133751809597015\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341229.868146\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0008665998466312885\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341229.8689592\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341239.9717512\n",
      "step: 1355\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1064678281545639\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341245.398651\n",
      "step: 1375\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1049375981092453\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341250.75463\n",
      "step: 1395\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09414434432983398\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341256.108517\n",
      "step: 1415\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10666773468255997\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341261.4784453\n",
      "step: 1435\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11891902983188629\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341266.8316352\n",
      "step: 1455\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11003921926021576\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341272.194185\n",
      "step: 1475\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10041718184947968\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341277.5524685\n",
      "step: 1495\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10767915844917297\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341282.9159906\n",
      "step: 1515\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09718719124794006\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341288.265564\n",
      "step: 1535\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10878092050552368\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341293.6303844\n",
      "step: 1555\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10176326334476471\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341299.0006886\n",
      "step: 1575\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10004307329654694\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341304.3802617\n",
      "step: 1595\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09611953794956207\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341309.7462053\n",
      "step: 1615\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10519159585237503\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341315.1289306\n",
      "step: 1635\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10950011014938354\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341320.5064976\n",
      "step: 1655\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11603513360023499\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341325.9072323\n",
      "step: 1675\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1032138392329216\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341331.3035002\n",
      "step: 1695\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09752762317657471\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341336.7045517\n",
      "step: 1715\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10329125076532364\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341342.102072\n",
      "step: 1735\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10824975371360779\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341347.4889753\n",
      "step: 1755\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10810881853103638\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341352.892071\n",
      "step: 1775\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11087220907211304\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341358.2950864\n",
      "step: 1795\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1091945469379425\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341363.692065\n",
      "step: 1815\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10053427517414093\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341369.0894945\n",
      "step: 1835\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10459209233522415\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341374.4879076\n",
      "step: 1855\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10157369822263718\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341379.8824072\n",
      "step: 1875\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1077212244272232\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341385.2845786\n",
      "step: 1895\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10342900454998016\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341390.6887376\n",
      "step: 1915\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10527189075946808\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341396.0915473\n",
      "step: 1935\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09994284063577652\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341401.494266\n",
      "step: 1955\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09261266142129898\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341406.9047635\n",
      "step: 1975\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09483054280281067\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341412.2981467\n",
      "step: 1995\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11269574612379074\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341417.679345\n",
      "step: 2015\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10529729723930359\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341423.0814555\n",
      "step: 2035\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10279148817062378\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341428.4828405\n",
      "step: 2055\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09636826813220978\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341433.862917\n",
      "step: 2075\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10808590054512024\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341439.2570796\n",
      "step: 2095\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10154476016759872\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341444.642432\n",
      "step: 2115\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09090660512447357\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341450.030594\n",
      "step: 2135\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09352247416973114\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341455.4152741\n",
      "step: 2155\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0893387421965599\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341460.8010602\n",
      "step: 2175\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10463201254606247\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341466.175543\n",
      "step: 2195\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1011793464422226\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341471.5684204\n",
      "step: 2215\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09658633172512054\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341476.976346\n",
      "step: 2235\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09950381517410278\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341482.3678308\n",
      "step: 2255\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09175937622785568\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341487.765723\n",
      "step: 2275\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10271073132753372\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341493.1592412\n",
      "step: 2295\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11065950989723206\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341498.552566\n",
      "step: 2315\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09558041393756866\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341503.9548686\n",
      "step: 2335\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11035553365945816\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341509.3656921\n",
      "step: 2355\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09986326098442078\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341514.7758305\n",
      "step: 2375\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09320767968893051\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341520.1838615\n",
      "step: 2395\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11413995921611786\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341525.585545\n",
      "step: 2415\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09573002904653549\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341530.986458\n",
      "step: 2435\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09842789173126221\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341536.3783998\n",
      "step: 2455\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10535281896591187\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341541.763989\n",
      "step: 2475\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10196232795715332\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341547.1591148\n",
      "step: 2495\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10915791988372803\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341552.5574422\n",
      "step: 2515\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10596823692321777\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341557.9375103\n",
      "step: 2535\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09252840280532837\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341563.3233483\n",
      "step: 2555\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08797963708639145\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341568.7139647\n",
      "step: 2575\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10361979901790619\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341574.1088848\n",
      "step: 2595\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09777633100748062\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341579.505067\n",
      "step: 2615\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09395590424537659\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341584.8932061\n",
      "step: 2635\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10435567051172256\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341590.2820156\n",
      "step: 2655\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1086256355047226\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341595.6637845\n",
      "step: 2675\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0998491421341896\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341601.0417798\n",
      "step: 2695\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11031979322433472\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341604.9451473\n",
      "step: 1\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.103725865483284\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341641.0161796\n",
      "step: 1\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0008087402675300837\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341641.0170546\n",
      "step: 1\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341651.2165046\n",
      "step: 2710\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10547152161598206\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341656.641006\n",
      "step: 2730\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0986643061041832\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341662.0187225\n",
      "step: 2750\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10031247138977051\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341667.4134212\n",
      "step: 2770\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08714345097541809\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341672.8042657\n",
      "step: 2790\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10660402476787567\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341678.205382\n",
      "step: 2810\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09664657711982727\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341683.5823276\n",
      "step: 2830\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0909789502620697\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341688.961168\n",
      "step: 2850\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09102116525173187\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341694.3349457\n",
      "step: 2870\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11089065670967102\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341699.7060854\n",
      "step: 2890\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09395530074834824\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341705.071827\n",
      "step: 2910\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09377658367156982\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341710.4425733\n",
      "step: 2930\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11256441473960876\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341715.8213985\n",
      "step: 2950\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10137136280536652\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341725.1629906\n",
      "step: 2970\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1015271544456482\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341733.0719523\n",
      "step: 2990\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10122402757406235\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341738.4413686\n",
      "step: 3010\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10714887082576752\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341743.8113365\n",
      "step: 3030\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09588602185249329\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341749.188216\n",
      "step: 3050\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10154639184474945\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341754.569694\n",
      "step: 3070\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0908598005771637\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341759.9661832\n",
      "step: 3090\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1028498187661171\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341765.3889163\n",
      "step: 3110\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10411913692951202\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341770.7807648\n",
      "step: 3130\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10229648649692535\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341776.1733599\n",
      "step: 3150\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1061842292547226\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341788.424637\n",
      "step: 3170\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10255859792232513\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341800.3185859\n",
      "step: 3190\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09244953095912933\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341805.7113807\n",
      "step: 3210\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10059577226638794\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341811.1378524\n",
      "step: 3230\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1031133383512497\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341816.6295545\n",
      "step: 3250\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09689003229141235\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341822.0316272\n",
      "step: 3270\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09537287801504135\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341830.0436287\n",
      "step: 3290\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08984465897083282\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341838.7795932\n",
      "step: 3310\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09598634392023087\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341847.5429385\n",
      "step: 3330\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10066352039575577\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341856.3346841\n",
      "step: 3350\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08834072202444077\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341865.1067078\n",
      "step: 3370\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09442897140979767\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341873.926485\n",
      "step: 3390\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10042104125022888\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341882.7317822\n",
      "step: 3410\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09214727580547333\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341891.555538\n",
      "step: 3430\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0944470465183258\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341900.3688102\n",
      "step: 3450\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09570349752902985\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341909.1925027\n",
      "step: 3470\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08082704246044159\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341918.018014\n",
      "step: 3490\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10195977985858917\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341926.8635623\n",
      "step: 3510\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10327902436256409\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341935.7171195\n",
      "step: 3530\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09718509018421173\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341943.7724173\n",
      "step: 3550\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09938620030879974\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341952.523674\n",
      "step: 3570\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10475969314575195\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341961.305912\n",
      "step: 3590\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10671170055866241\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341970.084506\n",
      "step: 3610\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0908183753490448\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341978.8687167\n",
      "step: 3630\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10921181738376617\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341987.6992261\n",
      "step: 3650\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.1036999300122261\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555341996.4705644\n",
      "step: 3670\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09354541450738907\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342005.260371\n",
      "step: 3690\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10501138865947723\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342014.0357382\n",
      "step: 3710\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09319043159484863\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342022.8414927\n",
      "step: 3730\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.112484872341156\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342031.6296916\n",
      "step: 3750\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10302488505840302\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342040.4236107\n",
      "step: 3770\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10388433933258057\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342049.2327347\n",
      "step: 3790\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10778355598449707\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342058.0513868\n",
      "step: 3810\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09037981927394867\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342066.8726895\n",
      "step: 3830\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.11232871562242508\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342075.6573007\n",
      "step: 3850\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09524862468242645\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342084.463022\n",
      "step: 3870\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08483631908893585\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342093.2283847\n",
      "step: 3890\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10542594641447067\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342102.0278742\n",
      "step: 3910\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09450750052928925\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342110.7958856\n",
      "step: 3930\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08588393032550812\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342119.6010132\n",
      "step: 3950\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0937945544719696\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342128.4236119\n",
      "step: 3970\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09028572589159012\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342137.2165718\n",
      "step: 3990\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09913863241672516\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342146.0093777\n",
      "step: 4010\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09776308387517929\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342154.8516145\n",
      "step: 4030\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09414584189653397\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342163.7018886\n",
      "step: 4050\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09208821505308151\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342170.049613\n",
      "step: 2\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.09782230854034424\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342224.1458347\n",
      "step: 2\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0008030991884879768\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342224.1466703\n",
      "step: 2\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342234.4897623\n",
      "step: 4065\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09505760669708252\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342243.2623632\n",
      "step: 4085\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08954553306102753\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342252.0403194\n",
      "step: 4105\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09348757565021515\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342260.8350372\n",
      "step: 4125\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08607222139835358\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342269.6451163\n",
      "step: 4145\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08838984370231628\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342278.3901103\n",
      "step: 4165\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08577758073806763\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342287.1777365\n",
      "step: 4185\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09268796443939209\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342296.0106487\n",
      "step: 4205\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08443945646286011\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342304.5110154\n",
      "step: 4225\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09318111836910248\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342310.4558196\n",
      "step: 4245\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09697379171848297\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342316.3943377\n",
      "step: 4265\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0922042727470398\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342322.3357148\n",
      "step: 4285\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08730188012123108\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342328.2670417\n",
      "step: 4305\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09691248834133148\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342334.2043796\n",
      "step: 4325\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09861265867948532\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342340.138017\n",
      "step: 4345\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10364031791687012\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342346.0828109\n",
      "step: 4365\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0864739716053009\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342352.011188\n",
      "step: 4385\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08719459176063538\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342357.9416258\n",
      "step: 4405\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10250678658485413\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342363.8774016\n",
      "step: 4425\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0891825258731842\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342369.813409\n",
      "step: 4445\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09330914169549942\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342375.7545445\n",
      "step: 4465\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10359960049390793\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342381.6850865\n",
      "step: 4485\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08529970049858093\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342387.6250494\n",
      "step: 4505\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08025192469358444\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342393.5721831\n",
      "step: 4525\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08526933938264847\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342399.511906\n",
      "step: 4545\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09907364845275879\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342405.4500935\n",
      "step: 4565\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09738267958164215\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342411.3820984\n",
      "step: 4585\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09046909213066101\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342417.32264\n",
      "step: 4605\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08985985815525055\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342423.261749\n",
      "step: 4625\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08881054818630219\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342429.2042913\n",
      "step: 4645\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08033590763807297\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342435.1376026\n",
      "step: 4665\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09459617733955383\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342441.076224\n",
      "step: 4685\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09474842250347137\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342447.0111337\n",
      "step: 4705\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09364106506109238\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342452.9510114\n",
      "step: 4725\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08941523730754852\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342458.890617\n",
      "step: 4745\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0939997062087059\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342464.8429117\n",
      "step: 4765\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08851601183414459\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342470.781267\n",
      "step: 4785\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09370797872543335\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342476.724277\n",
      "step: 4805\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09135033190250397\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342482.6599836\n",
      "step: 4825\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09669838845729828\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342488.5880911\n",
      "step: 4845\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08645294606685638\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342494.529725\n",
      "step: 4865\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08394917845726013\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342500.4605944\n",
      "step: 4885\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09497404098510742\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342506.3970191\n",
      "step: 4905\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09126785397529602\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342512.3396475\n",
      "step: 4925\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10063938796520233\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342518.2821033\n",
      "step: 4945\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09509880095720291\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342524.2206948\n",
      "step: 4965\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08813151717185974\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342530.1542768\n",
      "step: 4985\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0978030115365982\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342536.1043901\n",
      "step: 5005\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09532377123832703\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342542.0325863\n",
      "step: 5025\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09256334602832794\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342547.9726615\n",
      "step: 5045\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09453294426202774\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342553.9151852\n",
      "step: 5065\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10262913256883621\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342559.867055\n",
      "step: 5085\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09396447986364365\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342565.8070953\n",
      "step: 5105\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08394074440002441\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342571.745694\n",
      "step: 5125\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08623401820659637\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342577.691403\n",
      "step: 5145\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08934343606233597\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342583.628518\n",
      "step: 5165\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0991656631231308\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342589.578316\n",
      "step: 5185\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09685450792312622\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342595.5216444\n",
      "step: 5205\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10396721214056015\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342601.458698\n",
      "step: 5225\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09339674562215805\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342607.4055648\n",
      "step: 5245\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0969349816441536\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342613.3525276\n",
      "step: 5265\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09065704047679901\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342619.2908423\n",
      "step: 5285\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08899609744548798\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342625.2397895\n",
      "step: 5305\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0876653641462326\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342631.1917653\n",
      "step: 5325\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08627600967884064\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342637.1331034\n",
      "step: 5345\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08407694101333618\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342643.0691228\n",
      "step: 5365\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10248637199401855\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342649.0027797\n",
      "step: 5385\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08808690309524536\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342654.938427\n",
      "step: 5405\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09876318275928497\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342659.2361073\n",
      "step: 3\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.09314033389091492\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342698.3190424\n",
      "step: 3\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0007298414129763842\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342698.3192348\n",
      "step: 3\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342708.5705516\n",
      "step: 5420\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09187060594558716\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342714.5065708\n",
      "step: 5440\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08676528185606003\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342720.4278018\n",
      "step: 5460\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10114883631467819\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342726.3508868\n",
      "step: 5480\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08034263551235199\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342732.2688742\n",
      "step: 5500\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.091020368039608\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342738.1901972\n",
      "step: 5520\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09266389161348343\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342744.1175935\n",
      "step: 5540\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08236756920814514\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342750.0452766\n",
      "step: 5560\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08308378607034683\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342755.965108\n",
      "step: 5580\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0913839340209961\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342761.8982172\n",
      "step: 5600\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09182356297969818\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342767.826454\n",
      "step: 5620\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08583619445562363\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342773.7526271\n",
      "step: 5640\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09097368270158768\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342779.6654532\n",
      "step: 5660\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0743517279624939\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342785.594305\n",
      "step: 5680\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09475398063659668\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342791.520097\n",
      "step: 5700\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08540098369121552\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342797.4601462\n",
      "step: 5720\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08070306479930878\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342803.3919096\n",
      "step: 5740\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0919305831193924\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342809.3251028\n",
      "step: 5760\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08921824395656586\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342815.2635863\n",
      "step: 5780\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08242041617631912\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342821.2030656\n",
      "step: 5800\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.077668197453022\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342827.1393962\n",
      "step: 5820\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07795314490795135\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342833.081047\n",
      "step: 5840\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08159859478473663\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342839.0162942\n",
      "step: 5860\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09238898754119873\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342844.9501364\n",
      "step: 5880\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08301672339439392\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342850.8834288\n",
      "step: 5900\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09823015332221985\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342856.8220043\n",
      "step: 5920\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08236333727836609\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342862.7632005\n",
      "step: 5940\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08967877924442291\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342868.687351\n",
      "step: 5960\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09476655721664429\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342874.6197135\n",
      "step: 5980\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08861929178237915\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342880.5545647\n",
      "step: 6000\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08599977940320969\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342886.4964082\n",
      "step: 6020\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10235144942998886\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342892.4364603\n",
      "step: 6040\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08569008111953735\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342898.3786714\n",
      "step: 6060\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08597347140312195\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342904.3129725\n",
      "step: 6080\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0871543362736702\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342910.2566617\n",
      "step: 6100\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09230059385299683\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342916.1886485\n",
      "step: 6120\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08628114312887192\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342922.1295624\n",
      "step: 6140\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08957705646753311\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342928.0616655\n",
      "step: 6160\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08084860444068909\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342934.0145297\n",
      "step: 6180\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09011869877576828\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342939.95424\n",
      "step: 6200\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08944053947925568\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342945.8903852\n",
      "step: 6220\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08205042779445648\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342951.8294325\n",
      "step: 6240\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08535130321979523\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342957.7738168\n",
      "step: 6260\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08199302852153778\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342963.714943\n",
      "step: 6280\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0829036608338356\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342969.664343\n",
      "step: 6300\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08670982718467712\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342975.6058474\n",
      "step: 6320\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09043340384960175\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342981.5521097\n",
      "step: 6340\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09262154996395111\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342987.4953296\n",
      "step: 6360\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08734335005283356\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342993.4348376\n",
      "step: 6380\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0881785899400711\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555342999.3750653\n",
      "step: 6400\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08758336305618286\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343005.3144355\n",
      "step: 6420\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08488918840885162\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343011.2533355\n",
      "step: 6440\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08283627033233643\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343017.2013428\n",
      "step: 6460\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08000265061855316\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343023.1506934\n",
      "step: 6480\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08613978326320648\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343029.0909383\n",
      "step: 6500\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.10269398242235184\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343035.0330992\n",
      "step: 6520\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08085120469331741\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343040.9779239\n",
      "step: 6540\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09181277453899384\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343046.926296\n",
      "step: 6560\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09031794965267181\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343052.8663294\n",
      "step: 6580\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08839676529169083\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343058.8137248\n",
      "step: 6600\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08928313106298447\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343064.7391667\n",
      "step: 6620\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08463101834058762\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343070.6869855\n",
      "step: 6640\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08998459577560425\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343076.6322336\n",
      "step: 6660\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08077357709407806\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343082.5784063\n",
      "step: 6680\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09086529910564423\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343088.5222466\n",
      "step: 6700\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09304387867450714\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343094.4715774\n",
      "step: 6720\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09078168869018555\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343100.4083936\n",
      "step: 6740\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08963951468467712\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343106.350587\n",
      "step: 6760\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09042732417583466\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343110.6790748\n",
      "step: 4\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.08896736055612564\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343149.7517126\n",
      "step: 4\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0007367677171714604\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343149.7522974\n",
      "step: 4\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343151.4081516\n",
      "step: 6775\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08739634603261948\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343157.3993516\n",
      "step: 6795\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07600010931491852\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343163.3142307\n",
      "step: 6815\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08506850898265839\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343169.25501\n",
      "step: 6835\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08580595254898071\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343175.1832118\n",
      "step: 6855\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07896530628204346\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343181.1187658\n",
      "step: 6875\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0808323472738266\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343187.047895\n",
      "step: 6895\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08876144886016846\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343192.9872832\n",
      "step: 6915\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08578087389469147\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343198.9203486\n",
      "step: 6935\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07739654183387756\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343204.8537912\n",
      "step: 6955\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08746218681335449\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343210.784808\n",
      "step: 6975\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08288304507732391\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343216.7172499\n",
      "step: 6995\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07881712913513184\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343222.6514552\n",
      "step: 7015\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09056821465492249\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343228.5828187\n",
      "step: 7035\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08188490569591522\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343234.5302665\n",
      "step: 7055\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09498915076255798\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343240.466322\n",
      "step: 7075\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08082203567028046\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343246.4034169\n",
      "step: 7095\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.088287353515625\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343252.3408191\n",
      "step: 7115\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07833226770162582\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343258.285849\n",
      "step: 7135\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08094829320907593\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343264.2216566\n",
      "step: 7155\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08301195502281189\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343270.1596768\n",
      "step: 7175\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09338857233524323\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343276.0803473\n",
      "step: 7195\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09267014265060425\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343282.010904\n",
      "step: 7215\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09003602713346481\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343287.9410393\n",
      "step: 7235\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07738050818443298\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343293.87128\n",
      "step: 7255\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08558414876461029\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343299.6788118\n",
      "step: 7275\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09309324622154236\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343305.6186526\n",
      "step: 7295\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08533740043640137\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343311.554364\n",
      "step: 7315\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08631587028503418\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343317.4911296\n",
      "step: 7335\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08815440535545349\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343323.4252005\n",
      "step: 7355\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08786998689174652\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343329.355347\n",
      "step: 7375\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08911699056625366\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343335.2867808\n",
      "step: 7395\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08953405171632767\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343341.2191403\n",
      "step: 7415\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09318951517343521\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343347.151148\n",
      "step: 7435\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08723752200603485\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343353.0851898\n",
      "step: 7455\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07959189265966415\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343359.0272045\n",
      "step: 7475\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09017172455787659\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343364.9672143\n",
      "step: 7495\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09273034334182739\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343370.8999152\n",
      "step: 7515\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09286416321992874\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343376.8442936\n",
      "step: 7535\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07783126831054688\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343382.7842078\n",
      "step: 7555\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07378806918859482\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343388.721918\n",
      "step: 7575\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09351950883865356\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343394.6502125\n",
      "step: 7595\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08261068165302277\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343400.5929272\n",
      "step: 7615\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0907026156783104\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343406.5274312\n",
      "step: 7635\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08217824995517731\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343412.4648426\n",
      "step: 7655\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.087307408452034\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343418.397327\n",
      "step: 7675\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06980420649051666\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343424.3351555\n",
      "step: 7695\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07264123857021332\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343430.2683907\n",
      "step: 7715\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08706837147474289\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343436.2065182\n",
      "step: 7735\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07965561002492905\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343442.1415253\n",
      "step: 7755\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09487833082675934\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343448.0813563\n",
      "step: 7775\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0776592493057251\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343454.0077152\n",
      "step: 7795\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07353604584932327\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343459.9496706\n",
      "step: 7815\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08845704048871994\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343465.8945427\n",
      "step: 7835\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09132614731788635\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343471.8284101\n",
      "step: 7855\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08594197779893875\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343477.7607186\n",
      "step: 7875\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0821969211101532\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343483.6917105\n",
      "step: 7895\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08461971580982208\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343489.6241462\n",
      "step: 7915\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08592194318771362\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343495.5646696\n",
      "step: 7935\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08068349212408066\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343501.5032227\n",
      "step: 7955\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07295329868793488\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343507.438174\n",
      "step: 7975\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09906809031963348\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343513.3753943\n",
      "step: 7995\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07706578820943832\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343519.2961218\n",
      "step: 8015\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08946094661951065\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343525.2406144\n",
      "step: 8035\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09841776639223099\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343531.1825445\n",
      "step: 8055\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08373977243900299\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343537.1295412\n",
      "step: 8075\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08716043829917908\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343543.0536914\n",
      "step: 8095\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08819511532783508\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343548.9838343\n",
      "step: 8115\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.084781713783741\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343553.288956\n",
      "step: 5\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.08468176424503326\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343592.3252587\n",
      "step: 5\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0006947198999114335\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343592.3260982\n",
      "step: 5\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343602.4822748\n",
      "step: 8130\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07938027381896973\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343608.4390373\n",
      "step: 8150\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08667038381099701\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343614.3441675\n",
      "step: 8170\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08558011054992676\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343620.2534122\n",
      "step: 8190\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07362436503171921\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343626.1638553\n",
      "step: 8210\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07000861316919327\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343632.0799062\n",
      "step: 8230\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07528650760650635\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343637.9835346\n",
      "step: 8250\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07564114779233932\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343643.903882\n",
      "step: 8270\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07802113145589828\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343649.8149703\n",
      "step: 8290\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07102524489164352\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343655.7446623\n",
      "step: 8310\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08226773142814636\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343661.6650858\n",
      "step: 8330\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07043472677469254\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343667.593765\n",
      "step: 8350\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08614708483219147\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343673.5176053\n",
      "step: 8370\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07076042890548706\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343679.4419498\n",
      "step: 8390\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07169616222381592\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343685.3549411\n",
      "step: 8410\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08297690749168396\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343691.295029\n",
      "step: 8430\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07801096886396408\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343697.223468\n",
      "step: 8450\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07958506792783737\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343703.1453505\n",
      "step: 8470\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07060930132865906\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343709.081146\n",
      "step: 8490\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0819665864109993\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343715.0195184\n",
      "step: 8510\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07373035699129105\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343720.9521356\n",
      "step: 8530\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08949971199035645\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343726.8837543\n",
      "step: 8550\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07310403138399124\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343732.8130457\n",
      "step: 8570\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0788063257932663\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343738.7555954\n",
      "step: 8590\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08117170631885529\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343744.6995869\n",
      "step: 8610\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08870187401771545\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343750.6240778\n",
      "step: 8630\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07682619988918304\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343756.5589814\n",
      "step: 8650\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09153077006340027\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343762.4810405\n",
      "step: 8670\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08338207006454468\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343768.4167204\n",
      "step: 8690\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07567441463470459\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343774.3399591\n",
      "step: 8710\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08245167136192322\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343780.2772462\n",
      "step: 8730\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09517695009708405\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343786.2093585\n",
      "step: 8750\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07802551984786987\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343792.1420252\n",
      "step: 8770\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07931962609291077\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343798.0716736\n",
      "step: 8790\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08700541406869888\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343804.0139666\n",
      "step: 8810\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08740594238042831\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343809.9434028\n",
      "step: 8830\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0766441747546196\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343815.8938296\n",
      "step: 8850\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08185264468193054\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343821.8270974\n",
      "step: 8870\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08443920314311981\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343827.7726362\n",
      "step: 8890\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0780685693025589\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343833.7064018\n",
      "step: 8910\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0854882076382637\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343839.6384804\n",
      "step: 8930\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07991667836904526\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343845.572661\n",
      "step: 8950\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08604903519153595\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343851.5077877\n",
      "step: 8970\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08255967497825623\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343857.4356835\n",
      "step: 8990\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07570098340511322\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343863.374215\n",
      "step: 9010\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07072018086910248\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343869.3078518\n",
      "step: 9030\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07454083114862442\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343875.2465513\n",
      "step: 9050\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07272014766931534\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343881.1979833\n",
      "step: 9070\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0679861456155777\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343887.1259353\n",
      "step: 9090\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08853371441364288\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343893.0627294\n",
      "step: 9110\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0856114849448204\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343898.9902542\n",
      "step: 9130\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07858413457870483\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343904.9205854\n",
      "step: 9150\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08512850850820541\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343910.8538084\n",
      "step: 9170\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06915725022554398\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343916.7932067\n",
      "step: 9190\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08316455036401749\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343922.7260144\n",
      "step: 9210\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08977711200714111\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343928.6517377\n",
      "step: 9230\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09053853154182434\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343934.5882156\n",
      "step: 9250\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08850443363189697\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343940.5281644\n",
      "step: 9270\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07922029495239258\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343946.4607985\n",
      "step: 9290\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07845231890678406\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343952.402023\n",
      "step: 9310\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07997632771730423\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343958.339161\n",
      "step: 9330\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08775755017995834\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343964.2761748\n",
      "step: 9350\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08912038803100586\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343970.2079127\n",
      "step: 9370\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.09342877566814423\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343976.1514766\n",
      "step: 9390\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07342700660228729\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343982.0928774\n",
      "step: 9410\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08770940452814102\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343988.0304978\n",
      "step: 9430\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07991008460521698\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343993.9686987\n",
      "step: 9450\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0697590708732605\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555343999.7824342\n",
      "step: 9470\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07407835125923157\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344003.6610553\n",
      "step: 6\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.08082783967256546\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344039.5295532\n",
      "step: 6\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0007015675655566156\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344039.5304937\n",
      "step: 6\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344041.1163194\n",
      "step: 9485\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07953427731990814\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344046.540502\n",
      "step: 9505\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07307104766368866\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344051.89937\n",
      "step: 9525\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07148180902004242\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344057.289688\n",
      "step: 9545\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0707327127456665\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344062.6623945\n",
      "step: 9565\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07136623561382294\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344068.041297\n",
      "step: 9585\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07333990931510925\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344073.4107068\n",
      "step: 9605\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07856898009777069\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344078.809925\n",
      "step: 9625\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07014289498329163\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344084.1995223\n",
      "step: 9645\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07690340280532837\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344089.5930886\n",
      "step: 9665\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08469761908054352\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344094.965895\n",
      "step: 9685\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07925419509410858\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344100.3425164\n",
      "step: 9705\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08585038781166077\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344105.7266967\n",
      "step: 9725\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07332604378461838\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344111.119695\n",
      "step: 9745\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06739818304777145\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344116.51174\n",
      "step: 9765\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07219048589468002\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344121.8987184\n",
      "step: 9785\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08663912117481232\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344127.2728794\n",
      "step: 9805\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07553994655609131\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344132.6527317\n",
      "step: 9825\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07619176059961319\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344138.0286546\n",
      "step: 9845\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07753436267375946\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344143.416412\n",
      "step: 9865\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0669255182147026\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344148.819334\n",
      "step: 9885\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07890045642852783\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344154.2095866\n",
      "step: 9905\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0692138671875\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344159.610902\n",
      "step: 9925\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08503130823373795\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344165.007677\n",
      "step: 9945\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07193417847156525\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344170.3891592\n",
      "step: 9965\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07632222026586533\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344175.760417\n",
      "step: 9985\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07752871513366699\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344181.1389568\n",
      "step: 10005\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08069255203008652\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344186.5155826\n",
      "step: 10025\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06917832046747208\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344191.908553\n",
      "step: 10045\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08323588222265244\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344197.3052754\n",
      "step: 10065\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07780036330223083\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344202.6920712\n",
      "step: 10085\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07040772587060928\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344208.0826943\n",
      "step: 10105\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08065450191497803\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344213.459362\n",
      "step: 10125\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07487206161022186\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344218.8287718\n",
      "step: 10145\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07621787488460541\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344224.2254567\n",
      "step: 10165\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08548305183649063\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344229.6305957\n",
      "step: 10185\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07222072780132294\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344235.0227304\n",
      "step: 10205\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07520253956317902\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344240.4202812\n",
      "step: 10225\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0791153833270073\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344245.8037724\n",
      "step: 10245\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07641393691301346\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344251.185045\n",
      "step: 10265\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07713083922863007\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344256.5793798\n",
      "step: 10285\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07863123714923859\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344261.967727\n",
      "step: 10305\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07930342108011246\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344267.3615787\n",
      "step: 10325\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07859320938587189\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344272.759295\n",
      "step: 10345\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08993876725435257\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344278.1553898\n",
      "step: 10365\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07626967132091522\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344283.5597913\n",
      "step: 10385\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08007998764514923\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344288.958673\n",
      "step: 10405\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08980480581521988\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344294.347577\n",
      "step: 10425\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06871075183153152\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344299.7391114\n",
      "step: 10445\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07673763483762741\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344305.1237955\n",
      "step: 10465\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08427731692790985\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344310.5129812\n",
      "step: 10485\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07081714272499084\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344315.914204\n",
      "step: 10505\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08330391347408295\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344321.3020656\n",
      "step: 10525\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07237549871206284\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344326.679631\n",
      "step: 10545\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06584879755973816\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344332.0500257\n",
      "step: 10565\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08026876300573349\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344337.4395318\n",
      "step: 10585\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06386077404022217\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344342.8406222\n",
      "step: 10605\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08482745289802551\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344348.234747\n",
      "step: 10625\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08055576682090759\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344353.6289866\n",
      "step: 10645\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0777999684214592\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344359.0151107\n",
      "step: 10665\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08165518939495087\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344364.3861673\n",
      "step: 10685\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07484923303127289\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344369.7752464\n",
      "step: 10705\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0897710919380188\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344375.1647816\n",
      "step: 10725\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08167161047458649\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344380.5626776\n",
      "step: 10745\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08602380752563477\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344385.9608862\n",
      "step: 10765\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0750713050365448\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344391.359037\n",
      "step: 10785\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07682998478412628\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344396.7415876\n",
      "step: 10805\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0865040272474289\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344402.120457\n",
      "step: 10825\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07492271065711975\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344406.024529\n",
      "step: 7\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.07697490602731705\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344441.8636494\n",
      "step: 7\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0006827820907346904\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344441.8645742\n",
      "step: 7\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344451.9926808\n",
      "step: 10840\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07904814183712006\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344457.4236138\n",
      "step: 10860\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07897943258285522\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344462.7820985\n",
      "step: 10880\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06468507647514343\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344468.153433\n",
      "step: 10900\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07661496102809906\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344473.542257\n",
      "step: 10920\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0725400522351265\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344478.9379106\n",
      "step: 10940\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0742914080619812\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344484.3160508\n",
      "step: 10960\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07373567670583725\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344489.707646\n",
      "step: 10980\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07395027577877045\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344495.0920422\n",
      "step: 11000\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07864030450582504\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344500.4746082\n",
      "step: 11020\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07138407230377197\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344505.850011\n",
      "step: 11040\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06730138510465622\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344511.2108\n",
      "step: 11060\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07128245383501053\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344516.5695257\n",
      "step: 11080\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07609899342060089\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344521.9319384\n",
      "step: 11100\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07328033447265625\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344527.3189182\n",
      "step: 11120\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06868894398212433\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344532.7149487\n",
      "step: 11140\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06527682393789291\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344538.1035979\n",
      "step: 11160\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07705312222242355\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344543.475236\n",
      "step: 11180\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07395657896995544\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344548.8509462\n",
      "step: 11200\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08204077184200287\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344554.211047\n",
      "step: 11220\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0808018147945404\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344559.58562\n",
      "step: 11240\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08002816140651703\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344564.9743977\n",
      "step: 11260\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07242058962583542\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344570.3647108\n",
      "step: 11280\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06636063754558563\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344575.7626257\n",
      "step: 11300\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07313930243253708\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344581.161068\n",
      "step: 11320\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06831511855125427\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344586.5637143\n",
      "step: 11340\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07575970888137817\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344591.947841\n",
      "step: 11360\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07340697944164276\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344597.3247433\n",
      "step: 11380\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0731339305639267\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344602.6968348\n",
      "step: 11400\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07017774879932404\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344608.0691605\n",
      "step: 11420\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07626326382160187\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344613.4344819\n",
      "step: 11440\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07935157418251038\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344618.8120615\n",
      "step: 11460\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06675893068313599\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344624.2237816\n",
      "step: 11480\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0782821774482727\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344629.6099987\n",
      "step: 11500\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06472013890743256\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344634.9966698\n",
      "step: 11520\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07750949263572693\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344640.383459\n",
      "step: 11540\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0676523819565773\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344645.768674\n",
      "step: 11560\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06923326849937439\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344651.15418\n",
      "step: 11580\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07947466522455215\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344656.5328357\n",
      "step: 11600\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08048699796199799\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344661.9280996\n",
      "step: 11620\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08136188983917236\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344667.336959\n",
      "step: 11640\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0751943290233612\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344672.7237656\n",
      "step: 11660\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0748966783285141\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344678.1043377\n",
      "step: 11680\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07034645974636078\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344683.4802244\n",
      "step: 11700\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07256718724966049\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344688.8534184\n",
      "step: 11720\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07356498390436172\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344694.2426884\n",
      "step: 11740\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06892772018909454\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344699.6370106\n",
      "step: 11760\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07989270985126495\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344705.0379663\n",
      "step: 11780\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07503481954336166\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344710.4291625\n",
      "step: 11800\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08114689588546753\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344715.8126218\n",
      "step: 11820\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06629355251789093\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344721.1905205\n",
      "step: 11840\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07358630001544952\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344726.57598\n",
      "step: 11860\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08326256275177002\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344731.967452\n",
      "step: 11880\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0753239318728447\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344737.353035\n",
      "step: 11900\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06907275319099426\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344742.7632387\n",
      "step: 11920\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07323447614908218\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344748.1475668\n",
      "step: 11940\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07091452181339264\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344753.5309315\n",
      "step: 11960\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07060281932353973\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344758.9085808\n",
      "step: 11980\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07036717981100082\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344764.2848125\n",
      "step: 12000\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06378702819347382\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344769.6663427\n",
      "step: 12020\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07648449391126633\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344775.0735023\n",
      "step: 12040\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07074259221553802\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344780.4694045\n",
      "step: 12060\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07031891494989395\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344785.8527696\n",
      "step: 12080\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07241091132164001\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344791.240799\n",
      "step: 12100\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06980736553668976\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344796.6235702\n",
      "step: 12120\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07664524763822556\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344802.007637\n",
      "step: 12140\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07020213454961777\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344807.3945434\n",
      "step: 12160\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07643762975931168\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344812.7745388\n",
      "step: 12180\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08109923452138901\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344816.6667898\n",
      "step: 8\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.07377897948026657\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344852.5670264\n",
      "step: 8\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0006769648753106594\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344852.5677264\n",
      "step: 8\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344862.693242\n",
      "step: 12195\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06229298561811447\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344868.1109335\n",
      "step: 12215\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06137900799512863\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344873.4700396\n",
      "step: 12235\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06697984039783478\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344878.8446548\n",
      "step: 12255\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06601203978061676\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344884.2182682\n",
      "step: 12275\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0666331872344017\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344889.587282\n",
      "step: 12295\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0744466781616211\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344894.9596798\n",
      "step: 12315\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07594048976898193\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344900.32554\n",
      "step: 12335\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07731913775205612\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344905.6976905\n",
      "step: 12355\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06926298141479492\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344911.0576036\n",
      "step: 12375\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06885574012994766\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344916.4155643\n",
      "step: 12395\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0714130699634552\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344921.8067925\n",
      "step: 12415\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07047756016254425\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344927.19923\n",
      "step: 12435\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06996635347604752\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344932.5814836\n",
      "step: 12455\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07527294009923935\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344937.9540436\n",
      "step: 12475\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07461991161108017\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344943.3153532\n",
      "step: 12495\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06655704975128174\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344948.6853762\n",
      "step: 12515\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07078728079795837\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344954.0771656\n",
      "step: 12535\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06677144765853882\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344959.471421\n",
      "step: 12555\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06896485388278961\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344964.867343\n",
      "step: 12575\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06307719647884369\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344970.252179\n",
      "step: 12595\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06520839035511017\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344975.6190293\n",
      "step: 12615\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0777263268828392\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344980.9903727\n",
      "step: 12635\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06350941956043243\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344986.368712\n",
      "step: 12655\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07605668902397156\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344991.758253\n",
      "step: 12675\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06857786327600479\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555344997.145776\n",
      "step: 12695\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07179366052150726\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345002.5216398\n",
      "step: 12715\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0654086172580719\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345007.896966\n",
      "step: 12735\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07051139324903488\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345013.2913787\n",
      "step: 12755\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07430687546730042\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345018.6885183\n",
      "step: 12775\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07689376175403595\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345024.0781171\n",
      "step: 12795\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07635381072759628\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345029.4538422\n",
      "step: 12815\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07573298364877701\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345034.8451912\n",
      "step: 12835\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08123189210891724\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345040.215237\n",
      "step: 12855\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08365795016288757\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345045.587959\n",
      "step: 12875\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06738683581352234\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345050.9672012\n",
      "step: 12895\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0731629878282547\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345056.3586438\n",
      "step: 12915\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06857489794492722\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345061.7501504\n",
      "step: 12935\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06871809810400009\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345067.153732\n",
      "step: 12955\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07276437431573868\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345072.544498\n",
      "step: 12975\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.061097096651792526\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345077.9195697\n",
      "step: 12995\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07291153073310852\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345083.308953\n",
      "step: 13015\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06477870792150497\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345088.694569\n",
      "step: 13035\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08013325184583664\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345094.0709682\n",
      "step: 13055\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06654935330152512\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345099.4465358\n",
      "step: 13075\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06757418811321259\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345104.8124678\n",
      "step: 13095\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07274485379457474\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345110.1946957\n",
      "step: 13115\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07038658857345581\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345115.591282\n",
      "step: 13135\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06990843266248703\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345120.9769204\n",
      "step: 13155\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07282424718141556\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345126.3610861\n",
      "step: 13175\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07515926659107208\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345131.7503605\n",
      "step: 13195\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06617555022239685\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345137.1288905\n",
      "step: 13215\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06855660676956177\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345142.5241141\n",
      "step: 13235\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07984878867864609\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345147.9117491\n",
      "step: 13255\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07769014686346054\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345153.3011003\n",
      "step: 13275\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07394491136074066\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345158.681699\n",
      "step: 13295\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07122191786766052\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345164.063958\n",
      "step: 13315\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07052156329154968\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345169.439155\n",
      "step: 13335\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06997986882925034\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345174.8336463\n",
      "step: 13355\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06429558992385864\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345180.2309942\n",
      "step: 13375\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0648479238152504\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345185.644355\n",
      "step: 13395\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08383102715015411\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345191.0228364\n",
      "step: 13415\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07110868394374847\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345196.4004793\n",
      "step: 13435\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07559939473867416\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345201.7909775\n",
      "step: 13455\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07662893831729889\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345207.1692417\n",
      "step: 13475\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06902416795492172\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345212.567213\n",
      "step: 13495\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07082290947437286\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345217.9394355\n",
      "step: 13515\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06661388278007507\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345223.3156483\n",
      "step: 13535\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07357753813266754\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345227.2036746\n",
      "step: 9\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.070820651948452\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345263.1094038\n",
      "step: 9\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0007190208416432142\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345263.1101544\n",
      "step: 9\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345264.711158\n",
      "step: 13550\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07472069561481476\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345270.155445\n",
      "step: 13570\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0685071274638176\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345275.5351694\n",
      "step: 13590\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0666651502251625\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345280.9338384\n",
      "step: 13610\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07461472600698471\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345286.321245\n",
      "step: 13630\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06444206833839417\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345291.702777\n",
      "step: 13650\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05928472429513931\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345297.0557375\n",
      "step: 13670\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06813934445381165\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345302.4262855\n",
      "step: 13690\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07220587134361267\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345307.806569\n",
      "step: 13710\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06719787418842316\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345313.200155\n",
      "step: 13730\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06603962182998657\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345318.5696652\n",
      "step: 13750\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07330384105443954\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345323.940825\n",
      "step: 13770\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06915678828954697\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345329.327424\n",
      "step: 13790\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05654437839984894\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345334.6965628\n",
      "step: 13810\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06896787136793137\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345340.0799074\n",
      "step: 13830\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.061123549938201904\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345345.444289\n",
      "step: 13850\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06571350991725922\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345350.8375778\n",
      "step: 13870\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06659208238124847\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345356.2301238\n",
      "step: 13890\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07437697798013687\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345361.6408823\n",
      "step: 13910\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06672506034374237\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345367.0338137\n",
      "step: 13930\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06043952330946922\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345372.427663\n",
      "step: 13950\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07355181872844696\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345377.7901878\n",
      "step: 13970\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06393226236104965\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345383.1830158\n",
      "step: 13990\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06314709782600403\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345388.5856006\n",
      "step: 14010\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06343474239110947\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345393.989055\n",
      "step: 14030\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06861990690231323\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345399.3685362\n",
      "step: 14050\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06888599693775177\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345404.742761\n",
      "step: 14070\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06450693309307098\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345410.157121\n",
      "step: 14090\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07018134742975235\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345415.559277\n",
      "step: 14110\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06660545617341995\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345420.9551008\n",
      "step: 14130\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0645410418510437\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345426.3499553\n",
      "step: 14150\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06298802047967911\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345431.7196362\n",
      "step: 14170\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05658741667866707\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345437.1012013\n",
      "step: 14190\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06618047505617142\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345442.4950595\n",
      "step: 14210\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06198226287961006\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345447.8988237\n",
      "step: 14230\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06315908581018448\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345453.306489\n",
      "step: 14250\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07141715288162231\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345458.6979449\n",
      "step: 14270\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07277257740497589\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345464.0850575\n",
      "step: 14290\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07562956213951111\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345469.4785652\n",
      "step: 14310\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06563833355903625\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345474.879794\n",
      "step: 14330\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07564296573400497\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345480.2727864\n",
      "step: 14350\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07217654585838318\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345485.644301\n",
      "step: 14370\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.061500612646341324\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345491.0178113\n",
      "step: 14390\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06132393702864647\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345496.3996658\n",
      "step: 14410\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0698184072971344\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345501.781369\n",
      "step: 14430\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0721520483493805\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345507.1558218\n",
      "step: 14450\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05081462115049362\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345512.541775\n",
      "step: 14470\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07111790776252747\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345517.9396632\n",
      "step: 14490\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07491015642881393\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345523.32567\n",
      "step: 14510\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06634422391653061\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345528.7229028\n",
      "step: 14530\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05636690557003021\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345534.1080492\n",
      "step: 14550\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07008612900972366\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345539.4707525\n",
      "step: 14570\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0640106350183487\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345544.866575\n",
      "step: 14590\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06945237517356873\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345550.273606\n",
      "step: 14610\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06191952899098396\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345555.6801844\n",
      "step: 14630\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06585445255041122\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345561.0775201\n",
      "step: 14650\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07997623085975647\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345566.4609935\n",
      "step: 14670\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06494203954935074\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345571.8503425\n",
      "step: 14690\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06667158007621765\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345577.236674\n",
      "step: 14710\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.058965206146240234\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345582.649545\n",
      "step: 14730\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05954734981060028\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345588.046985\n",
      "step: 14750\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06332249939441681\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345593.4375365\n",
      "step: 14770\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07530401647090912\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345598.8260245\n",
      "step: 14790\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0741124376654625\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345604.2168677\n",
      "step: 14810\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07138079404830933\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345609.5968807\n",
      "step: 14830\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07506401836872101\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345614.9891267\n",
      "step: 14850\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0742415338754654\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345620.3717144\n",
      "step: 14870\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0674782395362854\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345625.7465434\n",
      "step: 14890\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07056757062673569\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345629.650958\n",
      "step: 10\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.06832718849182129\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345665.5289366\n",
      "step: 10\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0006821208517067134\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345665.5296507\n",
      "step: 10\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345667.129987\n",
      "step: 14905\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06235120818018913\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345672.557129\n",
      "step: 14925\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06161203235387802\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345677.938734\n",
      "step: 14945\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07394348084926605\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345683.322775\n",
      "step: 14965\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07137417793273926\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345688.6796014\n",
      "step: 14985\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06201079860329628\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345694.0438485\n",
      "step: 15005\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05224668234586716\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345699.4214668\n",
      "step: 15025\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06639021635055542\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345704.8118398\n",
      "step: 15045\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08351019024848938\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345710.1934583\n",
      "step: 15065\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07024283707141876\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345715.586405\n",
      "step: 15085\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07179611176252365\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345720.9551425\n",
      "step: 15105\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06982603669166565\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345726.3415782\n",
      "step: 15125\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06641493737697601\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345731.7343585\n",
      "step: 15145\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06857730448246002\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345737.1202786\n",
      "step: 15165\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06573273241519928\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345742.5187802\n",
      "step: 15185\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07225323468446732\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345747.9100814\n",
      "step: 15205\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06517797708511353\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345753.3022585\n",
      "step: 15225\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.061681345105171204\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345758.6818743\n",
      "step: 15245\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06269608438014984\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345764.0597641\n",
      "step: 15265\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06508156657218933\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345769.4375713\n",
      "step: 15285\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07540160417556763\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345774.8157492\n",
      "step: 15305\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05855008214712143\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345780.1953208\n",
      "step: 15325\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.061707936227321625\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345785.6199849\n",
      "step: 15345\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.061447788029909134\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345791.0060449\n",
      "step: 15365\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07472887635231018\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345796.3906116\n",
      "step: 15385\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06282688677310944\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345801.761256\n",
      "step: 15405\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.059753213077783585\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345807.1561801\n",
      "step: 15425\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0651407390832901\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345812.5501533\n",
      "step: 15445\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05620419979095459\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345817.9282813\n",
      "step: 15465\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06751088798046112\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345823.3193276\n",
      "step: 15485\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07858099788427353\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345828.7088127\n",
      "step: 15505\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.062415994703769684\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345834.1040285\n",
      "step: 15525\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07127279788255692\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345839.4999187\n",
      "step: 15545\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06321726739406586\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345844.8953116\n",
      "step: 15565\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06256578862667084\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345850.2833922\n",
      "step: 15585\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06747719645500183\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345855.670711\n",
      "step: 15605\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07113282382488251\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345861.0747557\n",
      "step: 15625\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06627967953681946\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345866.474716\n",
      "step: 15645\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07242782413959503\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345871.85646\n",
      "step: 15665\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0643528401851654\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345877.2467518\n",
      "step: 15685\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06950754672288895\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345882.636743\n",
      "step: 15705\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05277898907661438\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345888.0243025\n",
      "step: 15725\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06632385402917862\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345893.430717\n",
      "step: 15745\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07141904532909393\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345898.831585\n",
      "step: 15765\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07149943709373474\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345904.2230287\n",
      "step: 15785\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.063283272087574\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345909.6280909\n",
      "step: 15805\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0651458129286766\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345915.0254462\n",
      "step: 15825\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06856174767017365\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345920.4087799\n",
      "step: 15845\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0688072070479393\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345925.7826402\n",
      "step: 15865\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07061921060085297\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345931.1752174\n",
      "step: 15885\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05448371171951294\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345936.5588255\n",
      "step: 15905\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07118863612413406\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345941.9495933\n",
      "step: 15925\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07086432725191116\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345947.3203325\n",
      "step: 15945\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07069797813892365\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345952.7144446\n",
      "step: 15965\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06695777177810669\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345958.1145105\n",
      "step: 15985\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06186951696872711\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345963.513162\n",
      "step: 16005\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0649537742137909\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345968.896698\n",
      "step: 16025\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06856369227170944\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345974.2751565\n",
      "step: 16045\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06484994292259216\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345979.6678736\n",
      "step: 16065\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06055760011076927\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345985.0524921\n",
      "step: 16085\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.062458060681819916\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345990.4378626\n",
      "step: 16105\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06553816050291061\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555345995.8441913\n",
      "step: 16125\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07066889107227325\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346001.2463288\n",
      "step: 16145\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05969226360321045\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346006.6392126\n",
      "step: 16165\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06793703138828278\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346012.0268264\n",
      "step: 16185\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0754656195640564\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346017.4175837\n",
      "step: 16205\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06729957461357117\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346022.7998846\n",
      "step: 16225\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06042784824967384\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346028.1882222\n",
      "step: 16245\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07222817838191986\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346032.1016037\n",
      "step: 11\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.06600534170866013\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346068.0506697\n",
      "step: 11\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0007329382933676243\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346068.051437\n",
      "step: 11\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346069.6570222\n",
      "step: 16260\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06612782180309296\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346075.096533\n",
      "step: 16280\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05937311798334122\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346080.4674497\n",
      "step: 16300\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06357027590274811\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346085.8537014\n",
      "step: 16320\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05921515077352524\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346091.24232\n",
      "step: 16340\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05664350092411041\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346096.633365\n",
      "step: 16360\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05186858028173447\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346102.0191379\n",
      "step: 16380\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06563007831573486\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346107.3952608\n",
      "step: 16400\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05631124973297119\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346112.758926\n",
      "step: 16420\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0642864853143692\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346118.1574678\n",
      "step: 16440\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.057241540402173996\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346123.5535717\n",
      "step: 16460\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06326276808977127\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346128.9364543\n",
      "step: 16480\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05524769052863121\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346134.317667\n",
      "step: 16500\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06107616424560547\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346139.695571\n",
      "step: 16520\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05820668488740921\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346145.0787768\n",
      "step: 16540\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05886617675423622\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346150.4647985\n",
      "step: 16560\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06593553721904755\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346155.864798\n",
      "step: 16580\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06339354068040848\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346161.264727\n",
      "step: 16600\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06536401808261871\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346166.6532445\n",
      "step: 16620\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06357521563768387\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346172.0302415\n",
      "step: 16640\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0676129162311554\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346177.3992171\n",
      "step: 16660\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05971348285675049\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346182.7689564\n",
      "step: 16680\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06982462853193283\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346188.1527936\n",
      "step: 16700\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.08007989823818207\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346193.5445402\n",
      "step: 16720\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05803559347987175\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346198.9521718\n",
      "step: 16740\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.062445588409900665\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346204.3505237\n",
      "step: 16760\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05930167809128761\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346209.7358809\n",
      "step: 16780\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06552252173423767\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346215.1232088\n",
      "step: 16800\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0642663836479187\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346220.522476\n",
      "step: 16820\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07142569124698639\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346225.9221728\n",
      "step: 16840\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05740146338939667\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346231.3183331\n",
      "step: 16860\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07296151667833328\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346236.6961403\n",
      "step: 16880\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05579207465052605\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346242.0825944\n",
      "step: 16900\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05768747255206108\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346247.451494\n",
      "step: 16920\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06520193070173264\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346252.8364007\n",
      "step: 16940\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05596042424440384\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346258.2041624\n",
      "step: 16960\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05583060532808304\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346263.6004622\n",
      "step: 16980\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07147324085235596\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346268.9951444\n",
      "step: 17000\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06469079107046127\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346274.3829224\n",
      "step: 17020\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06915030628442764\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346279.7633965\n",
      "step: 17040\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06610417366027832\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346285.137254\n",
      "step: 17060\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05661877244710922\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346290.532573\n",
      "step: 17080\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.059268414974212646\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346295.9444814\n",
      "step: 17100\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06375682353973389\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346301.3487318\n",
      "step: 17120\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06776687502861023\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346306.736325\n",
      "step: 17140\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05540553852915764\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346312.1137333\n",
      "step: 17160\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.062330711632966995\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346317.4753838\n",
      "step: 17180\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05524550750851631\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346322.8453765\n",
      "step: 17200\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06773937493562698\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346328.2313108\n",
      "step: 17220\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06777192652225494\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346333.628496\n",
      "step: 17240\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06189095228910446\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346339.023123\n",
      "step: 17260\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06500904262065887\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346344.418764\n",
      "step: 17280\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06619563698768616\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346349.8141823\n",
      "step: 17300\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06258507817983627\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346355.21209\n",
      "step: 17320\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05931035429239273\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346360.617588\n",
      "step: 17340\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0687611848115921\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346366.01094\n",
      "step: 17360\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06564341485500336\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346371.3900905\n",
      "step: 17380\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06419384479522705\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346376.7766175\n",
      "step: 17400\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06055132672190666\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346382.1610103\n",
      "step: 17420\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06770624220371246\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346387.5356324\n",
      "step: 17440\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06713997572660446\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346392.9056125\n",
      "step: 17460\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.061145320534706116\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346398.3007188\n",
      "step: 17480\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06804132461547852\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346403.7044592\n",
      "step: 17500\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06709679961204529\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346409.1137743\n",
      "step: 17520\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06851924955844879\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346414.5004961\n",
      "step: 17540\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05955350771546364\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346419.893208\n",
      "step: 17560\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05778370797634125\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346425.271311\n",
      "step: 17580\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06404878944158554\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346430.6663342\n",
      "step: 17600\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0788569450378418\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346434.557136\n",
      "step: 12\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.06378360092639923\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346470.4985545\n",
      "step: 12\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0006595537415705621\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346470.4987922\n",
      "step: 12\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346480.6255906\n",
      "step: 17615\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.062148697674274445\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346486.0502064\n",
      "step: 17635\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0508834645152092\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346491.389219\n",
      "step: 17655\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06513135880231857\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346496.7585044\n",
      "step: 17675\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06095941364765167\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346502.1389916\n",
      "step: 17695\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06609898805618286\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346507.526796\n",
      "step: 17715\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06385798007249832\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346512.9116037\n",
      "step: 17735\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06470774114131927\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346518.28929\n",
      "step: 17755\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06611321866512299\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346523.6521232\n",
      "step: 17775\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0625537782907486\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346529.038305\n",
      "step: 17795\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05672451853752136\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346534.4128203\n",
      "step: 17815\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05713742598891258\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346542.3803627\n",
      "step: 17835\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06299205869436264\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346551.155833\n",
      "step: 17855\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.058244697749614716\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346559.9135478\n",
      "step: 17875\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05651431530714035\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346568.6894872\n",
      "step: 17895\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05654633790254593\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346577.4804413\n",
      "step: 17915\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0592355839908123\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346586.274155\n",
      "step: 17935\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04919193685054779\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346595.0339053\n",
      "step: 17955\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0701245442032814\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346603.808452\n",
      "step: 17975\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.049500972032547\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346612.6075742\n",
      "step: 17995\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06719857454299927\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346621.421865\n",
      "step: 18015\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06720581650733948\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346630.2171383\n",
      "step: 18035\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06140470504760742\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346639.0124054\n",
      "step: 18055\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06053623557090759\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346647.7988284\n",
      "step: 18075\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05908415466547012\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346655.8198833\n",
      "step: 18095\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05777861177921295\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346664.5999355\n",
      "step: 18115\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06410728394985199\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346673.402617\n",
      "step: 18135\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.056291915476322174\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346682.198685\n",
      "step: 18155\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06307650357484818\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346691.025237\n",
      "step: 18175\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.061772726476192474\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346699.825479\n",
      "step: 18195\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06130494922399521\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346708.633353\n",
      "step: 18215\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0563039630651474\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346717.4492304\n",
      "step: 18235\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06220217049121857\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346726.2653449\n",
      "step: 18255\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06842801719903946\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346735.080333\n",
      "step: 18275\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05178748816251755\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346743.8428895\n",
      "step: 18295\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06472774595022202\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346752.6260295\n",
      "step: 18315\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06455516070127487\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346761.4019463\n",
      "step: 18335\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07408177107572556\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346770.2115495\n",
      "step: 18355\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06275386363267899\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346779.045974\n",
      "step: 18375\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0654715746641159\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346787.8507633\n",
      "step: 18395\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06455700844526291\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346796.6239212\n",
      "step: 18415\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06816345453262329\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346805.3890243\n",
      "step: 18435\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06190863996744156\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346814.203994\n",
      "step: 18455\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07266907393932343\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346822.9734676\n",
      "step: 18475\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05384121462702751\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346831.7424834\n",
      "step: 18495\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05223776772618294\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346840.5258045\n",
      "step: 18515\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.068232461810112\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346849.3276663\n",
      "step: 18535\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.060583531856536865\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346858.158048\n",
      "step: 18555\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06387993693351746\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346866.9647458\n",
      "step: 18575\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06926114857196808\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346875.773843\n",
      "step: 18595\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05935809761285782\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346884.5529644\n",
      "step: 18615\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06436453014612198\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346893.3333013\n",
      "step: 18635\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.055527474731206894\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346902.1449687\n",
      "step: 18655\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06111473590135574\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346910.929893\n",
      "step: 18675\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06141884624958038\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346919.7521527\n",
      "step: 18695\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05902998149394989\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346928.5316195\n",
      "step: 18715\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06250566244125366\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346937.292957\n",
      "step: 18735\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06707029789686203\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346946.0802104\n",
      "step: 18755\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.052301518619060516\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346954.8833585\n",
      "step: 18775\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0676330104470253\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346963.6871202\n",
      "step: 18795\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04943675547838211\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346972.4621866\n",
      "step: 18815\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06029944121837616\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346981.2722433\n",
      "step: 18835\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05984720587730408\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346990.0799878\n",
      "step: 18855\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06237776577472687\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555346998.8673966\n",
      "step: 18875\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06242986023426056\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347007.6900496\n",
      "step: 18895\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06340735405683517\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347016.4992838\n",
      "step: 18915\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07149133831262589\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347025.1002445\n",
      "step: 18935\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06393277645111084\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347030.4856288\n",
      "step: 18955\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05853782221674919\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347034.398915\n",
      "step: 13\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.06186307594180107\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347070.2398748\n",
      "step: 13\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0006757600931450725\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347070.24065\n",
      "step: 13\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347071.8280842\n",
      "step: 18970\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.060704320669174194\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347077.2406938\n",
      "step: 18990\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05102180689573288\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347082.6252036\n",
      "step: 19010\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.060458481311798096\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347088.0151446\n",
      "step: 19030\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.057687100023031235\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347093.3990846\n",
      "step: 19050\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05740173161029816\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347098.7704885\n",
      "step: 19070\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0666549950838089\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347104.1421733\n",
      "step: 19090\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.061830975115299225\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347109.5069728\n",
      "step: 19110\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06121280416846275\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347114.9010713\n",
      "step: 19130\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05907896161079407\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347120.2794218\n",
      "step: 19150\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06116004288196564\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347125.6622677\n",
      "step: 19170\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05925636366009712\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347131.498503\n",
      "step: 19190\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05722767114639282\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347137.4167657\n",
      "step: 19210\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.060323044657707214\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347143.3364182\n",
      "step: 19230\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.053200528025627136\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347149.2553773\n",
      "step: 19250\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06637746095657349\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347155.1867948\n",
      "step: 19270\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06652718782424927\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347161.112473\n",
      "step: 19290\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.058984991163015366\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347167.0338361\n",
      "step: 19310\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.057970110327005386\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347172.9552746\n",
      "step: 19330\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.058917075395584106\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347178.87764\n",
      "step: 19350\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.049468815326690674\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347184.802361\n",
      "step: 19370\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05717267841100693\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347190.722763\n",
      "step: 19390\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06990506500005722\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347196.659325\n",
      "step: 19410\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06607639789581299\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347202.590112\n",
      "step: 19430\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05438164621591568\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347208.5344834\n",
      "step: 19450\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05644853413105011\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347214.4623487\n",
      "step: 19470\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0577101893723011\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347220.3978806\n",
      "step: 19490\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05277768149971962\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347226.3207464\n",
      "step: 19510\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.056708917021751404\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347232.2597563\n",
      "step: 19530\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05971372872591019\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347238.1960182\n",
      "step: 19550\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05704410374164581\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347244.139464\n",
      "step: 19570\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06207061931490898\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347250.0723581\n",
      "step: 19590\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.060331154614686966\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347256.0127678\n",
      "step: 19610\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05590876191854477\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347261.9444356\n",
      "step: 19630\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07506079971790314\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347267.8837755\n",
      "step: 19650\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05535263940691948\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347273.824373\n",
      "step: 19670\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.059090688824653625\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347279.767456\n",
      "step: 19690\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.061826419085264206\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347285.7062218\n",
      "step: 19710\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05715607851743698\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347291.6465697\n",
      "step: 19730\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05771973356604576\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347297.5801451\n",
      "step: 19750\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.057371921837329865\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347303.5089576\n",
      "step: 19770\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05933535471558571\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347309.4399667\n",
      "step: 19790\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.060991887003183365\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347315.378688\n",
      "step: 19810\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.055744800716638565\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347321.3166275\n",
      "step: 19830\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05499313771724701\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347327.2503276\n",
      "step: 19850\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05107037350535393\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347333.1771123\n",
      "step: 19870\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06013379618525505\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347339.1084414\n",
      "step: 19890\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06886696815490723\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347345.0397341\n",
      "step: 19910\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05881393700838089\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347350.9666119\n",
      "step: 19930\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0648568794131279\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347356.899468\n",
      "step: 19950\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06045331060886383\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347362.8436766\n",
      "step: 19970\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06061729043722153\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347368.7768998\n",
      "step: 19990\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07166806608438492\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347374.7154446\n",
      "step: 20010\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.053437698632478714\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347380.6523747\n",
      "step: 20030\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.056901078671216965\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347386.5847738\n",
      "step: 20050\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0661945641040802\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347392.5211046\n",
      "step: 20070\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06062021851539612\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347398.4609296\n",
      "step: 20090\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06242113560438156\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347404.4014933\n",
      "step: 20110\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06093728169798851\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347410.3308938\n",
      "step: 20130\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06680437922477722\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347416.2669613\n",
      "step: 20150\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.055330440402030945\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347422.2026625\n",
      "step: 20170\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07284830510616302\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347428.1420941\n",
      "step: 20190\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05916591361165047\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347434.0667794\n",
      "step: 20210\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05738123133778572\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347440.0121896\n",
      "step: 20230\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.061338018625974655\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347445.9348388\n",
      "step: 20250\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06366851180791855\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347451.873604\n",
      "step: 20270\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05755072087049484\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347457.8020833\n",
      "step: 20290\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.054419513791799545\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347463.7341018\n",
      "step: 20310\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06085740774869919\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347468.0193129\n",
      "step: 14\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.060055557638406754\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347507.1075056\n",
      "step: 14\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.000660747173242271\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347507.1083715\n",
      "step: 14\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347508.8321822\n",
      "step: 20325\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05747155845165253\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347514.790359\n",
      "step: 20345\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05547498166561127\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347520.7084122\n",
      "step: 20365\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05278497189283371\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347526.6360042\n",
      "step: 20385\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.049676939845085144\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347532.5699065\n",
      "step: 20405\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06089911237359047\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347538.5031006\n",
      "step: 20425\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06300536543130875\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347544.4267726\n",
      "step: 20445\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06020287051796913\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347550.3569255\n",
      "step: 20465\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05105242505669594\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347556.2899961\n",
      "step: 20485\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06266855448484421\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347562.2198064\n",
      "step: 20505\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.052117690443992615\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347568.148738\n",
      "step: 20525\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.056489769369363785\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347574.0829136\n",
      "step: 20545\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.07009942084550858\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347580.0101216\n",
      "step: 20565\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05279175564646721\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347585.9357617\n",
      "step: 20585\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05631823092699051\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347591.868997\n",
      "step: 20605\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05960693582892418\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347597.7964804\n",
      "step: 20625\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05287044867873192\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347603.718818\n",
      "step: 20645\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05772707611322403\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347609.6538224\n",
      "step: 20665\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.060712024569511414\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347615.5928457\n",
      "step: 20685\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05728665739297867\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347621.523095\n",
      "step: 20705\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06116804480552673\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347627.4693663\n",
      "step: 20725\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0621512196958065\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347633.398266\n",
      "step: 20745\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06163942813873291\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347639.3307927\n",
      "step: 20765\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04768207669258118\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347645.2677102\n",
      "step: 20785\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05418291687965393\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347650.9270198\n",
      "step: 20805\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05242408066987991\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347656.298673\n",
      "step: 20825\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05502714961767197\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347661.6863117\n",
      "step: 20845\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05399313196539879\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347667.0771651\n",
      "step: 20865\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05710075795650482\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347672.4806206\n",
      "step: 20885\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.055790968239307404\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347677.8826554\n",
      "step: 20905\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05639883875846863\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347683.2542896\n",
      "step: 20925\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05893317237496376\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347688.639943\n",
      "step: 20945\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04868423193693161\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347694.0276523\n",
      "step: 20965\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.062280066311359406\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347699.411552\n",
      "step: 20985\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05742805078625679\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347704.815917\n",
      "step: 21005\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.054325222969055176\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347710.207285\n",
      "step: 21025\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.052913274616003036\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347715.5969617\n",
      "step: 21045\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.054706621915102005\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347720.9806013\n",
      "step: 21065\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06156434118747711\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347726.3630195\n",
      "step: 21085\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05367780476808548\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347731.7724009\n",
      "step: 21105\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0623195543885231\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347737.1635244\n",
      "step: 21125\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05198455974459648\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347742.5504355\n",
      "step: 21145\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.062101371586322784\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347747.9244108\n",
      "step: 21165\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.059960395097732544\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347753.3122022\n",
      "step: 21185\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.060189709067344666\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347758.7019372\n",
      "step: 21205\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05825457721948624\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347764.1075373\n",
      "step: 21225\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05483980476856232\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347769.518447\n",
      "step: 21245\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06515905261039734\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347774.9281814\n",
      "step: 21265\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05276693031191826\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347780.3167772\n",
      "step: 21285\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05917523056268692\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347785.690238\n",
      "step: 21305\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05352024734020233\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347791.0609713\n",
      "step: 21325\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06304997205734253\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347796.4523454\n",
      "step: 21345\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05627305805683136\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347801.8473072\n",
      "step: 21365\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.056189779192209244\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347807.2671244\n",
      "step: 21385\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.060952045023441315\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347812.6688423\n",
      "step: 21405\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05531459301710129\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347818.0707757\n",
      "step: 21425\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05593043938279152\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347823.4635108\n",
      "step: 21445\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05456658452749252\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347828.8580093\n",
      "step: 21465\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05277350917458534\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347834.2436237\n",
      "step: 21485\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.055861398577690125\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347839.6305292\n",
      "step: 21505\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.053066007792949677\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347845.0256286\n",
      "step: 21525\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05641679838299751\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347850.433315\n",
      "step: 21545\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06115866079926491\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347855.844301\n",
      "step: 21565\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.054314445704221725\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347861.2351007\n",
      "step: 21585\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05329589173197746\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347866.637924\n",
      "step: 21605\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05911679193377495\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347872.032019\n",
      "step: 21625\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04966132342815399\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347877.4243643\n",
      "step: 21645\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06785726547241211\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347882.8186674\n",
      "step: 21665\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.055403925478458405\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347886.73352\n",
      "step: 15\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.058397162705659866\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347922.6298037\n",
      "step: 15\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0006719193770550191\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347922.6300354\n",
      "step: 15\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347924.1951222\n",
      "step: 21680\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06067493557929993\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347929.6342764\n",
      "step: 21700\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05714339762926102\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347935.0012136\n",
      "step: 21720\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06182575970888138\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347940.3766441\n",
      "step: 21740\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06728125363588333\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347945.7358391\n",
      "step: 21760\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05866425111889839\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347951.110587\n",
      "step: 21780\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05496803671121597\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347956.4926097\n",
      "step: 21800\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.057799506932497025\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347961.8818889\n",
      "step: 21820\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05110878124833107\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347967.2552624\n",
      "step: 21840\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05684375390410423\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347972.6434278\n",
      "step: 21860\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.057317283004522324\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347978.0170329\n",
      "step: 21880\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05250684171915054\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347983.390049\n",
      "step: 21900\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.057210538536310196\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347988.7579966\n",
      "step: 21920\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05836036801338196\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347994.1544497\n",
      "step: 21940\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0599314346909523\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555347999.5377138\n",
      "step: 21960\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05556778609752655\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348004.9336011\n",
      "step: 21980\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05706951022148132\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348010.325167\n",
      "step: 22000\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05856025218963623\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348015.6985135\n",
      "step: 22020\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05976669490337372\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348021.085371\n",
      "step: 22040\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04549216106534004\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348026.460754\n",
      "step: 22060\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05664711073040962\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348031.84686\n",
      "step: 22080\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.055370546877384186\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348037.2222314\n",
      "step: 22100\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05893890932202339\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348042.6164894\n",
      "step: 22120\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05603429675102234\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348048.0102172\n",
      "step: 22140\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06083951145410538\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348053.3823357\n",
      "step: 22160\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.055786095559597015\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348058.7502751\n",
      "step: 22180\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05341874063014984\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348064.1265745\n",
      "step: 22200\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.061397433280944824\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348069.5208662\n",
      "step: 22220\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04842222481966019\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348074.9094532\n",
      "step: 22240\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.053751759231090546\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348080.2965837\n",
      "step: 22260\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.064883753657341\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348085.6664307\n",
      "step: 22280\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05568815767765045\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348091.0757017\n",
      "step: 22300\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04851561784744263\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348096.4761004\n",
      "step: 22320\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05136945843696594\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348101.8737664\n",
      "step: 22340\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05769126117229462\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348107.260783\n",
      "step: 22360\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05616406351327896\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348112.6434884\n",
      "step: 22380\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06376340985298157\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348118.0308204\n",
      "step: 22400\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05244388058781624\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348123.410419\n",
      "step: 22420\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05201895162463188\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348128.8103938\n",
      "step: 22440\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05315198004245758\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348134.1940017\n",
      "step: 22460\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06245536729693413\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348139.56915\n",
      "step: 22480\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.058808282017707825\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348144.9451983\n",
      "step: 22500\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05332988500595093\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348150.341783\n",
      "step: 22520\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06471937149763107\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348155.737977\n",
      "step: 22540\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.046445660293102264\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348161.1415222\n",
      "step: 22560\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05581732839345932\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348166.5376873\n",
      "step: 22580\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05587012693285942\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348171.937785\n",
      "step: 22600\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05126609653234482\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348177.322105\n",
      "step: 22620\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05189403519034386\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348182.69757\n",
      "step: 22640\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0559655986726284\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348188.0877461\n",
      "step: 22660\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06094599515199661\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348193.47378\n",
      "step: 22680\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.056037552654743195\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348198.8747303\n",
      "step: 22700\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.051735326647758484\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348204.2683916\n",
      "step: 22720\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0543220117688179\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348209.6434395\n",
      "step: 22740\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05796948820352554\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348215.0272546\n",
      "step: 22760\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05699421465396881\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348220.421546\n",
      "step: 22780\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.058773577213287354\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348225.8220806\n",
      "step: 22800\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05720650404691696\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348231.2162619\n",
      "step: 22820\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.049601271748542786\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348236.584227\n",
      "step: 22840\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0576840341091156\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348241.9711185\n",
      "step: 22860\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05591527000069618\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348247.3832877\n",
      "step: 22880\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06452064961194992\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348252.783798\n",
      "step: 22900\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.055543720722198486\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348258.1770537\n",
      "step: 22920\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06915625184774399\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348263.6509793\n",
      "step: 22940\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05125188082456589\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348269.579102\n",
      "step: 22960\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05486395210027695\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348275.5105977\n",
      "step: 22980\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.053779929876327515\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348281.4507182\n",
      "step: 23000\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.059200502932071686\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348287.3865998\n",
      "step: 23020\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06509040296077728\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348291.6839108\n",
      "step: 16\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.05689621716737747\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348330.7736814\n",
      "step: 16\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0007065538666211069\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348330.7739687\n",
      "step: 16\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348332.437247\n",
      "step: 23035\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.050698041915893555\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348338.4366622\n",
      "step: 23055\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05702020972967148\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348344.3624551\n",
      "step: 23075\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04704806208610535\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348350.2864718\n",
      "step: 23095\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0573451966047287\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348356.206946\n",
      "step: 23115\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04728349298238754\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348362.1338315\n",
      "step: 23135\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.051914289593696594\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348368.0582867\n",
      "step: 23155\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05478091537952423\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348373.9910553\n",
      "step: 23175\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.046965550631284714\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348379.915088\n",
      "step: 23195\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.062028154730796814\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348385.84851\n",
      "step: 23215\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05811627209186554\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348391.7631707\n",
      "step: 23235\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05614212155342102\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348397.6943436\n",
      "step: 23255\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.055925071239471436\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348403.613982\n",
      "step: 23275\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04686171934008598\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348409.5455768\n",
      "step: 23295\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.057524651288986206\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348415.4835467\n",
      "step: 23315\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05613183230161667\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348421.4162655\n",
      "step: 23335\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.058683108538389206\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348427.356155\n",
      "step: 23355\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.054368164390325546\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348433.2886584\n",
      "step: 23375\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05672473832964897\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348439.2169003\n",
      "step: 23395\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05960094928741455\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348445.1453614\n",
      "step: 23415\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06046893447637558\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348451.0948725\n",
      "step: 23435\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05446842685341835\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348457.0244262\n",
      "step: 23455\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04827088490128517\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348462.9606922\n",
      "step: 23475\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05600833520293236\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348468.896997\n",
      "step: 23495\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05057334899902344\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348474.8349237\n",
      "step: 23515\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05298743396997452\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348480.7680118\n",
      "step: 23535\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0639701560139656\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348486.7127545\n",
      "step: 23555\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05389780551195145\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348492.6491706\n",
      "step: 23575\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.053591538220644\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348498.5868216\n",
      "step: 23595\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.049420081079006195\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348504.5279996\n",
      "step: 23615\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.055889301002025604\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348510.46437\n",
      "step: 23635\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.052332840859889984\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348516.3977094\n",
      "step: 23655\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05367496609687805\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348522.3410728\n",
      "step: 23675\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.055727340281009674\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348528.2729094\n",
      "step: 23695\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05873233452439308\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wall_time: 1555348534.2077096\n",
      "step: 23715\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.058494098484516144\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348540.1361544\n",
      "step: 23735\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05082256346940994\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348546.0775225\n",
      "step: 23755\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0553106814622879\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348552.0106182\n",
      "step: 23775\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06612145900726318\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348557.9561863\n",
      "step: 23795\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06328146159648895\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348563.8825192\n",
      "step: 23815\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.051186997443437576\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348569.8238232\n",
      "step: 23835\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06319685280323029\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348575.7597096\n",
      "step: 23855\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05681018531322479\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348581.6890376\n",
      "step: 23875\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.055754054337739944\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348587.6144056\n",
      "step: 23895\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05052928626537323\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348593.5540884\n",
      "step: 23915\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05314446985721588\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348599.4853637\n",
      "step: 23935\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.053065232932567596\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348605.4181757\n",
      "step: 23955\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05101107433438301\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348611.3541894\n",
      "step: 23975\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05580929294228554\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348617.2991343\n",
      "step: 23995\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05833616107702255\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348623.232399\n",
      "step: 24015\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05713021755218506\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348629.1666825\n",
      "step: 24035\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06681603193283081\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348635.1077912\n",
      "step: 24055\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06784844398498535\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348641.0419142\n",
      "step: 24075\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.060195647180080414\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348646.9781163\n",
      "step: 24095\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06566794216632843\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348652.9070804\n",
      "step: 24115\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.055704254657030106\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348658.838816\n",
      "step: 24135\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.050733864307403564\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348664.7821963\n",
      "step: 24155\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.057504504919052124\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348670.5613725\n",
      "step: 24175\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05170928314328194\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348676.0304098\n",
      "step: 24195\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05745178833603859\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348683.8422472\n",
      "step: 24215\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05391717702150345\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348693.3082962\n",
      "step: 24235\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06148381531238556\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348702.8560352\n",
      "step: 24255\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05773556977510452\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348712.4162002\n",
      "step: 24275\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.056723225861787796\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348721.9560556\n",
      "step: 24295\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05388202145695686\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348731.5072865\n",
      "step: 24315\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06281702220439911\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348741.0588374\n",
      "step: 24335\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05222732201218605\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348750.664559\n",
      "step: 24355\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.049456704407930374\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348760.30484\n",
      "step: 24375\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06439656019210815\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348767.1512802\n",
      "step: 17\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.05540813133120537\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348824.2425156\n",
      "step: 17\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0006845645839348435\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348824.243483\n",
      "step: 17\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348826.022438\n",
      "step: 24390\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.060401517897844315\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348835.5229406\n",
      "step: 24410\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06014302372932434\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348845.0631368\n",
      "step: 24430\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.050987616181373596\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348854.6193976\n",
      "step: 24450\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05086025595664978\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348864.1719706\n",
      "step: 24470\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04607749730348587\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348873.707226\n",
      "step: 24490\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04843644052743912\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348883.251505\n",
      "step: 24510\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04940889775753021\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348892.7563977\n",
      "step: 24530\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05265633016824722\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348902.3119333\n",
      "step: 24550\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05006687343120575\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348911.8371317\n",
      "step: 24570\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.056875549256801605\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348921.3791888\n",
      "step: 24590\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04720195755362511\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348930.9406214\n",
      "step: 24610\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0589781254529953\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348940.4834116\n",
      "step: 24630\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05020126700401306\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348950.0233493\n",
      "step: 24650\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.049920812249183655\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348959.540263\n",
      "step: 24670\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04169302433729172\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348969.1004155\n",
      "step: 24690\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04590252414345741\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348978.6386082\n",
      "step: 24710\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.060786012560129166\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348988.1617181\n",
      "step: 24730\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04738263785839081\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555348997.7252643\n",
      "step: 24750\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05562116950750351\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349007.2451262\n",
      "step: 24770\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05186910182237625\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349016.7968376\n",
      "step: 24790\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06056740880012512\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349026.370541\n",
      "step: 24810\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.047977641224861145\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349035.916392\n",
      "step: 24830\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.055745929479599\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349045.4626381\n",
      "step: 24850\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.052842505276203156\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349054.990301\n",
      "step: 24870\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0584321990609169\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349064.5183966\n",
      "step: 24890\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05437883362174034\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349074.0180476\n",
      "step: 24910\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.054398901760578156\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349083.5840933\n",
      "step: 24930\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05240795761346817\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349093.136229\n",
      "step: 24950\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0505802258849144\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349102.667504\n",
      "step: 24970\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05942492187023163\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349112.2427037\n",
      "step: 24990\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.048153430223464966\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349121.7779331\n",
      "step: 25010\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05148347467184067\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349131.3220315\n",
      "step: 25030\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05267537385225296\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349140.8596032\n",
      "step: 25050\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05973021313548088\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349150.4210696\n",
      "step: 25070\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0581396147608757\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349159.9650147\n",
      "step: 25090\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.051691289991140366\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349169.4874942\n",
      "step: 25110\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05065885931253433\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349179.0298514\n",
      "step: 25130\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05149165913462639\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349188.5775447\n",
      "step: 25150\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05217529833316803\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349198.1177871\n",
      "step: 25170\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.048342689871788025\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349207.6487157\n",
      "step: 25190\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05213949456810951\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349217.174252\n",
      "step: 25210\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05326216667890549\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349226.7449312\n",
      "step: 25230\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05136720836162567\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349236.281495\n",
      "step: 25250\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05127705633640289\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349245.8062909\n",
      "step: 25270\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05233876407146454\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349255.3651457\n",
      "step: 25290\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.060595519840717316\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349264.8960996\n",
      "step: 25310\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.057921431958675385\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349274.4340158\n",
      "step: 25330\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.060108136385679245\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349283.959797\n",
      "step: 25350\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05587776005268097\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349293.4980822\n",
      "step: 25370\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05938789248466492\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349303.0340044\n",
      "step: 25390\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.053415894508361816\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349312.5950928\n",
      "step: 25410\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05962799862027168\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349322.1457448\n",
      "step: 25430\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04677747189998627\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349331.6955578\n",
      "step: 25450\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05267536640167236\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349341.2222915\n",
      "step: 25470\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.050318825989961624\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349347.4975512\n",
      "step: 25490\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05292082577943802\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349353.5180495\n",
      "step: 25510\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.056494034826755524\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349359.5336852\n",
      "step: 25530\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04338708147406578\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349365.5367472\n",
      "step: 25550\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05318457633256912\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349371.5511808\n",
      "step: 25570\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04714757949113846\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349377.5659375\n",
      "step: 25590\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.060248952358961105\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349383.579074\n",
      "step: 25610\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.050696663558483124\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349389.5865848\n",
      "step: 25630\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05426662415266037\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349395.6011345\n",
      "step: 25650\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05089925229549408\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349401.5966253\n",
      "step: 25670\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05422387644648552\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349407.6103275\n",
      "step: 25690\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05849183350801468\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349413.6118116\n",
      "step: 25710\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05734845995903015\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349419.6249564\n",
      "step: 25730\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06317112594842911\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349423.9857166\n",
      "step: 18\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.05411136895418167\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349463.5090406\n",
      "step: 18\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0006725208950228989\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349463.51016\n",
      "step: 18\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349465.2478428\n",
      "step: 25745\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05351540446281433\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349471.2692611\n",
      "step: 25765\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05043281987309456\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349477.2557716\n",
      "step: 25785\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0485139936208725\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349483.2500935\n",
      "step: 25805\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.050675999373197556\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349489.248892\n",
      "step: 25825\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.055473148822784424\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349495.2516792\n",
      "step: 25845\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05085284262895584\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349501.259982\n",
      "step: 25865\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04967581108212471\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349507.266858\n",
      "step: 25885\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05569283664226532\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349513.2678344\n",
      "step: 25905\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04784204065799713\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349519.2692728\n",
      "step: 25925\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.049183156341314316\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349525.273315\n",
      "step: 25945\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05503172427415848\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349531.2731376\n",
      "step: 25965\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.056374043226242065\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349537.283511\n",
      "step: 25985\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05107603222131729\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349543.2866833\n",
      "step: 26005\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05812951922416687\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349549.2943006\n",
      "step: 26025\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05076359957456589\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349555.301349\n",
      "step: 26045\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04834973067045212\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349561.3088336\n",
      "step: 26065\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05073773115873337\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349567.3111722\n",
      "step: 26085\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.054040394723415375\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349573.3256404\n",
      "step: 26105\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0630294680595398\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349579.334166\n",
      "step: 26125\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04915330931544304\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349585.3397374\n",
      "step: 26145\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05588259547948837\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349591.355208\n",
      "step: 26165\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.052670206874608994\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349597.3769433\n",
      "step: 26185\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05391618236899376\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349603.381152\n",
      "step: 26205\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04392767325043678\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349609.3773384\n",
      "step: 26225\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05506684631109238\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349615.3831713\n",
      "step: 26245\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04498489201068878\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349621.3921907\n",
      "step: 26265\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05484294146299362\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349627.4024246\n",
      "step: 26285\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04737346991896629\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349633.4133117\n",
      "step: 26305\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04984752833843231\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349639.4169693\n",
      "step: 26325\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05914068967103958\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349645.4228854\n",
      "step: 26345\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05113867670297623\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349651.4291642\n",
      "step: 26365\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.057252198457717896\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349657.4451382\n",
      "step: 26385\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05093855783343315\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349663.4634156\n",
      "step: 26405\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0607336089015007\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349669.4733725\n",
      "step: 26425\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.053543251007795334\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349675.4865887\n",
      "step: 26445\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06140904128551483\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349681.507541\n",
      "step: 26465\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.061238259077072144\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349687.5138774\n",
      "step: 26485\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04545200616121292\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349693.522992\n",
      "step: 26505\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05347002297639847\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349699.5336096\n",
      "step: 26525\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06690115481615067\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349705.536924\n",
      "step: 26545\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06024055555462837\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349711.550408\n",
      "step: 26565\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04980595409870148\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349717.569906\n",
      "step: 26585\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05924548953771591\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349723.5771308\n",
      "step: 26605\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05249505490064621\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349729.5941515\n",
      "step: 26625\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05387463420629501\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349735.6010764\n",
      "step: 26645\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06466307491064072\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349741.610184\n",
      "step: 26665\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04415837302803993\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349747.6062772\n",
      "step: 26685\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05836542323231697\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349753.6166615\n",
      "step: 26705\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04968360438942909\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349759.6329439\n",
      "step: 26725\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05539148673415184\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349765.651929\n",
      "step: 26745\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.042902879416942596\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349771.6648839\n",
      "step: 26765\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05372421815991402\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349777.6790035\n",
      "step: 26785\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0548182874917984\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349783.6920862\n",
      "step: 26805\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04980820044875145\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349789.7103195\n",
      "step: 26825\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05540449917316437\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349795.7177393\n",
      "step: 26845\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.054849110543727875\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349801.7188988\n",
      "step: 26865\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05456193536520004\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349807.746455\n",
      "step: 26885\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05434076860547066\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349813.7626579\n",
      "step: 26905\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.056212835013866425\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349819.7685194\n",
      "step: 26925\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05584172531962395\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349825.7734942\n",
      "step: 26945\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.059231940656900406\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349831.7790437\n",
      "step: 26965\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.044626303017139435\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349837.792736\n",
      "step: 26985\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0494946613907814\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349843.7992146\n",
      "step: 27005\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05767553299665451\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349849.8061879\n",
      "step: 27025\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04863349720835686\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349855.8150659\n",
      "step: 27045\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.060238972306251526\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349861.8237042\n",
      "step: 27065\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.058200955390930176\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349867.8270452\n",
      "step: 27085\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05701500177383423\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349872.1640937\n",
      "step: 19\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.05311038717627525\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349911.7895875\n",
      "step: 19\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0007163169793784618\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349911.790377\n",
      "step: 19\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349913.4395175\n",
      "step: 27100\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05200023204088211\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349919.506988\n",
      "step: 27120\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.054513342678546906\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349925.5136795\n",
      "step: 27140\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05450883507728577\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349931.5209057\n",
      "step: 27160\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.051493845880031586\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349937.5274935\n",
      "step: 27180\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.048202309757471085\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349943.5368009\n",
      "step: 27200\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06284861266613007\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349949.538266\n",
      "step: 27220\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05061943829059601\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349955.5375042\n",
      "step: 27240\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04860365390777588\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349961.5340724\n",
      "step: 27260\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05694941058754921\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349967.529451\n",
      "step: 27280\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05124116688966751\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349973.5424318\n",
      "step: 27300\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04772862046957016\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349979.5524783\n",
      "step: 27320\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05457257479429245\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349985.5574186\n",
      "step: 27340\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.053864985704422\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349991.553233\n",
      "step: 27360\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04677232727408409\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555349997.5592244\n",
      "step: 27380\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05401037633419037\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350003.5565135\n",
      "step: 27400\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05475924536585808\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350009.562456\n",
      "step: 27420\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0545683354139328\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350015.564208\n",
      "step: 27440\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.052942171692848206\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350021.5674796\n",
      "step: 27460\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04772952198982239\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350027.5618925\n",
      "step: 27480\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.054062746465206146\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350033.5695672\n",
      "step: 27500\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04662028327584267\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350039.5786662\n",
      "step: 27520\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05136885866522789\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350045.5908694\n",
      "step: 27540\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.053493525832891464\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350051.6045315\n",
      "step: 27560\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05199899524450302\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350057.6111176\n",
      "step: 27580\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05372006073594093\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350063.611899\n",
      "step: 27600\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.053564466536045074\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350069.6097848\n",
      "step: 27620\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0596165657043457\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350075.611418\n",
      "step: 27640\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04557160660624504\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350081.617493\n",
      "step: 27660\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.059996169060468674\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350087.619628\n",
      "step: 27680\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04643856734037399\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350093.6235528\n",
      "step: 27700\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05224718153476715\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350099.6332936\n",
      "step: 27720\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.051540177315473557\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350105.655758\n",
      "step: 27740\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04149810969829559\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350111.6655533\n",
      "step: 27760\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05981651321053505\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350117.6777825\n",
      "step: 27780\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05374307557940483\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350123.68695\n",
      "step: 27800\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.057129986584186554\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350129.7061377\n",
      "step: 27820\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04828879237174988\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350135.7155755\n",
      "step: 27840\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04863830655813217\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350141.7243288\n",
      "step: 27860\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.046989522874355316\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350147.7390037\n",
      "step: 27880\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05563003569841385\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350153.7553968\n",
      "step: 27900\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.043617866933345795\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350159.7751698\n",
      "step: 27920\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05657443404197693\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350165.7935262\n",
      "step: 27940\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04939725622534752\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350171.7885566\n",
      "step: 27960\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04118935391306877\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350177.7978103\n",
      "step: 27980\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.048864565789699554\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350183.8137143\n",
      "step: 28000\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04545557498931885\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350189.814749\n",
      "step: 28020\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05100216716527939\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350195.8345497\n",
      "step: 28040\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04188856482505798\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350201.8322291\n",
      "step: 28060\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.046667471528053284\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350207.8360832\n",
      "step: 28080\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.052658818662166595\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350213.852587\n",
      "step: 28100\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.055353693664073944\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350219.867969\n",
      "step: 28120\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.055969905108213425\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350225.8755174\n",
      "step: 28140\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04589838534593582\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350231.902805\n",
      "step: 28160\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05413416028022766\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350237.9172976\n",
      "step: 28180\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.054018594324588776\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350243.9290545\n",
      "step: 28200\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05977197363972664\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350249.9425259\n",
      "step: 28220\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05234792083501816\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350255.9585571\n",
      "step: 28240\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05048238858580589\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350261.9599855\n",
      "step: 28260\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04668416827917099\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350267.967329\n",
      "step: 28280\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05536980181932449\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350273.976474\n",
      "step: 28300\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.043424107134342194\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350279.9833128\n",
      "step: 28320\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.048664141446352005\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350285.9878697\n",
      "step: 28340\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05273820087313652\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350291.9969692\n",
      "step: 28360\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05565279722213745\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350297.9998398\n",
      "step: 28380\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.053624555468559265\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350304.0162966\n",
      "step: 28400\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05632031708955765\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350310.029727\n",
      "step: 28420\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.049863606691360474\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350316.0366359\n",
      "step: 28440\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05322451889514923\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350320.3966436\n",
      "step: 20\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.05191519111394882\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350359.9912055\n",
      "step: 20\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0006924510234966874\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350359.9920301\n",
      "step: 20\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350361.7181463\n",
      "step: 28455\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04206693544983864\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350367.7330503\n",
      "step: 28475\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04992179945111275\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350373.7267773\n",
      "step: 28495\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05847807601094246\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350379.739764\n",
      "step: 28515\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.044411007314920425\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350385.7333448\n",
      "step: 28535\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.050517693161964417\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350391.7445579\n",
      "step: 28555\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05567874759435654\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350397.69968\n",
      "step: 28575\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05333476886153221\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350403.7160842\n",
      "step: 28595\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.054952919483184814\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350409.706839\n",
      "step: 28615\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05765093117952347\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350415.7081344\n",
      "step: 28635\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05311894789338112\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350421.7027428\n",
      "step: 28655\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0395037978887558\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350427.7036872\n",
      "step: 28675\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06391850113868713\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350433.7138448\n",
      "step: 28695\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05110762268304825\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350439.7176383\n",
      "step: 28715\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04861859232187271\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350445.7216728\n",
      "step: 28735\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.049850646406412125\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350451.7284818\n",
      "step: 28755\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04857302084565163\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350457.730245\n",
      "step: 28775\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.049915723502635956\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350463.7269664\n",
      "step: 28795\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.054269492626190186\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350469.7216723\n",
      "step: 28815\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04774180054664612\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350475.7267234\n",
      "step: 28835\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05893535912036896\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350481.7423925\n",
      "step: 28855\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05336211621761322\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350487.747579\n",
      "step: 28875\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05520288646221161\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350493.7490187\n",
      "step: 28895\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04443514347076416\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350499.756317\n",
      "step: 28915\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.045455873012542725\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350505.7704623\n",
      "step: 28935\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04448198899626732\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350511.7790544\n",
      "step: 28955\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.046108096837997437\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350517.7836044\n",
      "step: 28975\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.047472238540649414\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350523.7848253\n",
      "step: 28995\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.050879158079624176\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350529.8016732\n",
      "step: 29015\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0499049574136734\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350535.8175967\n",
      "step: 29035\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04387292638421059\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350541.835817\n",
      "step: 29055\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06074024736881256\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350547.8437986\n",
      "step: 29075\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05085045099258423\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350553.8569703\n",
      "step: 29095\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05132193863391876\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350559.867023\n",
      "step: 29115\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.040079522877931595\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350565.8855789\n",
      "step: 29135\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05336112529039383\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350571.900386\n",
      "step: 29155\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05205550044775009\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350577.914012\n",
      "step: 29175\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.06200636178255081\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350583.9182198\n",
      "step: 29195\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05474770814180374\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350589.918264\n",
      "step: 29215\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05112593621015549\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350595.931037\n",
      "step: 29235\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04616694897413254\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350601.938378\n",
      "step: 29255\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.047824401408433914\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350607.9541721\n",
      "step: 29275\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04662952572107315\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350613.9729192\n",
      "step: 29295\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.056692540645599365\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350619.9827826\n",
      "step: 29315\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04997014254331589\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350625.9973645\n",
      "step: 29335\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04476016387343407\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350632.0065625\n",
      "step: 29355\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04178301990032196\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350638.0268838\n",
      "step: 29375\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05973398685455322\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350644.044014\n",
      "step: 29395\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05149003490805626\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350650.0555356\n",
      "step: 29415\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04365522414445877\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350656.0730965\n",
      "step: 29435\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05064970254898071\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350662.0881417\n",
      "step: 29455\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05342607945203781\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350668.104946\n",
      "step: 29475\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.042097896337509155\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350674.1095068\n",
      "step: 29495\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05948694050312042\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350680.1169872\n",
      "step: 29515\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.050000905990600586\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350686.1221895\n",
      "step: 29535\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0480196475982666\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350692.1442192\n",
      "step: 29555\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05180255323648453\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350698.1496267\n",
      "step: 29575\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04334715008735657\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350704.1641617\n",
      "step: 29595\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05161098763346672\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350710.168698\n",
      "step: 29615\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04844561964273453\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350716.1833768\n",
      "step: 29635\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04736042022705078\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350722.1942348\n",
      "step: 29655\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.046837739646434784\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350728.21118\n",
      "step: 29675\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05428135022521019\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350734.206512\n",
      "step: 29695\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.061118729412555695\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350740.2211254\n",
      "step: 29715\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.044022709131240845\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350746.2340283\n",
      "step: 29735\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05168784409761429\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350752.2656155\n",
      "step: 29755\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05108219385147095\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350758.2781289\n",
      "step: 29775\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05098605901002884\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350764.2991765\n",
      "step: 29795\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05463811755180359\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350768.634002\n",
      "step: 21\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.050927288830280304\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350808.2516146\n",
      "step: 21\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0006956036668270826\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350808.252463\n",
      "step: 21\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350809.964125\n",
      "step: 29810\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05026768147945404\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350815.9988232\n",
      "step: 29830\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05011232942342758\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350821.9930978\n",
      "step: 29850\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05409245193004608\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350827.994018\n",
      "step: 29870\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05309954658150673\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350833.9894302\n",
      "step: 29890\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05771826207637787\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350839.9986598\n",
      "step: 29910\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04119512438774109\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350845.9930081\n",
      "step: 29930\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0476149320602417\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350851.9848676\n",
      "step: 29950\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05381767451763153\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350857.9799194\n",
      "step: 29970\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05224897339940071\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350863.990602\n",
      "step: 29990\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04884989559650421\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350870.0045893\n",
      "step: 30010\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.057231172919273376\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350876.0007832\n",
      "step: 30030\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04232323169708252\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350882.0086224\n",
      "step: 30050\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05302491784095764\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350888.01449\n",
      "step: 30070\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.053549543023109436\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350894.0328262\n",
      "step: 30090\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.053823668509721756\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350900.0440326\n",
      "step: 30110\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05101097375154495\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350906.0507307\n",
      "step: 30130\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05529850721359253\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350912.060815\n",
      "step: 30150\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05196110159158707\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350918.068443\n",
      "step: 30170\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05098075419664383\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350924.0748863\n",
      "step: 30190\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04923796281218529\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350930.0955043\n",
      "step: 30210\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.040784720331430435\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350936.098497\n",
      "step: 30230\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05659835413098335\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350942.0998518\n",
      "step: 30250\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.046810392290353775\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350948.0973952\n",
      "step: 30270\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04891218990087509\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350954.1006362\n",
      "step: 30290\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.051565565168857574\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350960.099136\n",
      "step: 30310\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05238461494445801\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350966.1212437\n",
      "step: 30330\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.054671403020620346\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350972.1193137\n",
      "step: 30350\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04744227975606918\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350978.140525\n",
      "step: 30370\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04950990527868271\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350984.1544497\n",
      "step: 30390\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04097035527229309\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350990.1674516\n",
      "step: 30410\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04835542291402817\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555350996.1887639\n",
      "step: 30430\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.056000854820013046\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351002.1956155\n",
      "step: 30450\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04957909137010574\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351008.2079952\n",
      "step: 30470\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.049964211881160736\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351014.223449\n",
      "step: 30490\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.054129794239997864\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351020.2292984\n",
      "step: 30510\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.053790345788002014\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351026.2481344\n",
      "step: 30530\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.045048877596855164\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351032.2532692\n",
      "step: 30550\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.049160052090883255\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351038.2712185\n",
      "step: 30570\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04221327602863312\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351044.2841406\n",
      "step: 30590\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05185858532786369\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351050.2960563\n",
      "step: 30610\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.043954603374004364\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351056.3151772\n",
      "step: 30630\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04932524636387825\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351062.3294303\n",
      "step: 30650\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05212128162384033\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351068.3351483\n",
      "step: 30670\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04595814645290375\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351074.349756\n",
      "step: 30690\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05074029415845871\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351080.3671582\n",
      "step: 30710\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05459324270486832\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351086.384838\n",
      "step: 30730\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04301992058753967\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351092.4000752\n",
      "step: 30750\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04454908147454262\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351098.4190705\n",
      "step: 30770\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05268429219722748\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351104.4435256\n",
      "step: 30790\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05024074390530586\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351110.4637263\n",
      "step: 30810\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.048800695687532425\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351116.4831107\n",
      "step: 30830\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.046110279858112335\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351122.497942\n",
      "step: 30850\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.048063695430755615\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351128.523828\n",
      "step: 30870\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0653904527425766\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351134.5461502\n",
      "step: 30890\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0561504140496254\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351140.5559552\n",
      "step: 30910\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05101730674505234\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351146.5579321\n",
      "step: 30930\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04787495732307434\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351152.570873\n",
      "step: 30950\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05567105486989021\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351158.5833573\n",
      "step: 30970\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04465791583061218\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351164.591678\n",
      "step: 30990\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.051636580377817154\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351170.6034625\n",
      "step: 31010\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05042704939842224\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351176.3623767\n",
      "step: 31030\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04591364786028862\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351181.7525938\n",
      "step: 31050\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.053017757833004\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351187.1444864\n",
      "step: 31070\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.047926247119903564\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351192.5460389\n",
      "step: 31090\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05421813577413559\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351197.9315283\n",
      "step: 31110\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05069885775446892\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351203.3132963\n",
      "step: 31130\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.051661085337400436\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351208.672871\n",
      "step: 31150\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05468103289604187\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351212.5785754\n",
      "step: 22\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.050083767622709274\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351248.5603077\n",
      "step: 22\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0006915733683854342\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351248.560993\n",
      "step: 22\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351250.1620147\n",
      "step: 31165\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.056505102664232254\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351255.5825703\n",
      "step: 31185\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04683219641447067\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351260.9625027\n",
      "step: 31205\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05198434740304947\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351266.3391974\n",
      "step: 31225\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.042755573987960815\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351271.7125123\n",
      "step: 31245\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04714693874120712\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351277.1133735\n",
      "step: 31265\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.044975198805332184\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351282.4960673\n",
      "step: 31285\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04712869971990585\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351287.8924096\n",
      "step: 31305\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0495835542678833\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351293.2755919\n",
      "step: 31325\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05506286770105362\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351298.6436636\n",
      "step: 31345\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05436640605330467\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351304.0143788\n",
      "step: 31365\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0476035475730896\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351309.3920684\n",
      "step: 31385\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05153132230043411\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351314.774636\n",
      "step: 31405\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.052155859768390656\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351320.1665142\n",
      "step: 31425\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04644281044602394\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351325.5554223\n",
      "step: 31445\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05273091420531273\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351330.9498124\n",
      "step: 31465\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04909061640501022\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351336.325321\n",
      "step: 31485\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05186399817466736\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351341.6955905\n",
      "step: 31505\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04382222145795822\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351347.0958817\n",
      "step: 31525\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05178477615118027\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351352.4895954\n",
      "step: 31545\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04791628569364548\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351357.8907747\n",
      "step: 31565\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0502496063709259\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351363.2627883\n",
      "step: 31585\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.052543479949235916\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351368.6418872\n",
      "step: 31605\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05689924210309982\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351374.0344377\n",
      "step: 31625\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04903930425643921\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351379.429967\n",
      "step: 31645\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04448710009455681\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351384.8097136\n",
      "step: 31665\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.037864770740270615\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351390.1913936\n",
      "step: 31685\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05216678977012634\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351395.5827434\n",
      "step: 31705\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.048320986330509186\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351400.9855525\n",
      "step: 31725\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04797651618719101\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351406.3946087\n",
      "step: 31745\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04826042056083679\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351411.7961714\n",
      "step: 31765\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.055001430213451385\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351417.1719894\n",
      "step: 31785\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0516369566321373\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351422.5589056\n",
      "step: 31805\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.049191851168870926\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351427.933715\n",
      "step: 31825\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04269225522875786\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351433.3322656\n",
      "step: 31845\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.042900629341602325\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351438.721927\n",
      "step: 31865\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.046805333346128464\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351444.1087492\n",
      "step: 31885\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04091600328683853\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351449.4846127\n",
      "step: 31905\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.053789008408784866\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351454.8658109\n",
      "step: 31925\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04804952070116997\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351460.2441483\n",
      "step: 31945\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04470094293355942\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351465.6291742\n",
      "step: 31965\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.058478113263845444\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351471.0277\n",
      "step: 31985\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0482846200466156\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351476.4234009\n",
      "step: 32005\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05071892961859703\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351481.8135521\n",
      "step: 32025\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05402433127164841\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351487.196586\n",
      "step: 32045\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05047759786248207\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351492.5658727\n",
      "step: 32065\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04972153529524803\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351497.9518147\n",
      "step: 32085\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04587029665708542\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351503.3390965\n",
      "step: 32105\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.045513227581977844\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351508.7385876\n",
      "step: 32125\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04650488495826721\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351514.1181412\n",
      "step: 32145\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04564417898654938\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351519.5109293\n",
      "step: 32165\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05111805349588394\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351524.886393\n",
      "step: 32185\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04637610912322998\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351530.2581449\n",
      "step: 32205\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05253288149833679\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351535.6470563\n",
      "step: 32225\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05169951170682907\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351541.0353522\n",
      "step: 32245\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04531247913837433\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351546.437611\n",
      "step: 32265\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.045708395540714264\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351551.8254917\n",
      "step: 32285\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04558204114437103\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351557.2004504\n",
      "step: 32305\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05771595984697342\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351562.5753598\n",
      "step: 32325\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.050893090665340424\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351567.964193\n",
      "step: 32345\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05004949867725372\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351573.3283167\n",
      "step: 32365\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05319077521562576\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351578.7187018\n",
      "step: 32385\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04724906384944916\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351584.1065652\n",
      "step: 32405\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.046374157071113586\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351589.496471\n",
      "step: 32425\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.052209921181201935\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351594.8907847\n",
      "step: 32445\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04805390536785126\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351600.2743795\n",
      "step: 32465\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.05388762056827545\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351605.6380024\n",
      "step: 32485\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.04906867444515228\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351611.0052607\n",
      "step: 32505\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Training loss\"\n",
      "    simple_value: 0.0497012734413147\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351614.903692\n",
      "step: 23\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Train loss during training\"\n",
      "    simple_value: 0.04927339777350426\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351650.909393\n",
      "step: 23\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test loss during training\"\n",
      "    simple_value: 0.0007085640681907535\n",
      "  }\n",
      "}\n",
      "\n",
      "wall_time: 1555351650.9104767\n",
      "step: 23\n",
      "summary {\n",
      "  value {\n",
      "    tag: \"Test accuracy during training\"\n",
      "    simple_value: 0.0\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name = 'unaligned_hard/'\n",
    "in_path = os.path.join('logs/',name)\n",
    "print(in_path)\n",
    "print(os.listdir(in_path))\n",
    "for event_name in os.listdir(in_path):\n",
    "    event_path = os.path.join(in_path, event_name)\n",
    "    print(event_path)\n",
    "    for event in tf.train.summary_iterator(event_path):\n",
    "        \n",
    "        #print(event.summary.value)\n",
    "        #print(event.summary.value.get('tag'))\n",
    "        step = event.step\n",
    "        val = event.summary.value\n",
    "        if val:\n",
    "            val = val[0]\n",
    "            #print('VALEUR ')\n",
    "            tag =val.tag\n",
    "            print(event)\n",
    "            #if tag == \"Training loss\":\n",
    "            #    print(event)\n",
    "        #print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
